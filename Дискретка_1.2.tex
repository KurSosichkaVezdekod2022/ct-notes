\documentclass{article}
\input{Headers/header}
\input{Headers/old-formal}

\usepackage[outputdir=T:/TeX, cachedir=_minted-caches]{minted}
\usemintedstyle{native}

\usetikzlibrary{circuits.logic.IEC}
\tikzset{
    every node/.style={
        not gate IEC symbol={},
        or gate IEC symbol={$\scriptstyle\lor$},
        and gate IEC symbol={$\scriptstyle\land$},
        xor gate IEC symbol={$\scriptstyle\oplus$}
    }
}
\tikzstyle{branch}=[fill,shape=circle,minimum size=3pt,inner sep=0pt]
\tikzset{treenode/.style={
        inner sep=0pt,
        text width=5mm,
        scale=.8,
        circle,
        outer sep=2mm,
        align=center,
        draw=black,
        text=black,
        fill=white,
        thin
}}

\makeatletter
\renewcommand*{\minted@cleancache}{}
\makeatother

\usepackage{pgf}
\newcommand{\eval}[1]{\pgfmathparse{#1}\pgfmathresult}

\geometry{legalpaper, paperheight=16383pt, margin=1in}
\setcounter{totalnumber}{100}
\pagestyle{empty}

\begin{document}
    \section{Введение в дискретную теорию вероятности.}
    \paragraph{\undercolorblack{orange}{Основные понятия}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Сейчас у нас по сути будет поддержка АиСД, чтобы мы уже по-честному могли анализировать quick sort и прочие вероятностные алгоритмы. Полный курс теорвера будет в четвёртом семестре, где всё пройденное сейчас будет быстро повторено.\\
            После нашего теорвера будет часть теории формальных языков (это о том, как компьютеру сообщить какие-то данные). Это о том, как писать/читать конфиги, как с математической точки зрения работают ЯП и прочее.
        \end{Comment}
        \begin{Comment}
            Мы здесь в теорвере будет часто говорить слово <<дискретная>>, и в этом есть смысл. В теорвере для более чем счётных штук будет всё по-другому, и это будет потом.\\
            Итак, с древности людей интересовала случайность, философы писали многостраничные трактаты о том, есть ли случайность вообще, либо же всё детерминировано. Имело это всё прикладное значение, потому что азартные игры в основном основаны на случайности, а значит можно пытаться выигрывать в казино. Вопрос азартных игр мы обсуждать не будем. Философский вопрос о том, детерминировано ли всё либо нет, оставим на шестой сем в курс философии. А у нас будет аксиоматическая вероятность по Колмогорову.\\
            Итак, что такое случайность с этой точки зрения?
        \end{Comment}
        \dfn \undercolor{red}{Дискретное вероятностное пространство} состоит из следующих штук: $\Omega$ --- \undercolor{red}{множество элементарных исходов} и функция $p\colon\Omega\to[0;1]$ --- \undercolor{red}{вероятностная мера} --- такая, что $\sum\limits_{\omega\in\Omega}p(\omega)=1$. Тогда если $\omega\in\Omega$, то $p(\omega)$ --- \undercolor{red}{вероятность события} $\omega$.
        \begin{Comment}
            Стоит заметить, что в нашем случае $\Omega$ не более чем счётно, а все $p(\omega)$ положительны, что значит, что сумма $\sum\limits_{\omega\in\Omega}p(\omega)$ корректна.
        \end{Comment}
        \begin{Example}
            Рассматривать $\Omega=\varnothing$ бессмысленно, потому что тогда сумма будет 0. Рассматривать одноэлементное $\Omega$ осмысленно, но скучно.
        \end{Example}
        \begin{Example}
            Самое минимальное интересное $\Omega$ --- $\{0;1\}$. Самое простое $p$ --- $p(0)=p(1)=\frac12$. Это <<честная монета>>. А если есть честная, то должна быть и нечестная ($p(0)\neq p(1)$). Это также называется распределением Бернулли, потому что он первым изучил ситуацию, когда мы много раз подбрасываем монетку. В данном примере очень часто $p(1)$ обозначают за $p$, а $p(0)$ --- за $q$, чтобы быстрее было писать.
        \end{Example}
        \begin{Comment}
            Понятно, что рассматривать пространство, обладающее физическим смыслом, абсолютно необязательно. Но мы всё же рассмотрим ещё пару.
        \end{Comment}
        \begin{Example}
            <<Честная игральная кость>> --- $\Omega=\{1;2;3;4;5;6\}$, $p(1)=p(2)=\cdots=p(6)=\frac16$.
        \end{Example}
        \dfn Пусть $(\Omega_1;p_1)$, $(\Omega_2;p_2)$ --- дискретные вероятностные пространства. Тогда их \undercolor{red}{прямым произведением} называется вероятностное пространство $(\Omega;p)$, где $\Omega=\Omega_1\times\Omega_2$, $p(\omega_1;\omega_2)=p_1(\omega_1)\times p(\omega_2)$.
        \thm Прямая сумма также является дискретным вероятностным пространством.
        \begin{Proof}
            Ну, нужно проверить сумму. $\sum\limits_{\substack{\omega_1\in\Omega_1\\\omega_2\in\Omega_2}}p_1(\omega_1)p_2(\omega_2)=\sum\limits_{\omega_1\in\Omega_1}p_1(\omega_1)\cdot\sum\limits_{\omega_2\in\Omega_2}p_2(\omega_2)=1\cdot1=1$.
        \end{Proof}
        \begin{Example}
            Что ещё можно сделать? Можно сделать колоду карт. Мы можем вытянуть карту некоторой масти --- $\Omega_1=\{\heartsuit,\spadesuit,\clubsuit,\diamondsuit\}$ и некоторого значения --- $\Omega_2=\{6,7,8,9,10,\mathrm J,\mathrm Q,\mathrm K,\mathrm A\}$. И мы можем их перемножить и получить вероятность конкретной карты.
        \end{Example}
        \begin{Comment}
            Ну, определение это, конечно, хорошо, но нам бы ещё делать что-нибудь с ним.
        \end{Comment}
        \dfn Подмножества $\Omega$ называют \undercolor{red}{случайным событием}. \undercolor{red}{Вероятность случайного события} $A\subset\Omega$ --- это $\sum\limits_{a\in A}p(a)$. Обычно это обозначается $P(A)$, но иногда также $Pr(A)$ или $\mathbb P(A)$.
        \begin{Comment}
            Никогда не говорите, что случайное событие --- событие, которое либо происходит, либо нет. Это, во-первых, не правда ($P(\varnothing)=0$, то есть $\varnothing$ никогда не происходит, а $P(\Omega)=1$, то есть $\Omega$ всегда происходит). И к тому же это не единственные такие множества, потому что вы можете иметь элементарный исход с вероятностью ноль.
        \end{Comment}
        \begin{Example}
            Рассмотрим честную монету. У честной монеты есть четыре исхода --- $\varnothing$ с вероятностью 0, $\{0\}$ и $\{1\}$ с вероятностями $\frac12$ и $\{0;1\}$ с вероятностью 1.
        \end{Example}
        \begin{Example}
            Ещё рассмотрим кость. Что такое событие <<выпало чётное число>>? Это $\{2;4;6\}$. Чему равна его вероятность? Ну, $p(2)+p(4)+p(6)=\frac16+\frac16+\frac16=\frac12$.
        \end{Example}
        \begin{Example}
            А давайте рассмотрим $\Omega=\mathbb N$. В таком случае очень грустно жить, ведь нельзя сделать равномерное распределение (то есть чтобы элементарные исходы были равновероятны). Потому что если получится, и вероятность будет $p$, то сумма всех исходов будет либо 0, либо $\infty$.\\
            Но вообще на $\mathbb N$ можно задать вероятностное пространство. Например, такое: $p(i)=\frac1{2^i}$. У этого есть даже смысл и название. Смысл --- количество бросков честной монеты, пока не выпадет орёл, название --- геометрическое распределение. То же самое можно сделать с нечестной монетой и иметь $p(i)=pq^{i-1}$.
        \end{Example}
        \begin{Comment}
            Нам хочется ввести независимые события, но на самом деле пока не хочется, мотивации нет. Поэтому мы введём понятие условной вероятности. Зачем?
            Рассмотрим кость. Её уже кинули и сказали вам, что выпало число больше трёх. Вы не видите какое, но должны ответить, какова вероятность, что это число чётное? То есть мы по сути урезаем наше вероятностное пространство для только <<больших>> чисел. И нам нужно объявить новую вероятность элементарного исхода.
        \end{Comment}
        \dfn Если у нас есть события $A$ и $B$, то \undercolor{red}{условная вероятность} $P(A|B)$ (<<вероятность $A$ при условии $B$>>) --- это $\frac{P(A\cap B)}{P(B)}$.
        \thm Очевидно проверяется, что для заданного $B$ $P(A|B)$ действительно является функцией вероятности.
        \begin{Comment}
            Как сказать, что события $A$ и $B$ независимы? Ну, очевидно, что $P(A|B)=P(A)$. Это равносильно $\frac{P(A\cap B)}{P(B)}=P(A)$.
        \end{Comment}
        \dfn События $A$ и $B$ называются \undercolor{red}{независимыми}, если $P(A\cap B)=P(A)\cdot P(B)$.
        \begin{Example}
            В нашем примере $A=\text{ <<выпало чётное число>>}$ и $B=\text{ <<выпало число больше 3>>}$ не являются независимыми, потому что $P(A)=P(B)=\frac12$, а $P(A\cup B)=\frac13$, не хорошо.
        \end{Example}
        \begin{Comment}
            Почему $P(A|B)=P(A)$ --- плохое определение, а $P(A\cap B)=P(A)\cdot P(B)$ --- хорошее? Потому что $\varnothing$ тоже вполне себе событие. В первом варианте вообще не понятно, что у него с независимостью. А во втором он независим с любым. А в недискретных вероятностных пространствах вообще грустно, там вероятность равная нулю встречается намного чаще.
        \end{Comment}
        \begin{Example}
            Давайте рассмотрим всё же независимые события. Можно легко изменить предыдущий пример: $A=\text{ <<выпало чётное число>>}$ и $B=\text{ <<выпало число больше 4>>}$. Тут уже всё хорошо.
        \end{Example}
        \begin{Comment}
            Всё же наше определение недостаточно отражает ситуацию <<Вася и Петя независимо друг от друга кидают монеты>>.
        \end{Comment}
        \dfn Пусть $(\Omega_1;p_1)$ и $(\Omega_2;p_2)$ --- вероятностные пространства. Пусть $A_1\subset\Omega_1$, $A_2\subset\Omega_2$ Тогда $A_1\times\Omega_2$ и $\Omega_1\times A_2$ --- \undercolor{red}{цилиндрические события}. Аналогично определение обобщается на произведение большего количества пространств. При это $A_1$ и $A_2$ --- \undercolor{red}{основания} цилиндрических событий.
        \thm $A_1\times\Omega_2$ и $\Omega_1\times A_2$ независимы.
        \begin{Proof}
            Ну, очевидно, $P(A_1\times\Omega_2)=P_1(A_1)$, а $P(\Omega_1\times A_2)=P_2(A_2)$. Не менее очевидно, что $P((A_1\times\Omega_2)\cap(\Omega_1\times A_2))=P(A_1\times A_2)=P_1(A_1)P_2(A_2)$.
        \end{Proof}
        \begin{Comment}
            Это нам, собственно, и позволяет делать ситуации вида <<Вася и Петя независимо друг от друга кидают монеты>>, достаточно сделать два вероятных пространства: на Васю и на Петю.
        \end{Comment}
        \begin{Comment}
            Окей, а как определить независимые события, если их больше двух? Нетривиально. Можно попарно, но это плохо. Рассмотрим вот что: $\Omega=\{00;01;10;11\}$, $A_1=\{00;10\}$, $A_2=\{00;01\}$, $A_3=\{00;11\}$. Они попарно независимы. Но в совокупности какая-то хрень, потому что если мы уже знаем $A_1$ и $A_2$, то мы уже точно знаем, что $A_3$. А не хотелось бы.
        \end{Comment}
        \dfn События $A_1;A_2;\ldots;A_n$ называются \undercolor{red}{независимыми в совокупности}, если $$\forall I\subset\{1;2;\ldots;n\}~P\left(\bigcup\limits_{i\in I}A_i\right)=\prod\limits_{i\in I}P(A_i)$$
        \dfn Система событий $A_1;A_2;\ldots;A_n$ называется \undercolor{red}{полной}, если $\forall A_i;A_j\colon i\neq j~A_i\cap A_j=\varnothing$ и $\bigcup\limits_{i=1}^nA_i=\Omega$.
        \thm \undercolor{darkgreen}{Формула полной вероятности}. Если $A_1;A_2;\ldots;A_n$ --- полная система событий, а $B$ --- событие, то $P(B)=\sum\limits_{i=1}^nP(B|A_i)P(A_i)$.
        \begin{Proof}
            Это доказывается довольно просто. Из теории множеств очевидно следует, что $P(B)=\sum\limits_{i=1}^nP(B\cap |A_i)$ (каждый элемент $B$ есть строго в одном $B\cap A_i$). При этом $P(B\cap |A_i)=P(B|A_i)P(A_i)$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Формула Байеса}. Если $A_1;A_2;\ldots;A_n$ --- полная система событий, а $B$ --- событие, $P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum\limits_{j=1}^nP(B|A_j)P(A_j)}$.
        \begin{Proof}
            Ну, это тоже легко доказать. Что такое $P(A_i|B)$? Единственное, что мы про него знаем --- это $P(A_i|B)=\frac{P(A_i\cap B)}{P(B)}$. Верхнее --- $P(B|A_i)P(A_i)$, а нижнее (по формуле полной вероятности) --- $\sum\limits_{j=1}^nP(B|A_j)P(A_j)$.
        \end{Proof}
        \begin{Comment}
            Формулу Байеса также называют формулой апостериорной вероятности и вот, почему. Есть априорная вероятность --- вероятность до проведения эксперимента, а апостериорная --- после. То есть мы уже знаем, что $B$, а нам хочется узнать, что же (из набора $A_1;A_2;\cdots;A_n$) верно. Чтобы стало более понятно, приведём пример с тестами на различные болезни.
        \end{Comment}
        \begin{Example}
            Рассмотрим человека, которых чихает и кашляет. В нашем случае это, ну, процентов 70 болезни коронавирусом (то есть $P(A_1)=0.7$, $P(A_2)=0.3$). И у нас есть  <<хороший тест>>, который работает так: если человек болен, то тест положительный с вероятностью $0.9$, а если человек здоров, то тест точно отрицательный. То есть $P(B|A_1)=\frac9{10}$, $P(B|A_2)=0$. Тогда с какой вероятностью человек с положительным тестом болен? По формуле Байеса, $P(A_1|B)=\frac{0.9\cdot0.7}{0.9\cdot0.7+0\cdot0.3}=1$, что в общем-то логично, ведь здоровый человек не может получить положительный тест ($P(B|A_2)=0$). Поэтому давайте рассмотрим чуть более плохой тест, у которого $P(B|A_2)=0.1$. Тогда получится, что $P(A_1|B)=\frac{63}{66}$, что, в общем-то тоже не так плохо. А всё потому, что коронавирус --- плохой пример тут, если брать более редкие болезни, до вероятность будет сильно хуже.
        \end{Example}
        \dfn Пусть $(\Omega;p)$ --- дискретное вероятностное пространство. Тогда \undercolor{red}{дискретная случайная величина} --- это функция $\xi\colon\Omega\to\mathbb R$.
        \begin{Comment}
            Это определение как-то не отражает наше понимание того, что такое случайная величина. Чтобы связать это с нашим жизненным опытом, достаточно сказать, что это просто какая-то численная интерпретация элементарных событий. Например, количество точек на честной игральной кости таковой величиной является. Нужно это потому, чо обычно нам какая-то численная характеристика про исход интересна, а не сам исход.
        \end{Comment}
        \begin{Example}
            Например, если мы подкидываем честную монетку 1000 раз, наши элементарные исходы --- двоичные вектора размерности 1000. Но нам обычно интересно не это, а, скажем, количество орлов. Тогда математически $\Omega=\{0;1\}$, $p(0)=p(1)$, а наша случайная величина задана на пространстве $\Omega^{1000}$ и работает так: $\xi((a_1;a_2;\ldots;a_{1000}))=\sum\limits_{i=1}^{1000}a_i$.
        \end{Example}
        \begin{Comment}
            Что вообще нам может быть интересно про случайную величину? Нам интересна вот такая величина: $[\xi=a]$. Это по сути событие, что случайная величина равна $a$. Понятно, что математически это множество $\xi^{-1}(\{a\})$. С этим обозначением связано такое: $P(\xi=a)$. Это $\sum\limits_{\substack{\omega\in\Omega\\\xi(\omega)=a}}p(\omega)$. Аналогично определяется $P(\xi>a)$ и $P(\xi<a)$.
        \end{Comment}
        \begin{Example}
            Рассмотрим величину на двух честных кубиках. И величина будет $\xi((i;j))=i+j$. То есть наша случайная величина принимает значения от 2 до 12, но не равновероятно. $P(\xi=2)=\frac1{36}$, а $P(\xi=10)=\frac3{36}=\frac1{12}$.
        \end{Example}
        \dfn \undercolor{red}{Дискретная плотность случайной величины} $\xi$ --- функция $f_\xi$, которая числу $a$ ставит в соответствие число $P(\xi=a)$.
        \begin{Comment}
            Поскольку живём мы в дискретных вероятностных пространствах, тут множество значений случайной величины конечно либо счётно, а значит эта функция не равна нулю только для небольшого количества $a$ и их можно занумеровать.
        \end{Comment}
        \begin{Example}
            Нарисуем график этой функции в описанном нами примере из двух честных кубиков.
            \begin{center}
                \begin{tikzpicture}
                    \begin{axis}[
                        width = 10cm,
                        height = 10cm,
                        grid = both,
                        xmin = -.5,
                        xmax = 12.5,
                        ymin = -1/72,
                        ymax = 15/72,
                        axis x line = middle,
                        axis y line = middle,
                        axis line style = {->},
                        xtick = {2,...,12},
                        ytick = {1/36,2/36,3/36,4/36,5/36,6/36},
                        yticklabels = {$\frac1{36}$,$\frac1{18}$,$\frac1{12}$,$\frac19$,$\frac5{36}$,$\frac16$},
                        xlabel = {$a$},
                        ylabel = {$P(\xi=a)$},
                        ]
                        \foreach\x in {2,...,12}
                        {
                            \pgfmathparse{(6-abs(\x-7))/36}
                            \let\y\pgfmathresult
                            \edef\temp{\noexpand\node[draw, inner sep=1pt, draw=red, fill=red, circle] at (axis cs:\x,\y) {};}
                            \temp
                        }
                    \end{axis}
                \end{tikzpicture}
            \end{center}
            При этом по графику нельзя провести прямые, потому что вероятность $[\xi=2.5]$ равна нулю, а не $\frac{3.5}{36}$.
        \end{Example}
        \thm Очевидно, $\sum\limits_{a}P(\xi=a)=1$.
        \dfn \undercolor{red}{Функция распределения} случайной величины $\xi$ --- функция $F_\xi\colon a\mapsto P(\xi\leqslant a)$.
        \begin{Comment}
            Иногда вместо $\leqslant$ в определении стоит $<$, но у нас будет $\leqslant$.
        \end{Comment}
        \begin{Comment}
            Также такая штука также определена для непрерывных случайных величин. А вот плотность в них сильно сложнее устроена.
        \end{Comment}
        \begin{Comment}
            А если вы знаете плотность, как посчитать распределение? Ну, достаточно легко, это префиксная сумма (только в дискретном случае, очевидно; в непрерывном вместо суммы возникнет интеграл с переменным верхним пределом).
        \end{Comment}
        \begin{Comment}
            Какие ещё численные характеристики нам интересны о случайной величине? Ну, например, что-то наподобие среднего значения. Например, вам интересно, сколько в среднем точек выпадает на честном кубе.
        \end{Comment}
        \dfn \undercolor{red}{Математическое ожидание} величины $\xi$ --- число $E\xi$ (в некоторых источниках $M\xi$), равное $\sum\limits_{\omega\in\Omega}p(\omega)\xi(\omega)$
        \begin{Comment}
            Есть проблема с этим определением, когда пространство $\Omega$ бесконечно. Даже если счётно. Поэтому иногда математическое ожидание может не существовать. Причём сумма может не бесконечности быть равна, а просто не существовать, если в ней есть элементы разного знака.
        \end{Comment}
        \begin{Comment}
            Хорошо, а что если мы не знаем структуру $\Omega$, но знаем $P(\xi=a)$ для любого $a$? В таком случае сумму можно переписать как $\sum\limits_{\omega\in\Omega}\sum\limits_{a}p(\omega)a[\xi(\omega)=a]=\sum\limits_{a}a\sum\limits_{\omega\in\Omega}p(\omega)[\xi(\omega)=a]=\sum\limits_{a}aP(\xi=a)$. Такая формула как правило проще, чем изначальная.
        \end{Comment}
        \begin{Example}
            Возьмём один куб: $\Omega=\{1;2;3;4;5;6\}$. И возьмём $\xi(\omega)=\omega$. Тогда $E\xi=\frac16\cdot1+\frac16\cdot2+\frac16\cdot3+\frac16\cdot4+\frac16\cdot5+\frac16\cdot6=3.5$.
        \end{Example}
        \begin{Example}
            А если два кубика и $\xi((i;j))=i+j$? Можно перебрать 36 исходов или 12 значений величины. Вот первый вариант: $E\xi=\sum\limits_{i=1}^6\sum\limits_{j=1}^6\frac1{36}(i+j)=\sum\limits_{i=1}^6\sum\limits_{j=1}^6\frac1{36}i+\sum\limits_{i=1}^6\sum\limits_{j=1}^6\frac1{36}j=\sum\limits_{i=1}^6\frac16i+\sum\limits_{j=1}^6\frac16j=3.5+3.5=7$.
        \end{Example}
        \begin{Comment}
            Заметим, что раз уж у нас случайные величины --- это функции, мы можем их складывать, умножать друг на друга, умножать на число,  в степени возводить и т.п.
        \end{Comment}
        \thm \undercolor{darkgreen}{Аддитивность математического ожидания}. $E[\xi+\eta]=E\xi+E\eta$.
        \begin{Proof}
            $$\sum\limits_{\omega\in\Omega}p(\omega)(\xi(\omega)+\eta(\omega))=\sum\limits_{\omega\in\Omega}p(\omega)\xi(\omega)+\sum\limits_{\omega\in\Omega}p(\omega)\eta(\omega)=E\xi+E\eta$$
        \end{Proof}
        \thm \undercolor{darkgreen}{Однородность математического ожидания}. $E[\alpha \xi]=\alpha E\xi$.
        \begin{Proof}
            Аналогично предыдущему.
        \end{Proof}
        \thm Отсюда математическое ожидание линейно.
        \begin{Comment}
            Теперь мы можем очень легко вычислить мат. ожидание 1000 брошенных честных кубиков. Оно будет равно 3500.
        \end{Comment}
        \begin{Example}
            Рассмотрим нечестную монету (вероятность единицы $p$, вероятность нуля --- $q$). Рассмотрим случайную величину $\xi(\omega)=\omega$. Понятно, что её ожидание --- $p$. А что будет, если мы подкинем монетку $n$ раз и возьмём понятно какую величину. Чему будет равно её мат. ожидание. Ну, если без линейности, придётся много думать, а с ним всё ясно --- $np$. Ещё для этой величины не так уж сложно посчитать плотность $P(\xi=k)$. С какой вероятностью ровно $k$ раз выпадет единица? Ну, сначала разберёмся с вероятностью конкретного одного исхода, в котором в каком-то порядке $k$ единиц и $n-k$ нулей. Она равна, очевидно, $p^kq^{n-k}$. Значит для получения $P(\xi=k)$ нужно умножить эту штуку на количество интересных нам исходов. А она $\Cnk nk$. То есть $P(\xi=k)=\Cnk nkp^kq^{n-k}$. Кстати, если мы просуммируем это, получим 1 (как, в общем-то и должны: вероятность того, что величина будет равна хоть чему-то --- один). Потому что эта штука --- слагаемое бинома Ньютона.
        \end{Example}
        \dfn Распределение из комментария выше --- \undercolor{red}{биномиальное распределение}.
        \dfn Случайная величина, принимающая только значения 0 и 1 --- \undercolor{red}{индикатор}.
        \dfn \undercolor{red}{Индикаторная случайная величина} события $A\subseteq\Omega$ --- $\xi_A(\omega)=\begin{cases}
            1 & \omega\in A\\
            0 & \omega\notin A
        \end{cases}$.
        \thm Несложно заметить, что $P(\xi_A=1)=P(A)$, $P(\xi_A=0)=1-P(A)$.
        \begin{Comment}
            То есть это своего рода нечестная монета с параметром.
        \end{Comment}
        \begin{Example}
           Рассмотрим теперь $\Omega=S_n$, при этом все перестановки равновероятны (вероятность каждой $\frac1{n!}$). Теперь мы берём перестановку и считаем количество её неподвижных точек. Это случайная величина. Для неё нетривиально (но возможно) считать плотность, однако очень легко мы можем посчитать её мат. ожидание. С виду, ну, хер знает. Поэтому представим её как сумму простых случайных величин. Введём случайные величины $\xi_i$ --- равна единице, если $i$-тая точка неподвижна, нулю иначе ($\xi_i(\pi)=\begin{cases}
               1 & \pi_i=i\\
               0 & \pi_i\neq0
           \end{cases}$). Все индикаторные величины имеют одинаковое ожидание. Какое? Посчитаем, с какой вероятностью $\pi_i=i$. Поскольку все перестановки равновероятны, а таких перестановок $(n-1)!$, $P(\pi_i=i)=\frac{(n-1)!}{n!}=\frac1n$. Тогда $E\xi_i=\frac1n$, а $E\xi=\sum\limits_{i=1}^nE\xi_i=n\frac1n=1$.
        \end{Example}
        \begin{Comment}
            Хорошо, ожидание суммы мы знаем, а что с ожиданием произведения? Давайте рассмотрим две ситуации.\\
            Сначала подкинем один кубик дважды и умножим результаты. Потом подкинем один кубик и умножим результаты на противоположных гранях (противоположные --- это с суммой 7). Если бы мы считали сумму мат. ожиданий, мы бы получили 7 в обоих случаях. Но мы считаем произведение. В нашем примере результаты будут разные. Это наталкивает нас на концепт того, что величины бывают зависимыми, а бывают --- нет.
        \end{Comment}
        \dfn Случайные величины $\xi$ и $\eta$ \undercolor{red}{независимы}, если $\forall i,j~[\xi=i]\text{ и }[\eta=j]\text{ независимы}$.
        \thm Предыдущее определение равносильно $\forall i,j~[\xi\leqslant i]\text{ и }[\eta\leqslant j]\text{ независимы}$.
        \begin{Proof}
            Для конечных всё легко, для счётных потребуется немного матана, которого у нас, увы, нет, поэтому доказывать это мы не будем. Верно это, кстати, только для дискретных величин.
        \end{Proof}
        \thm Если $\xi$ и $\eta$ независимы, то $E[\xi\eta]=E\xi\cdot E\eta$.
        \begin{Proof}
            $E[\xi\eta]=\sum\limits_ttP(\xi\eta=t)=\sum\limits_{t}\sum\limits_{\substack{i;j\\ij=t}}ijP([\xi=i]\cup[\eta=i])\overset{(*)}=\sum\limits_{t}\sum\limits_{\substack{i;j\\ij=t}}ijP(\xi=i)P(\eta=i)=\sum\limits_{i;j}iP(\xi=i)jP(\eta=i)=E\xi\cdot E\eta$.\\
            В равенстве $(*)$ используется независимость.
        \end{Proof}
        \begin{Comment}
            Мат. ожидание --- одна численная характеристика величины. Она не даёт нам различить величину, всегда равную, 10 и величину, которая равномерно размазана от 0 до 20. А хочется. Поэтому вводится величина, которая определяет, насколько в среднем далеко величина отклоняется от своего ожидания.
        \end{Comment}
        \dfn \undercolor{red}{Дисперсия} величины $\xi$ --- число $D\xi$ (иногда $\operatorname{Var}(\xi)$), равное $E[(\xi-E\xi)^2]$.
        \thm $D\xi=E[\xi^2]-(E\xi)^2$.
        \begin{Proof}
            $E[(\xi-E\xi)^2]=E[\xi^2-2\xi E\xi+(E\xi)^2]=E[\xi^2]-2E\xi E\xi+(E\xi)^2=E[\xi^2]-(E\xi)^2$,
        \end{Proof}
        \dfn \undercolor{red}{Среднеквадратичное отклонение} величины $\xi$ --- $\sigma \xi=\sqrt{D\xi}$.
        \begin{Comment}
            Эта величина чуть более полезна, потому что имеет ту же размерность, что и мат. ожидание.
        \end{Comment}
        \thm Пусть $\xi$, $\eta$ независимы, тогда $D(\xi+\eta)=D\xi+D\eta$.
        \begin{Proof}
            $D(\xi+\eta)=E[(\xi+\eta)^2]-(E[\xi+\eta])^2=E[\xi^2]+2E[\xi\eta]+E[\eta^2]-(E\xi)^2-2E\xi E\eta-(E\eta)^2=D\xi+D\eta$.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Хвостовые неравенства}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Хвостовые неравенства --- это неравенства, в которых вы звоните случайному человеку к Нью-Йорке и спрашиваете его зарплату. А после этого сравниваете это со средней зарплатой в городе.\\
            Функции распределения случайных величин обычно выглядят как-то так:
            \begin{center}
                \begin{tikzpicture}
                    \begin{axis}[
                        width = 8cm, height = 8cm,
                        trig format plots = rad,
                        grid = none,
                        xmin = -1,
                        xmax = 8,
                        ymin = -.1,
                        ymax = .5,
                        axis x line = middle,
                        axis y line = middle,
                        axis line style = {->},
                        xtick = \empty,
                        ytick = \empty,
                        ]
                        \addplot[restrict y to domain=-.5:.5, domain=-1:8, samples=100, color=red] gnuplot{gamma(21)/(gamma(x+1)*gamma(21-x)) * 0.125^x * 0.875^(20-x)};
                    \end{axis}
                \end{tikzpicture}
            \end{center}
            А ещё есть heavy-tail случайные величины, когда у величины большой хвост (на рисунке хвост справа, который в ноль медленно уходит, и очень длинный).
            \begin{center}
                \begin{tikzpicture}
                    \begin{axis}[
                        width = 8cm, height = 8cm,
                        trig format plots = rad,
                        grid = none,
                        xmin = -1,
                        xmax = 7,
                        ymin = -.1,
                        ymax = 1,
                        axis x line = middle,
                        axis y line = middle,
                        axis line style = {->},
                        xtick = \empty,
                        ytick = \empty,
                        ]
                        \addplot[domain=-1:7, samples=100, color=red] {2.5 * x^2.5 / (1+x^2.5)^2};
                    \end{axis}
                \end{tikzpicture}
            \end{center}
        \end{Comment}
        \thm \undercolor{darkgreen}{Неравенство Маркова}. Пусть у нас есть случайная величина $\xi$ с ожиданием $\mu=E\xi\neq0$. Пусть $\xi\geqslant0$. Тогда $\forall c>0~P(\xi\geqslant c\mu)\leqslant\frac1c$.
        \begin{Proof}
            $\mu=\sum\limits_aaP(\xi=a)$. Разобьём эту сумму на то, что меньше $c\mu$ и на то, что больше: $\mu=\sum\limits_{0\leqslant a<c\mu}aP(\xi=a)+\sum\limits_{a\geqslant c\mu}aP(\xi=a)$. Первое больше либо равно нуля, а второе --- больше либо равно $\sum\limits_{a\geqslant c\mu}c\mu P(\xi=a)=c\mu\sum\limits_{a\geqslant\mu}P(\xi=a)=c\mu P(\xi\geqslant c\mu)$. Если $\mu=0$, то наше неравенство (в записанной нами форме), к сожалению, не верно, это мы записали в условие. Если $\mu\neq0$, то $\mu>0$, а значит на него можно сократить наше неравенство $\mu\geqslant c\mu P(\xi\geqslant c\mu)$ и поучить, что требуется.
        \end{Proof}
        \begin{Comment}
            Во-первых, это базовое неравенство, по меркам математики оно тривиально. Но при этом оно даёт фундамент для всех остальных более сложных и применимых на практике неравенств.\\
            Во-вторых, неравенство Маркова нельзя <<улучшить>>, то есть придумать что-то, что сильнее неравенства Маркова и выполнено для любой случайной величины.
        \end{Comment}
        \thm \undercolor{darkgreen}{Относительное неравенство Чебышёва}. Пусть $\xi\geqslant0$, $\xi\neq\mathrm{const}$, $E\xi\neq0$, $c>0$. Также пусть $\xi$ имеет среднеквадратичное отклонение $\sigma$. Тогда $P(|\xi-E\xi|\geqslant c\sigma)\leqslant\frac1{c^2}$.
        \begin{Proof}
            Рассмотрим $\eta=(\xi-E\xi)^2$. Это тоже случайная величина, причём неотрицательна. Также, поскольку $\xi\neq\mathrm{const}$, $E\eta\neq0$. Тогда рассмотрим $c^2>0$ и подставим $\eta$ в неравенства Маркова.\\
            $P(\eta\geqslant c^2E\eta)\leqslant\frac1{c^2}\Leftrightarrow P((\xi-E\xi)^2\geqslant c^2D\xi)\leqslant\frac1{c^2}\Leftarrow P(|\xi-E\xi|\geqslant c\sigma)\leqslant\frac1{c^2}$
        \end{Proof}
        \begin{Comment}
            То есть то, что величина отклонится на $c$ своих среднеквадратичных отклонений от ожидания обратно пропорциональна квадрату $c$. Поэтому экспериментаторы берут, например, $5\sigma$ и говорят, что это допустимая погрешность.
        \end{Comment}
        \thm \undercolor{darkgreen}{Абсолютное неравенство Чебышёва}. $P(|\xi-E\xi|\geqslant a)\leqslant\frac{D\xi}{a^2}$.
        \begin{Proof}
            В относительном неравенстве Чебышёва обозначим $a=\sigma c$.
        \end{Proof}
        \begin{Example}
            Рассмотрим пример. Наш пример с зарплатой из Нью-Йорка какой-то глупый, давайте приведём чуть более искусственный, но уже более интересный.\\
            У нас есть нечестная монета у которой одна сторона выпадает вдвое чаще, чем другая, но мы не знаем, какая чаще. И мы хотим это узнать. Давайте подкинем его, скажем, 99 раз и посмотрим, что будет. Если наша монета имеет $p=\frac23$, $q=\frac13$ и $\xi$ --- количество единиц. А $\eta$ --- количество единиц в обратном случае ($p=\frac13$, $q=\frac23$). Тогда $E\xi=66$, $E\eta=33$. А вот мы это сделали и узнали, что получилось 70 раз. Тогда мы можем предположить, что это $\xi$. И вообще можно сказать, что у нас то, что выпало больше раз, то мы и считаем выпадающим чаще. Какова вероятность, что мы ошибёмся? Ну, это $P(\xi\leqslant49)$. То есть $P(\xi-E\xi\leqslant-17)$.\\
            Что мы имеем? Мы имеем наше распределение, похожее на нормальное.
            \begin{center}
                \begin{tikzpicture}
                    \begin{axis}[
                        width = 8cm, height = 8cm,
                        trig format plots = rad,
                        grid = none,
                        xmin = -2,
                        xmax = 34,
                        ymin = -.009,
                        ymax = .2,
                        axis x line = middle,
                        axis y line = middle,
                        axis line style = {->},
                        xtick = {16.333, 22, 27.667},
                        xticklabels = {49, 66, 83},
                        ytick = \empty,
                        ]
                        \addplot[domain=0:33, samples=100, color=red] gnuplot{gamma(34)/(gamma(x+1)*gamma(34-x)) * (.667)^x * (.333)^(33-x)};
                        \addplot[domain=0:33, samples=100, color=darkgreen, dashed] gnuplot{gamma(34)/(gamma(x+1)*gamma(34-x)) * (.333)^x * (.667)^(33-x)};
                    \end{axis}
                \end{tikzpicture}
            \end{center}
            Мы хотим оценить один хвост, но умеем оценивать только два. Ну и фиг с ним, мы же оцениваем, а не считаем.\\
            $P(\xi-E\xi\leqslant-17)\leqslant P(|\xi-E\xi|\geqslant17)\leqslant\frac{D\xi}{17^2}$. Посчитаем дисперсию монетки. Пусть $\xi_1$ --- одна монетка. Поскольку $\xi_1$ --- это ноль или единица, $\xi_1^2=\xi_1$. Тогда $D\xi_1=E[\xi_1^2]-(E\xi_1)^2=p-p^2=p(1-p)=pq$. В нашем примере $D\xi_1=\frac29$. Заметим, что $\xi$ состоит из 99 независимых в совокупности случайных величин, дисперсия каждой из которых равна $\frac29$, а значит $D\xi=22$, а значит $P(|\xi-E\xi|\geqslant17)\leqslant\frac{22}{289}\approx0.07$.\\
            Конкретно в нашем случае оценку можно сильно улучшить.
        \end{Example}
        \begin{Comment}
            У неравенства Чебышёва есть та же патология, что и неравенства Маркова --- оно верно всегда, даже в самых дегенератских функций, например, таких:
            \begin{center}
                \begin{tikzpicture}
                    \begin{axis}[
                        width = 12cm, height = 8cm,
                        trig format plots = rad,
                        grid = none,
                        xmin = -5,
                        xmax = 5,
                        ymin = -.1,
                        ymax = .25,
                        axis x line = middle,
                        axis y line = middle,
                        axis line style = {->},
                        xtick = \empty,
                        ytick = \empty,
                        ]
                        \addplot[domain=-5:5, samples=100, color=red] {.5 * (abs(x)-2)^2.5 / (1+(abs(x)-2)^2.5)^2};
                    \end{axis}
                \end{tikzpicture}
            \end{center}
            А значит оно недостаточно точно оценивает нормальные случайные величины.\\
            Так, наша случайная величина равна сумме совершенно одинаковых случайных величин, а значит она не настолько непонятная, как нарисовано выше. В теории вероятности есть центральная предельная теорема и закон больших чисел, которые позволяют оценить что-то такое. Но они не входят наш курс, мы обсудим хвостовое неравенство, следующее из них. Оно называется <<границей Чернова>> (Chernoff bound). Пусть у нас есть величина $\xi$, такая что $P(\xi=1)=p$, $P(\xi=0)=q=1-p$. И пусть мы делаем $n$ экспериментов и смотрим на то, как далеко мы удалимся от среднего значения. Это неравенство утверждает, что $P(\sum\limits_{i=1}^n\xi_i\geqslant(1+\delta)np)\leqslant e^{-\frac{\delta^2}{2+\delta}np}$. От $\delta$ это как-то не очень тривиально зависит, а вот от $n$ --- экспоненциально. В нашем случае ошибка убывала как обратный квадрат. При этом подставив в эту формулу 99, получим не такую уж хорошую оценку (потому что у нас будет $\delta=\frac16$, а значит её квадрат достаточно большой), но если мы начнём подставлять большие числа, то получим уже более точный результат, чем по неравенству Чебышёва.\\
            В доказательстве этого утверждения интересный приём. Возьмём $P(\xi\geqslant a)$ и $s>0$. Тогда $\xi\geqslant a\Leftrightarrow e^{s\xi}\geqslant e^{sa}$. На кой нам это? А это превращает нам сумму в произведение. Что такое $s$? А это то, что мы выберем позже. Итак, имеем $P(\xi\geqslant a)=P(e^{s\xi}\geqslant e^{sa})$. Сюда применим неравенство Маркова ($e^{x}$ неотрицательная функция), получим $P(\xi\geqslant a)\leqslant\frac{Ee^{s\xi}}{e^{sa}}=\frac{Ee^{s\sum\limits_{i=1}^n\xi_i}}{e^{sa}}=\frac{E\prod\limits_{i=1}^ne^{s\xi_i}}{e^{sa}}=\frac{(Ee^{s\xi_i})^n}{e^{sa}}=\frac{(pe^s+q)^n}{e^{sa}}=\frac{(1+p(e^s-1))^n}{e^{sa}}$. Из математического анализа возьмём неравенство $e^x\geqslant1+x$. Тогда имеем $\frac{(1+p(e^s-1))^n}{e^{sa}}\leqslant\frac{e^{np(e^s-1)}}{e^{sa}}$. А это значит $P(\xi\geqslant(1+\delta)np)\leqslant\frac{e^{np(e^s-1)}}{e^{s(1+\delta)np}}$. Применяя ещё несколько неравенств и подбирая $s$, получим границу Чернова.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Теория информация и энтропия Шеннона}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Начинается всё с того, что мы пытаемся дать определение слову <<информация>>. И первое определение дано именно что Шенноном, и заключается оно в следующем. Вот мы получаем какую-то информацию. Скажем, содержание сегодняшней лекции. Перед этой лекцией мы не знаем ни что такое энтропия, какие есть формулы, а потом на лекцию приходим и узнаём это. Но как-то непонятно, а сколько информации мы получили за эту лекцию вообще. И вот одно из определений --- количества информации на базе случайного источника. Вот пусть у нас есть некоторый <<чёрный ящик>>, который умеет выдавать нам число от 1 до $n$. Как это может быть? Ну, например, в ящике сидит гном, который там бросает честную монетку и выдаёт 1 или 2. То есть пусть этот случайный источник выдаёт нам эти числа с некоторым случайным распределением (вероятность $i$ равна $p(i)$). Вот мы получаем его число, и это даёт нам довольно бесполезную, но всё же информацию. Это даёт нам следующий вывод: информация --- это уменьшение неопределённости. Когда мы бросаем монету, у нас есть неопределённость, а когда мы получаем результат, неопределённость уменьшается, потому что нам дали информацию. То есть информация --- минус неопределённость. Ну, хорошо, но это не даёт нам никакого определения этой самой неопределённости.\\
            Давайте попробуем найти численную характеристику этой самой неопределённости. От чего она зависит? От $p_1$, $p_2$, ..., $p_n$. И вот это число $H(p_1;p_2;\ldots;p_m)$ называется энтропией. И придумаем же формулу для неопределённости. И давайте придумаем свойства для неё, чтобы ей можно было математически пользоваться. Если все события $\Omega$ равновероятны (т.е. $p_1=\frac1n$), то, вероятно, она зависит только от $n$. Обозначим её за $h(n)=H(\frac1n;\frac1n;\ldots;\frac1n)$. Логично, что чем больше исходов, тем больше неопределённость. То есть мы получаем аксиому монотонности, $h(n)<h(n+1)$. Вторая аксиома достаточно трудная для восприятия. Пусть есть какие-то исходы $1,2,\ldots,n$. Давайте считать, что результат эксперимента состоит из двух частей (например, у нас есть карты с мастью и значением). Если мы каким-то образом узнаем только масть, то снизится ли неопределённость? Определённо, снизится. И на самом деле в данном случае мы сначала получаем информацию о масти, а когда перевернём карту --- о ранге. При этом если мы сразу перевернём карту, мы получим и ту, и другую информацию. И вроде как в эти двух экспериментах мы получаем одинаковую информацию. Итак, как это сформулировать? Пусть у нас события --- $(i;j)$. И пусть $p_i=P(A_i=\{i;j\})$, а $q_{ij}=P((i;j)\mid A_i)$. Тогда $p(i;j)=p_iq_{ij}$. Тогда информация в одном эксперименте --- $H(p_1q_{11};p_1q_{12};\ldots;p_1q_{1n};p_2q_{21};\ldots;p_nq_{nm})$. Это количество информации, которое мы получим, если посмотрим на всю карту сразу. А в другом эксперименте $H(p_1;p_2;\ldots;p_n)$, а потом $H(q_{i1};q_{i2};\ldots;q_{im_i})$. Но второе же зависит от $i$, причём вероятности могут быть разными. Но мы жу получим $H(q_{i1};q_{i2};\ldots;q_{im_i})$ информации с вероятностью $p_i$, так что давайте возьмём взвешенную сумму. Итого аксиома выглядит так:
            $$
            H(p_1q_{11};p_1q_{12};\ldots;p_1q_{1n};p_2q_{21};\ldots;p_nq_{nm})=
            H(p_1;p_2;\ldots;p_n)+\sum\limits_{i=1}^nH(q_{i1};q_{i2};\ldots;q_{im_i})
            $$
            Это называется линейность энтропии.\\
            Третья аксиома немного проще формулируется, чем первые две, но достаточно туманна. Смотрите, если мы немного поменяем вероятности, у нас же неопределённость тоже немного поменяется. Поэтому $H$ --- непрерывная функция. Что такое непрерывная функция переменного количества аргументов --- хз, так что будем считать, что это значит, что при фиксированном количестве она непрерывна.
        \end{Comment}
        \begin{Comment}
            Начнём экспериментировать с $h$. Она возрастает. Попробуем воспользоваться формулой суммирования, $p_i=\frac1n$, $q_{ij}=\frac1m$. Что получим? $h(mn)=h(n)+\sum\limits_{i=1}^n\frac1n h(m)=h(n)+h(m)$. Какие функции с таким свойством мы знаем? Ну, очевидно, логарифм. Утверждается, что ничего, кроме $\mathrm{const}\cdot\ln x$ не подходит. Сначала заметим, что $h(1)=0$ (если результат всего один, то неопределённости нет). Чему равно $h(2)$? Ну, хз. Давайте обозначим его за $c$. Тогда утверждается, что $h(n)=c\log_2n$.
        \end{Comment}
        \thm $h(n)=h(2)\log_2n$.
        \begin{Proof}
            Мы знаем, что $h(1)=0$, $h(2)=c$, $h(mn)=h(n)+h(m)$. Сначала заметим, что $h(2^k)=kc$. Это легко доказывается по индукции: $h(2^k)=h(2\cdot2^{k-1})=c+h(2^{k-1})=c+(k-1)c=kc$. Теперь рассмотрим $h(n)$. Возьмём произвольное целое $t$. Заметим, что $2^i\leqslant n^t<2^{i+1}$. Тогда по монотонности $h$, $ic\leqslant t\cdot h(n)<(i+1)c$. А значит $\frac{ci}t\leqslant h(t)<\frac{c(i+1)}t$. Также несложно понять, что этим же свойством обладает $c\log_2n$. А это значит, что $\forall t\in\mathbb N~|h(n)-c\log_2n|<\frac cn$. Не значить ли это то, что нам нужно.
        \end{Proof}
        \begin{Comment}
            Давайте теперь попробуем, зная линейность и доказанную лемму, узнать что-то про $H$. Пусть $p_i\in\mathbb Q$. Также приведём все $n$ штук к общему знаменателю $b$. То есть мы хотим посчитать $H\left(\frac{a_1}b;\frac{a_2}b;\ldots;\frac{a_n}b\right)$. Давайте выберем $m_i=a_i$, $q_{ij}=\frac1{m_i}$. Тогда мы получаем $p_iq_{ij}=\frac1b$. Тогда по формуле мы получаем
            $$
            H(\frac1b;\ldots;\frac1b)=H(p_1;\ldots;p_n)+\sum\limits_{i=1}^np_iH(\frac1{a_i};\frac1{a_i};\ldots;\frac1{a_i})
            $$
            Это равносильно
            $$
            c\log_2(b)=H(p_1;\ldots;p_n)+\sum\limits_{i=1}^n c\log_2(a_i)
            $$
            Что равносильно
            $$
            H(p_1;\ldots;p_n)=-\sum\limits_{i=1}^n c\log_2(a_i)+\sum_{i=1}^ncp_i\log_2(b)=-c\left(\sum\limits_{i=1}^n p_i(\log_2(a_i)-\log_2(b))\right)=-c\sum\limits_{i=1}^np_i\log_2p_i
            $$
        \end{Comment}
        \thm Для $H(p_1;\ldots;p_n)=-h(2)\sum\limits_{i=1}^np_i\log_2p_i$.
        \begin{Proof}
            Для рациональных уже указано выше, для иррациональных пользуемся непрерывностью.
        \end{Proof}
        \begin{Comment}
            Вопрос --- а что такое $c$? Ну, $h(2)$, это да, но вопрос в том, какой у этого физический смысл. А это единица измерения. То есть если вы переведёте всё из джоулей м электрон-вольты, то у вас всё домножится на константу. Вот эта константа у нас и есть. И её обычно принимают равной единице, чтобы получить известную нам единицу измерения информации --- бит.
        \end{Comment}
        \dfn Бит --- единица измерения информации, у которой $h(2)=1$.
        \dfn Энтропия Шеннона --- $H(p_1;\ldots;p_n)=-\sum\limits_{i=1}^np_i\log_2p_i$.
        \begin{Comment}
            Почему тут минус в начале, хотя величина должна бы быть положительной? Потому что логарифмы отрицательные.
        \end{Comment}
        \begin{Comment}
            Что мы глобально сделали? Мы ввели величину для измерения количества информации. Ввели три логичных аксиомы для информации, получили, что несмотря на их размытость, они дают только одну функцию с точности до константы.\\
            А нахера всё это нужно-то? А вот можно вспомнить предыдущий семестр, где было кодирование. А точнее возьмём арифметическое кодирование. У нас был алфавит $\Sigma=\{c_1;\ldots;c_n\}$ и текст $t=t_1t_2\cdots t_L$ на базе этого алфавита. После этого мы берём $f_i$ --- количество вхождений $c_i$ в $t$, берём $p_i=\frac{f_i}L$. Как можно смотреть на $p_i$? Как будто мы берём случайный символ нашего текста и смотрим на то, с какой вероятностью он равен $c_i$. Как происходит кодирование? Мы делим отрезок в пропорции $p_1:p_2:\cdots:p_n$, берём правильный отрезок, и уже делим его в нужном нам соотношении. В конце мы получаем отрезок $[a;b]$, в котором выбираем двоичное рациональное число вида $\frac p{2^q}$, дополняем $p$ лидирующими нулями до $q$ знаков и записываем его в ответ. Вспомним, какая тут была оценка. Длина отрезка $[a;b]$ была $\left(\prod_{i=1}^np_i^{p_i}\right)^L$, а $\frac1{2^q}$ меньше либо равно этой длины. А это значит, что $q\geqslant-L\sum\limits_{i=1}^np_i\log_2p_i$. То есть тут мы как будто берём случайный источник символов с вероятностями $p_i$ и в среднем на один символ мы тратим энтропию Шеннона бит. При чём по сути мы столько информации и получаем, мы получаем энтропию Шеннона бит на символ. Поэтому, собственно, арифметическое кодирование --- это лучший алгоритм, не учитывающий взаимное расположение символов.
        \end{Comment}
        \begin{Comment}
            Хорошо, а что если мы вдруг берём не случайный источник, а что-то другое. Вот, например, сколько информации даёт нам <<Война и мир>>? Ну, это не случайный источник, так что непонятно. Или вот программа решения какой-то задачи тоже даёт нам информацию (как решать задачу, собственно). Но программа уж ещё более явно не случайный источник. В домашнем задании мы свяжем энтропию Шеннона с Колмогоровской сложностью. Колмогоров тоже подумал, что разумные источники не описываются энтропией Шеннона. И придумал альтернативную теорию информации. Пусть у нас есть некоторая программа $K$, которую мы называем <<разархиватор>>. И вот Колмогоровская сложность слова $x$ --- минимальная длина слова $y$, что $K(y)=x$. Тут сложно вообще понять, что такое разархиватор, не виснет ли программа и т.д. Есть ещё другое определение, когда мы берём компилятор и сложность --- минимальная длины программы $p$, что $p()=x$. Это, по сути, то же самое.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Марковские цепи}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Что такое марковская цепь? Это нечто, что описывает случайный процесс. Вот у нас были монетки, которые мы подкидывали много раз, и каждый раз никак не зависит от предыдущих бросков. То есть процесс, в некотором смысле <<без памяти>>. Понятно, что не все процессы так устроены. Часто бывает некоторое состояние, от которого зависит то, что будет дальше. Это состояние мы будем изображать графами. Например, если игра состоит в том, что два игрока подкидывают монету, и если дважды подряд выпала единица, побеждает первый, а если дважды подряд ноль --- то второй. Марковская цепь этого выглядит как-то так:
            \begin{center}
                \begin{tikzpicture}[scale=1.5]
                    \node[circle, draw] (0) {0};
                    \node[circle, draw] at ($(0)+(1,-1)$) (1) {1};
                    \node[circle, draw] at ($(0)+(-1,-1)$) (2) {2};
                    \node[circle, draw] at ($(1)+(2,0)$) (3) {3};
                    \node[circle, draw] at ($(2)-(2,0)$) (4) {4};
                    \draw[->] (0) -- (1) node[midway,above,sloped] {0.5};
                    \draw[->] (0) -- (2) node[midway,above,sloped] {0.5};
                    \draw[->] (1) -- (3) node[midway,above] {0.5};
                    \draw[->] (2) -- (4) node[midway,above] {0.5};
                    \draw[->] (3) to[out=135, in=45, looseness=10] node[midway,above] {1} (3);
                    \draw[->] (4) to[out=135, in=45, looseness=10] node[midway,above] {1} (4);
                    \draw[->] (2) to[out=20, in=160, looseness=1] node[midway,above] {0.5} (1);
                    \draw[->] (1) to[out=-160, in=-20, looseness=1] node[midway,below] {0.5} (2);
                \end{tikzpicture}
            \end{center}
            Где 3 --- победа первого, а 4 --- победа второго.
        \end{Comment}
        \dfn \undercolor{red}{Марковская цепь} --- это пара, из конечного числа состояний и вероятности перехода для каждой пары состояний (которые обычно в матрице записываются). При этом $\forall i~\sum\limits_{i=1}^n p_{ij}=1$, где $p_{ij}$ --- вероятность перехода из $i$ в $j$. Матрица, удовлетворяющая такому свойству --- \undercolor{red}{стохастическая}.
        \begin{Comment}
            Это, собственно, и есть взвешенный граф --- множество вершин-состояний и рёбер-переходов. При этом переходы с нулевой вероятностью обычно не изображаются на этом графе.
        \end{Comment}
        \begin{Example}
            Матрица для уже нарисованного нами графа игры выглядит так:
            $$
            \matr{
                0 & \frac12 & \frac12 & 0 & 0\\
                0 & 0 & \frac12 & \frac12 & 0\\
                0 & \frac12 & 0 & 0 & \frac12\\
                0 & 0 & 0 & 1 & 0\\
                0 & 0 & 0 & 0 & 1
            }
            $$
        \end{Example}
        \begin{Comment}
            Если пытаться говорить более формально, то получится, что $p_{ij}=P(X_{k+1}=j\mid X_k=i)$, где $X_k$ --- наше текущее состояние.
        \end{Comment}
        \begin{Comment}
            Вернёмся к нашей игре. Когда мы пришли в состояние 4 либо 3, то выйти из них мы не сможем. Это --- поглощающее состояние.
        \end{Comment}
        \dfn \undercolor{red}{Поглощающее состояние} --- такое состояние $x$, что $p_{xx}=1$.
        \begin{Comment}
            Поглощающее состояние обозначает, что мы заканчиваем нашу симуляцию/игру/etc.
        \end{Comment}
        \begin{Comment}
            Прежде чем поговорить о применении марковских цепей или о поглощающий состояниях, надо научиться как-то взаимодействовать с цепями. Для этого введём такой термин как случайное блуждание. Пусть у нас есть вероятность того, что мы в какой-то момент находимся в каждом из состояний (то есть вектор из $n$ элементов возьмём, который сам по себе является распределением). Вот пусть, например, у нас есть наш вектор вероятностей --- это $b_0=\matr{\frac12 & \frac12 & 0}$. А матрица выглядит так: $\matr{\frac12 & 0 & \frac12\\\frac13 & 0 & \frac23\\0 & 1 & 0}$. Тогда чему \textbf{на следующем шаге} будет равна вероятность находиться в состоянии 1? Ну, мы либо были в 1 и перешли в него же, либо были в 2 и перешли в 1, либо были в 3 и перешли в 1. Это $\frac12\cdot\frac12+\frac12\cdot\frac13+0\cdot0=\frac5{12}$. Аналогично можно посчитать вероятность нахождения во втором или третьем состоянии. И мы получим $b_1=\matr{\frac5{12} & 0 & \frac7{12}}$. Как нам в общем случае посчитать $b_1$, если у нас есть $b_0$ и $P$. Ну, мы имеем по сути $b_1^{(0)}=\sum\limits_{j=1}b^{(0)}_0P_{ji}$, то есть $b_1=b_0P$. И вообще $b_k$ --- это распределение вероятностей на $k$-том шаге --- это $b_0P^k$.
        \end{Comment}
        \begin{Comment}
            Давайте перенумеруем нашу матрицу так, чтобы в начале шли не поглощающие состояния, а потом --- поглощающие. Переходы между не поглощающими состояниями --- они какие-то. Обозначим тот кусок $P$, который отвечает за переходы из не поглощающих состояний в не поглощающие, за $Q$. Про переходы из не поглощающих в поглощающие мы тоже ничего не знаем --- это $R$. А вот про последние $n-k$ строк мы всё знаем. Матрица перехода выглядит так:
            $$
            \arr{c|c}{
                Q & R\\
                \hline
                \mathbb0 & E
            }
            $$
            Где $E$ --- единичная матрица, а $\mathbb0$ --- нулевая.\\
            Мы возводим эту матрицу в степень. Причём нам интересно, а каком поглощающем состоянии мы закончим, то есть вообще нам интересно $P^\infty$.\\
            Давайте рассмотрим $P^2=\matr{Q&P\\\mathbb0&E}\matr{Q&P\\\mathbb0&E}=\matr{Q^2+P\mathbb0 & QR+RE \\\mathbb0Q+\mathbb0E & \mathbb0R+E^2}=\matr{Q^2 & QR+R\\\mathbb0 & E}$. Почему так законно умножать блоки как будто это элементы матрицы --- вопрос не настолько элементарный, но всё же так можно, что доказывается не так уж и сложно. Впрочем, нам интересно не это, а физический смысл $QR+R$. Это, как мы знаем, переходы из не поглощающего состояния в поглощающее за 2 шага. Ну, это логично, $R$ --- сразу пришли в поглощающее, а $QR$ --- переход сначала в не поглощающее, а из него в поглощающее.\\
            А вообще по индукции легко доказать, что $P^k=\matr{Q^k & R+QR+Q^2R+\cdots+Q^{k-1}R\\\mathbb0 & E}$. Что будет, если мы $k$ устремим к $\infty$. И сейчас мы будем говорить, когда этот предел вообще существует. Сначала надо понять, придём ли мы вообще когда-нибудь в поглощающее состояние? То есть не застопоримся ли в не поглощающих. Для этого посмотрим, стремится ли к нулю вероятность остаться в не поглощающих состояниях. Ну, в общем случае нет, ведь поглощающий состояний может в принципе не быть. Про отсутствие поглощающих состояний мы поговорим не сейчас, но тем не менее. Впрочем, даже если поглощающее состояние есть, то не обязательно мы в него придём. Можно рассмотреть патологический пример:
            \begin{center}
                \begin{tikzpicture}[scale=1.5]
                    \node[circle, draw] (0) {};
                    \node[circle, draw] at ($(0)+(1,-1)$) (1) {};
                    \node[circle, draw] at ($(0)+(-1,-1)$) (2) {};
                    \draw[->] (0) to[out=135, in=45, looseness=10] node[midway,above] {1} (0);
                    \draw[->] (2) to[out=20, in=160, looseness=1] node[midway,above] {0.5} (1);
                    \draw[->] (1) to[out=-160, in=-20, looseness=1] node[midway,below] {0.5} (2);
                \end{tikzpicture}
            \end{center}
            В некотором смысле пару состояний снизу можно рассмотреть как поглощающие вместе, но это опять же не сейчас.
        \end{Comment}
        \dfn \undercolor{red}{Поглощающая марковская цепь} --- цепь, в которой для любой вершины существует путь до некоторой поглощающей, причём каждое ребро пути имеет ненулевую вероятность.
        \thm \undercolor{darkgreen}{Теорема о поглощении}. Поглощающая марковская цепь поглощается с вероятностью 1. Говоря более понятным языком, для любого вектора $b_0$ $b_0P^k$ стремится к вектору, у которого все состояния поглощающие.
        \begin{Proof}
            Нам достаточно показать, что $Q^k\underset{k\to\infty}\rightarrow\mathbb0$.\\
            Рассмотрим $P^n$, где $n$ --- количество состояний. Это, как мы знаем, $\matr{Q^n & (1+Q+\cdots+Q^{n-1})R\\\mathbb0 & E}$. Как мы знаем, матрица стохастическая, то есть сумма в каждой строке равна единице. Также несложно заметить, что в каждой строчке матрицы $(1+Q+\cdots+Q^{n-1})R$ есть ненулевой элемент, потому что у нас в из каждого состояния есть путь в какое-то поглощающее. А это значит, что для данной строки матрицы $Q$ сумма элементов в ней меньше либо равна некоторого $q<1$.\\
            Давайте докажем, что сумма в каждой строке $Q^{2n}$ меньше либо равна $q^2$. Это докажем нам теорему, потому что будем очевидно, что каждая строка $Q$ стремится к нулевой, а значит и вся $Q$ --- тоже. Итак,
            $$
            \sum\limits_{j=1}^nq^{(2n)}_{ij}=\sum\limits_{j=1}^n\sum\limits_{k=1}^nq^{(n)}_{ik}q^{(n)}_{kj}=\sum\limits_{k=1}^nq^{(n)}_{ik}\sum\limits_{j=1}^nq^{(n)}_{kj}
            $$
            Обе части произведения --- сумма в некоторой строке $Q^n$, а значит оба меньше либо равны $q$.
        \end{Proof}
        \begin{Proof}
            Ну, хорошо, а теперь нам интересно, а в каком именно поглощающем состоянии мы окажемся через $\infty$ шагов? Для этого у нас есть правая часть.
        \end{Proof}
        \dfn \undercolor{red}{Фундаментальная матрица} --- $N=\sum\limits_{i=0}^\infty Q^i$.
        \thm $N=(E-Q)^{-1}$.
        \begin{Proof}
            Для этого докажем, что $\left(\sum\limits_{i=0}^kQ^i\right)(E-Q)\underset{k\to\infty}\rightarrow E$. Ну, лол, это доказывается также, как сумма геометрической прогрессии. Раскройте и получите.
        \end{Proof}
        \begin{Comment}
            Ну, хорошо, $P^k\rightarrow\matr{\mathbb0 & NR\\\mathbb0 & E}$. За что отвечает $NR$? А на самом деле $(NR)_{ij}$ --- вероятность из $i$-того не поглощающего состояния перейти в $j$-тое поглощающее.
        \end{Comment}
        \dfn \undercolor{red}{Время поглощения} --- случайная величина, равная количеству шагов до прихода в поглощающее состояние из данного начального распределения $b_0$.
        \begin{Comment}
            Тогда $ET=\sum\limits_{k=0}^\infty P(T>k)$. Это вероятность того, что мы за $k$ шагов не пришли в поглощающее состояние. Давайте считать, что $b_0$ имеет размерность не $n$, а количество не поглощающих состояний, потому что поглощающие нам не интересны. Итого $ET=\sum\limits_{t=0}^\infty b_0Q^t\matr{1\\1\\\vdots\\1}$. Последняя штука нужна, чтобы просуммировать $k$ компонент вектора. А эта сумма, как несложно заметить, равна $b_0N\matr{1\\\vdots\\1}$.
        \end{Comment}
        \begin{Comment}
            А сейчас поговорим о том, как устроены произвольные цепи, не обязательно поглощающие.\\
            Рассмотрим, например, вот такую цепь:
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw] (1) {$1$};
                    \node[circle,draw,right=of 1] (2) {$2$};

                    \draw[->] (1) to[out=135,in=-135,looseness=5] node[midway,left] {$0.5$} (1);
                    \draw[->] (2) to[out=-45,in=45,looseness=5] node[midway,right] {$0.5$} (2);
                    \draw[->] (1) to[out=45,in=135,looseness=1] node[midway,above] {$0.5$} (2);
                    \draw[->] (2) to[out=-135,in=-45,looseness=1] node[midway,below] {$0.5$} (1);
                \end{tikzpicture}
            \end{figure}\noindent
            Пусть исходное распределение было $b_0=\matr{a & 1-a}$. Тогда чему будет равно $b_1$? Ну, $b_1=\matr{a & 1-a}\matr{0.5 & 0.5\\0.5 & 0.5}=\matr{0.5 & 0.5}$. То есть цепь полностью <<забыла>>, что с ней было раньше. Давайте тогда немного нашу цепь модифицируем:
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw] (1) {$1$};
                    \node[circle,draw,right=of 1] (2) {$2$};

                    \draw[->] (1) to[out=135,in=-135,looseness=5] node[midway,left] {$0.7$} (1);
                    \draw[->] (2) to[out=-45,in=45,looseness=5] node[midway,right] {$0.7$} (2);
                    \draw[->] (1) to[out=45,in=135,looseness=1] node[midway,above] {$0.3$} (2);
                    \draw[->] (2) to[out=-135,in=-45,looseness=1] node[midway,below] {$0.3$} (1);
                \end{tikzpicture}
            \end{figure}\noindent
            Что получится тогда? Тогда будет $b_1=\matr{a & 1-a}\matr{0.7 & 0.3\\0.3 & 0.7}=\matr{0.3-0.4a & 0.7+0.4a}$. Что у нас было исходно? Мы с вероятностью $a$ были слева, а с $1-a$ --- справа. А после этого наш отрезок сжался в $[0.3;0.7]$, и теперь такая же пропорция там
            \begin{figure}[H]
                \begin{tikzpicture}[scale=5]
                    \draw[thick,red] (0,0) -- (.4,0);
                    \draw[thick,blue] (.4,0) -- (1,0);
                    \draw (0,-.05) -- (0,.05);
                    \draw (1,-.05) -- (1,.05);
                    
                    \draw (0,0) arc (120:60:.4) node[midway,above] {$a$};
                    \draw (.4,0) arc (120:60:.6) node[midway,above] {$1-a$};
                \end{tikzpicture}\\
                \begin{tikzpicture}[scale=5]
                    \draw[thick,red] (0,0) -- (.3+.4*.4,0);
                    \draw[thick,blue] (.3+.4*.4,0) -- (1,0);
                    \draw (0,-.05) -- (0,.05);
                    \draw (1,-.05) -- (1,.05);
                    
                    \draw (.3,0) arc (120:60:.4*.4) node[midway,above] {$a$};
                    \draw (.3+.4*.4,0) arc (120:60:.6*.4) node[midway,above] {$1-a$};
                    \draw (.3,0) -- (.3,.02);
                    \draw (.7,0) -- (.7,.02);
                    \node[below] at (.3,0) {$0.3$};
                    \node[below] at (.7,0) {$0.7$};
                \end{tikzpicture}
            \end{figure}\noindent
            А что же будет дальше? $b_2=\matr{0.3-0.4a & 0.7+0.4a}\matr{0.7 & 0.3\\0.3 & 0.7}=\matr{0.42+0.16a & 0.58-0.16a}$. То есть теперь всё съехало ещё сильнее внутрь.\\
            В нашем первом случае всё было логично, что она сразу всё забыла. А вот тут ситуация похожая, но только цепь забывает всё постепенно. Несложно заметить, что тут, если устремить $n$ к $\infty$, то $b_n=b_0P^n$ будет стремиться к $\matr{0.5 & 0.5}$. Причём это предельное распределение никак не зависит от $b_0$.\\
            Понятно, что таким свойством обладает не любая цепь. Вот такая, например, не обладает.
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw] (1) {$1$};
                    \node[circle,draw,right=of 1] (2) {$2$};
                    \node[circle,draw,below=of 1] (3) {$3$};
                    \node[circle,draw,below=of 2] (4) {$4$};
                    
                    \draw[->] (1) -- (3) node[midway,left] {$0.8$};
                    \draw[->] (2) -- (4) node[midway,right] {$0.8$};
                    \draw[->] (1) -- (4) node[midway,sloped,above left] {$0.2$};
                    \draw[->] (2) -- (3) node[midway,sloped,below right] {$0.2$};
                    \draw[->] (3) to[out=-135,in=-45,looseness=5] node[midway,below] {$1$} (3);
                    \draw[->] (4) to[out=-135,in=-45,looseness=5] node[midway,below] {$1$} (4);
                \end{tikzpicture}
            \end{figure}\noindent
            Если взять $b_0=\matr{1 & 0 & 0 & 0}$ и $b_0=\matr{0 & 1 & 0 & 0}$, то результаты получатся разными ($\matr{0 & 0 & 0.8 & 0.2}$ и $\matr{0 & 0 & 0.2 & 0.8}$ соответственно). Чтобы проанализировать, зависит ли предельное распределение от начального состояния, нам понадобится немного теории графов.
        \end{Comment}
        \dfn Вершины называются \undercolor{red}{сильно связными}, если из первой можно дойти до второй, а из второй в первую.
        \thm Сильная связность является отношением эквивалентности.
        \dfn Классы эквивалентности сильной связности называются \undercolor{red}{компонентами сильной связности}.
        \dfn Рассмотрим марковскую цепь и оставим только положительные переходы. \undercolor{red}{Эргодическим классом} называется компонента сильной связности графа марковской цепи.
        \begin{Example}
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw] (1) {\phantom1};
                    \node[circle,draw,right=of 1] (3) {\phantom3};
                    \node[circle,draw,below=of 1] (2) {\phantom2};
                    \node[circle,draw,right=of 2] (4) {\phantom4};

                    \draw[->] (1) -- (3) node[midway,above] {$0.3$};
                    \draw[->] (2) -- (4) node[midway,below] {$0.3$};
                    \draw[->] (1) to[out=-45,in=45,looseness=.75] node[midway,right] {$0.2$} (2);
                    \draw[->] (2) to[out=135,in=-135,looseness=.75] node[midway,left] {$0.2$} (1);
                    \draw[->] (1) to[out=45,in=135,looseness=5] node[midway,above] {$0.5$} (1);
                    \draw[->] (2) to[out=-45,in=-135,looseness=5] node[midway,below] {$0.5$} (2);
                    \draw[->] (3) to[out=45,in=-45,looseness=5] node[midway,right] {$1$} (3);
                    \draw[->] (4) to[out=45,in=-45,looseness=5] node[midway,right] {$1$} (4);
                \end{tikzpicture}
                \caption*{Тут три эргодических класса.}
            \end{figure}\noindent
        \end{Example}
        \begin{Example}
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw] (1) {\phantom1};
                    \node[circle,draw,right=of 1] (3) {\phantom3};
                    \node[circle,draw,below=of 1] (2) {\phantom2};
                    \node[circle,draw,right=of 2] (4) {\phantom4};
                    
                    \draw[->] (1) -- (3) node[midway,above] {$0.3$};
                    \draw[->] (2) -- (4) node[midway,below] {$0.3$};
                    \draw[->] (1) to[out=-45,in=45,looseness=.75] node[midway,right] {$0.2$} (2);
                    \draw[->] (2) to[out=135,in=-135,looseness=.75] node[midway,left] {$0.2$} (1);
                    \draw[->] (1) to[out=45,in=135,looseness=5] node[midway,above] {$0.5$} (1);
                    \draw[->] (2) to[out=-45,in=-135,looseness=5] node[midway,below] {$0.5$} (2);
                    \draw[->] (3) to[out=45,in=135,looseness=5] node[midway,above] {$0.5$} (3);
                    \draw[->] (4) to[out=-45,in=-135,looseness=5] node[midway,below] {$0.5$} (4);
                    \draw[->] (3) to[out=-45,in=45,looseness=.75] node[midway,right] {$0.5$} (4);
                    \draw[->] (4) to[out=135,in=-135,looseness=.75] node[midway,left] {$0.5$} (3);
                \end{tikzpicture}
                \caption*{А тут два.}
            \end{figure}\noindent
        \end{Example}
        \dfn Эргодический класс называется \undercolor{red}{поглощающим}/\undercolor{red}{существенным}/\undercolor{red}{терминальным}, если из него не ведёт рёбер иначе он называется \undercolor{red}{непоглощающим}/\undercolor{red}{несущественным}/\undercolor{red}{промежуточным}.
        \thm По определению у поглощающей марковской цепи все существенные классы имеют размер 1.
        \begin{Example}
            Рассмотрим вот какую цепь:
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw] (1) {\phantom1};
                    \node[circle,draw,right=of 1] (3) {\phantom3};
                    \node[circle,draw,above=of $(1)!.5!(3)$] (2) {\phantom2};
                    
                    \draw[->] (1) -- (2) node[midway,above left] {$1$};
                    \draw[->] (2) -- (3) node[midway,above right] {$1$};
                    \draw[->] (3) -- (1) node[midway,below] {$1$};
                \end{tikzpicture}
            \end{figure}\noindent
            Как утроена матрица перехода? $P=\matr{0&1&0\\0&0&1\\1&0&0}$. Получается, что если $b_0=\matr{a&b&c}$, то $b_1=\matr{c&a&b}$, $b_2=\matr{b&c&a}$, $b_3=b_0=\matr{a&b&c}$. То есть каждый следующий вектор --- циклический сдвиг предыдущего. А с точки зрения эргодических классов, это всё один класс.\\
            Самое интересное, что эта цепь не забывает изначальное состояние, но более того она помнит остаток количества переходов от деления на 3.
        \end{Example}
        \begin{Example}
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw] (2) {$2$};
                    \node[circle,draw,below=of 2] (3) {$3$};
                    \node[circle,draw,left=of $(2)!.5!(3)$] (1) {$1$};
                    \node[circle,draw,right=of $(2)!.5!(3)$] (4) {$4$};
                    
                    \draw[->] (1) -- (2) node[midway,above] {$0.5$};
                    \draw[->] (1) -- (3) node[midway,below] {$0.5$};
                    \draw[->] (2) -- (4) node[midway,above] {$1$};
                    \draw[->] (3) -- (4) node[midway,below] {$1$};
                    \draw[->] (4) to[out=-90,in=-90,looseness=1.5] node[midway,below] {$1$} (1);
                \end{tikzpicture}
            \end{figure}\noindent
            $P=\matr{
                0 & 0.5 & 0.5 & 0\\
                0 & 0 & 0 & 1\\
                0 & 0 & 0 & 1\\
                1 & 0 & 0 & 0
            }$. Пусть $b_0=\matr{1 & 0 & 0 & 0}$. Тогда $b_1=\matr{0&0.5&0.5&0}$, $b_2=\matr{0&0&0&1}$, $b_3=b_0$. То есть эта цепь тоже помнит остаток от деления на 3.
        \end{Example}
        \dfn \undercolor{red}{Периодом эргодического класса} называется Н.О.Д. длин всех его циклов.
        \dfn Эргодический класс называется \undercolor{red}{периодическим}, если его период больше единицы.
        \dfn Эргодический класс называется \undercolor{red}{непериодическим}, если его период равен единицы или не определён.
        \dfn Марковская цепь называется \undercolor{red}{эргодической}, если она состоит из одного эргодического класса, который не является периодическим.
        \thm \undercolor{darkgreen}{Эргодическая теорема для марковских цепей} или \undercolor{darkgreen}{теорема о забывании}. Пусть $P$ --- матрица переходов эргодической цепи. Тогда $\exists b*~\forall b_0\text{ --- начальное распределение}~\lim\limits_{m\to\infty}b_0P^m=b^*$.
        \begin{Comment}
            То есть цепь в пределе забывает распределение.
        \end{Comment}
        \begin{Comment}
            Мы это не будем доказывать полностью, а докажем lite версию теоремы, а дальше дадим комментарии, как распространять на общий случай.
        \end{Comment}
        \dfn Марковская цепь \undercolor{red}{регулярна}, если $\forall i,j~p_{ij}>0$.
        \thm Регулярная цепь является эргодической.
        \begin{Proof}
            Очевидно, что она состоит из одного класса, а среди циклов есть петли --- циклы длины 1.
        \end{Proof}
        \thm \undercolor{darkgreen}{Эргодическая теорема для регулярных цепей}. Пусть $P$ --- матрица переходов \textbf{регулярной} цепи. Тогда $\exists b^*~\forall b_0\text{ --- начальное распределение}~\lim\limits_{m\to\infty}b_0P^m=b^*$.
        \begin{Proof}
            Пусть $P^*=\lim\limits_{m\to\infty}P^m$. Тогда $b^*=b_0P^*$. Тогда $b_i^*=\sum\limits_{j=1}^n(b_0)_jP^*_{ij}$. Тогда нам подходит такое $P^*$, что $\forall j~P^*_{ij}=b^*_i$, потому что тогда $\sum\limits_{j=1}^n(b_0)_jb^*_i=b^*_i\sum\limits_{j=1}^n(b_0)_j=b^*_i$. То есть если мы окажем, что $P^*$ действительно равно тому, что написано, то получится что нам хочется. Пусть $\delta_m^i=\min\limits_{j\in[1:n]}P^m_{ji}$, $\Delta_m^i=\max\limits_{j\in[1:n]}P^m_{ji}$. Тогда нам хочется доказать, что $\Delta_m^i-\delta_m^i\underset{m\to\infty}\longrightarrow0$, тогда это даст нам то, что $P^*$ имеет именно нужный нам вид, а значит и доказательство теоремы.\\
            Обозначим $\varepsilon=\min_{i,j}p_{ij}$. Посмотрим вот на что. $P^{m+1}_{ji}=\sum\limits_{k=1}^nP_{jk}P^m_{ki}$. Выделим отсюда максимум:
            $$
            P^{m+1}_{ji}=\sum\limits_{\substack{k=1\\k\neq k_{\max}}}^nP_{jk}P^m_{ki}+P_{jk_{\max}}\Delta_m^i\geqslant
            \sum\limits_{\substack{k=1\\k\neq k_{\max}}}^nP_{jk}\delta_m^i+P_{jk_{\max}}\Delta_m^i
            $$
            Ещё выделим минимум:
            $$
            P^{m+1}_{ji}=\sum\limits_{\substack{k=1\\k\neq k_{\min}}}^nP_{jk}P^m_{ki}+P_{jk_{\min}}\delta_m^i\leqslant
            \sum\limits_{\substack{k=1\\k\neq k_{\min}}}^nP_{jk}\Delta_m^i+P_{jk_{\min}}\delta_m^i
            $$
            Давайте к первому неравенству прибавим и вычтем $\delta_m^i$:
            $$
            P^{m+1}_{ji}=\geqslant
            \sum\limits_{k=1}^nP_{jk}\delta_m^i+P_{jk_{\max}}(\Delta_m^i-\delta_m^i)\geqslant\delta_m^i+\varepsilon(\Delta_m^i-\delta_m^i)
            $$
            Аналогично из второго неравенства:
            $$
            P^{m+1}_{ji}=\leqslant\Delta_m^i-\varepsilon(\Delta_m^i-\delta_m^i)
            $$
            Вычитая неравенства друг из друга, получаем
            $$
            \delta_{m+1}^i\geqslant\delta_m^i+\varepsilon(\Delta_m^i-\delta_m^i)
            $$
            И
            $$
            \Delta_{m+1}^i\leqslant\Delta_m^i-\varepsilon(\Delta_m^i-\delta_m^i)
            $$
            А это в свою очередь значит что
            $$
            \Delta_{m+1}^i-\delta_{m+1}^i\leqslant(1-2\varepsilon)(\Delta_m^i-\delta_m^i)\leqslant(1-2\varepsilon)^m(\Delta_0^i-\delta_0^i)\leqslant(1-2\varepsilon)^m\to0
            $$
        \end{Proof}
        \begin{Comment}
            Что делать в общем случае делать-то? Ну, перебирать случаи. Если наша цепь произвольная эргодическая, то существует такое число $k$, что $P^k$ даёт нам матрицу регулярной цепи. B дальше можно применить теорему для регулярных цепей, получить, что ${P^k}^m$ имеет предел, который состоит из одинаковых строк (а такой матрице пофиг вообще, на что вы её умножаете).\\
            Если наца цепь вообще не эргодическая, то нужно две фазы анализа. Первое --- сжать эргодические классы до вершин (то есть конденсируем граф). Получаем поглощающую цепь, анализируем её. То есть для каждого существенного эргодического класса мы имеем вероятность поглощения в нём. Второе --- внутри каждого класса смотрим на то, периодический этот класс или нет. Если непериодический, то с ним всё понятно, его нужно проанализировать как эргодическую цепь, а результирующее распределение умножить на вероятность поглощения в нём. Если же он периодический, то там печально. Там картина повторяется с периодом, важно становится, на каком шаге и в какую вершину произошло поглощение. Это тоже можно анализировать, но так красиво и просто там не получается.
        \end{Comment}
    \end{itemize}
    \section{Формальные языки.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Сначала немного повторения прошлого семестра.
        \end{Comment}
        \dfn Некоторое конечное непустое множество называется \undercolor{red}{алфавитом} и обозначается $\Sigma$.
        \dfn $\Sigma^*=\bigcup\limits_{i=0}^\infty\Sigma^i$ --- \undercolor{red}{множество всех строк}.
        \dfn $\cdot\colon\Sigma^*\times\Sigma^*\to\Sigma^*$ --- \undercolor{red}{конкатенация} (для её определения почитайте конспекты первого семестра).
        \thm $\Sigma^*$ с конкатенацией --- свободный моноид.
        \begin{Comment}
            Отсюда уже идут новые определения.
        \end{Comment}
        \dfn \undercolor{red}{Формальный язык} --- это некоторое подмножество $\Sigma^*$.
        \begin{Example}
            Пустой язык, например, $\varnothing$. Больше про него нечего сказать.
        \end{Example}
        \begin{Example}
            Ещё более интересный пример --- $\Sigma^*$.
        \end{Example}
        \begin{Example}
            Давайте возьмём некоторое конечное множество слов. Например, $\Sigma^*=\{\mathrm a;\mathrm b;\ldots;\mathrm z\}$, $L=\{\mathrm{itmo};\mathrm{dm}\}$.
        \end{Example}
        \begin{Example}
            Теперь чуть более интересный пример: $\Sigma=\{\mathrm a;\mathrm b\}$ $\{\varepsilon;\mathrm a;\mathrm{aa};\mathrm{aaa};\ldots\}$.
        \end{Example}
        \begin{Example}
            Ещё язык можно правилом описать. Например таким правилом: после каждой буквы $\mathrm a$ находится буква $\mathrm b$. Тогда $L=\{\varepsilon;\mathrm b;\mathrm{ab};\mathrm{bb};\mathrm{abb};\ldots\}$.
        \end{Example}
        \begin{Example}
            Снова берём $\Sigma=\{\mathrm a;\mathrm b\}$, а $L$ состоит из палиндромов: $L=\{\varepsilon;\mathrm a;\mathrm b;\mathrm{aa};\mathrm{bb};\ldots\}$.
        \end{Example}
        \begin{Example}
            Ещё необязательно говорить что-то про структуру этих слов. Они могут что-то кодировать, а говорить мы будем про объекты. Например, мы можем взять $\Sigma=\{0;1\}$, а слова из $\Sigma^*$ интерпретировать как двоичную запись натурального числа. И тогда никто не мешает нам взять, например, множество слов, кодирующих число, кратное трём. Или мы можем взять перестановки, занумеровать их и взять, там, номера перестановок из десяти элементов, в которых нет неподвижных точек.
        \end{Example}
        \begin{Comment}
            То есть формальные языки кодируют вообще произвольные множества, а не только те, которые мы можем нормально описать в терминах букв.
        \end{Comment}
        \begin{Example}
            Мы можем взять хоть множество двоичных записей чисел $n$, что $\exists x;y;z\in\mathbb N~x^2+y^2=z^2$ без ведущих нулей. Тогда получим $L=\{1;10\}$.
        \end{Example}
        \begin{Example}
            А давайте теперь в качестве $\Sigma$ возьмём Юникод, а в качестве $L$ возьмём все слова русского языка.\\
            Это вообще пример довольно странный, потому что тут надо формализовать. Например, <<рофл>> --- это слово русского языка или всё-таки нет? Ну, непонятно. Но вообще мы можем взять словарь Ожегова или ещё чего, и формализация получится.
        \end{Example}
        \begin{Example}
            Или можно взять ASCII и множество программ на C++, которые компилируются. Но опять же, надо уточнить, каким компилятором и с какими флагами.
        \end{Example}
        \pagebreak
        \begin{Comment}
            Последний пример намекает, что формальные языки нужны для общения с компьютером.\\
            Ещё стоит сказать, что во всех примерах, кроме русского языка, существуют некоторые правила проверки, принадлежит ли слово языку или нет. Но если скормить нейросети фото конспекта, то один хрен она вам ответит, принадлежит ли $\mathrm{abab}$ пятому примеру. Поэтому надо придумать некоторое более понятное компьютеру описание языка.\\
            Какие у нас вообще есть способы описать язык?
            Можно перечислить все слова. Это единственный сходу понятный способ. Им можно описать первый, третий, девятый и (возможно) десятый. Тут, понятно, есть проблема --- бесконечные языки. А бесконечные языки могут быть простыми, например уже описанный выше $\{\varepsilon;\mathrm a;\mathrm{aa};\mathrm{aaa};\ldots\}$.\\
            Поэтому надо как-то описывать правила, как язык строится. Есть два способа \sout{(и два стула)}: порождение и распознавание. Порождение --- это файловые маски, регулярные выражения и прочее подобное. То есть указание, что мы можем что-то повторить, что-то вставлять или нет и т.д. Так, пример 4 легко порождается как \verb|a|{\color{orange}\verb|*|}. А распознавание --- это когда мы скармливаем программу компилятору. Если съел, то хорошо. А что такое компилятор? Это программа. То есть у нас есть по сути некоторая процедура, которая проверяет, принадлежит ли слово языку или нет. Для палиндромов мы можем тоже такую программу написать. А теперь давайте перенесётся в 97-й год и посмотрим на пример 9. Эндрю Уайлдс ещё не пришёл с доказательством Великой Теоремы Ферма. Можем ли мы создать программу, которая проверяет существование чего-то? Ну, на самом деле, не вполне, потому что мы бесконечное количество времени будем перебирать числа. И ещё б\'{о}льшая проблема --- в ней нигде не печатается ложь. Поэтому не можем мы так сделать. Так что, мы можем не любой язык описать программой.\\
            Почему? С математической точки зрения большую часть языков описать нельзя никак. Потому что какой бы способ описания мы не придумали, описание состоит из букв какого-то мета-алфавита и счётно. А количество языков --- $2^{|\mathbb N|}$. А это, как мы знаем, больше, чем $|\mathbb N|$, увы. Но это не наш случай, потому что описали наш язык. Мы не можем написать программу, но мы его описали. Да и вообще неописываемые языки не могут возникнуть в задаче. Если язык возник, значит мы его описали. Поэтому тут проблема в ущербности наших способов описания.
        \end{Comment}
        \dfn \undercolor{red}{Регулярный язык нулевого поколения} (или \undercolor{red}{базовый регулярный язык}) --- это $\varnothing;\{\varepsilon\}$ и $\{c_i\}$ для любого $i\in|\Sigma|$. Множество этих языков обозначается $R_0$.
        \dfn \undercolor{red}{Регулярными операциями} называются
        \begin{enumerate}
            \item Объединение языков (как множеств).
            \item \undercolor{red}{Конкатенация языков} $AB=A\cdot B=\{x\cdot y\mid x\in A;y\in B\}$.
            \item \undercolor{red}{Замыкание Клини} (Kleene) $A^*=\bigcup\limits_{i=0}^\infty A^i$, где $A^0=\{\varepsilon\}$, $A^i=A^{i-1}\cdot A$.
        \end{enumerate}
        \begin{Example}
            Если $A=\{\mathrm{ab};\mathrm a\}$, $B=\{\mathrm{ba};\mathrm b\}$, то $AB=\{\mathrm{abba};\mathrm{aba};\mathrm{aa}\}$.
        \end{Example}
        \dfn \undercolor{red}{Регулярным языком $i$-того поколения} называется язык, получаемый применением регулярной операции к языку/языкам $i-1$-го поколения. То есть $R_{i+1}=\{L\mid L=AB\lor L=A\cup B\lor L=A^*;A\in R_i,B\in R_i\}$.
        \begin{Example}
            Если взять $\Sigma=\{\mathrm a;\mathrm b\}$, то $R_0=\{\varnothing;\{\varepsilon\};\{\mathrm a\};\{\mathrm b\}\}$, а
            $$
            R_1=\{\varnothing;\{\varepsilon\};\{\mathrm a\};\{\mathrm b\};\{\varepsilon;\mathrm a\};\{\varepsilon;\mathrm b\};\{\mathrm a;\mathrm b\};\{\mathrm{aa}\};\{\mathrm{ab}\};\{\mathrm{ba}\};\{\mathrm{bb}\};\mathrm{a}^*;\mathrm{b}^*\}
            $$
            Во втором поколении происходят ещё более интересные вещи.
        \end{Example}
        \dfn \undercolor{red}{Регулярный язык} --- регулярный язык некоторого поколения. То есть $\mathrm{Reg}=\bigcup\limits_{i=0}^\infty R_i$.
        \dfn \undercolor{red}{Академическим регулярным выражением} некоторого языка называются следующая форма:
        \begin{itemize}
            \item Для $\varnothing;\{\varepsilon\};\{c_i\}$ академическим регулярным выражением являются $\varnothing;\varepsilon;c_i$ соответственно.
            \item Если $A$ и $B$ обозначаются регулярными выражениями $\varphi$ и $\psi$, то регулярным выражением $A\cup B$ является $(\varphi)|(\psi)$, регулярным выражением $AB$ является $(\varphi)(\psi)$, а регулярным выражением $A$ --- $\varphi^*$.
        \end{itemize}
        \begin{Comment}
            Дальше слово <<академическое>> будет опускаться, потому что тут мы будем обсуждать именно их.
        \end{Comment}
        \begin{Comment}
            Также всем лень писать скобки, поэтому скажем, что у замыкания Клини максимальный приоритет, а у объединения --- минимальный и скажем, что вместо $(((\mathrm a)(\mathrm b))|(\mathrm b))^*$ мы будем писать $(\mathrm a\mathrm b|\mathrm b)^*$. Кстати, это запись одного из наших примеров --- после любой буквы $\mathrm a$ идёт $\mathrm b$.
        \end{Comment}
        \begin{Example}
            Рассмотрим пример: $(\mathrm{aa}|\mathrm{ab}|\mathrm{ba}|\mathrm{bb})^*$. Что это? Это множество слов чётной длины Также его можно обозначить как $((\mathrm a|\mathrm b)(\mathrm a|\mathrm b))^*$.
        \end{Example}
        \begin{Comment}
            Регулярное выражение является порождением. А порождение не всегда удобно. Мы возьмём некоторое здоровенное регулярное выражение. Породить все языки --- очень легко, а вот проверить, принадлежит ли слово он языку, нетривиально. Но на самом деле мы потом докажем, что регулярные выражение эквивалентны конечным автоматам. И конечный автомат как раз очень легко проверяет слово, но трудно порождает.
        \end{Comment}
        \begin{Comment}
            Конечный автомат работает так. Он является ориентированным графом, у которого есть состояния, и на каждом ребре написана буква алфавита. Из каждого состояния исходит по одному ребру для каждой буквы. Одно состояние помечено как <<стартовое>>, некоторое количество как <<допускающие>>. И тогда автомат читает строку по символам и из начального состояния по переходит нужным переходам. Если мы в конце придём в допускающее состояние, автомат говорит, что строка хорошая, а иначе --- плохая.
        \end{Comment}
        \begin{Example}
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (1) {};
                    \node[circle,draw,text width=1ex] at (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    
                    \draw[<-] (1) -- ++(-1,0);
                    \draw[->] (1) to[out=30,in=150,looseness=.75] node[midway,above] {$1$} (2);
                    \draw[->] (2) to[out=-150,in=-30,looseness=.75] node[midway,below] {$1$} (1);
                    \draw[->] (1) to[out=-110,in=-70,looseness=10] node[midway,below] {$0$} (1);
                    \draw[->] (2) to[out=-110,in=-70,looseness=10] node[midway,below] {$0$} (2);
                \end{tikzpicture}
            \end{figure}\noindent
            Этот автомат принимает только строки с чётным количеством единиц.
        \end{Example}
        \begin{Example}
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (1) {};
                    \node[circle,draw,text width=1ex] at (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    \node[circle,draw,text width=1em,below=of $(1)!.5!(2)$] (3) {};
                    
                    \draw[<-] (1) -- ++(-1,0);
                    \draw[->] (1) to[out=30,in=150,looseness=.75] node[midway,above] {$\mathrm a$} (2);
                    \draw[->] (2) to[out=-150,in=-30,looseness=.75] node[midway,below] {$\mathrm b$} (1);
                    \draw[->] (1) to[out=-110,in=-70,looseness=10] node[midway,below] {$\mathrm b$} (1);
                    \draw[->] (2) -- (3) node[midway,right] {$\mathrm a$};
                    \draw[->] (3) to[out=-70,in=-30,looseness=10] node[midway,below] {$\mathrm b$} (3);
                    \draw[->] (3) to[out=-150,in=-110,looseness=10] node[midway,below] {$\mathrm a$} (3);
                \end{tikzpicture}
            \end{figure}\noindent
            Кто это? А это наш старый знакомый, <<в строке после каждой $\mathrm a$ идёт $\mathrm b$>>. Кстати, тут есть состояние, из которого не выйти. Его называют стоковым или дьявольским, и иногда не рисуют, допуская синтаксический сахар. То есть также этот автомат можно нарисовать так:
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (1) {};
                    \node[circle,draw,text width=1ex] at (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    
                    \draw[<-] (1) -- ++(-1,0);
                    \draw[->] (1) to[out=30,in=150,looseness=.75] node[midway,above] {$\mathrm a$} (2);
                    \draw[->] (2) to[out=-150,in=-30,looseness=.75] node[midway,below] {$\mathrm b$} (1);
                    \draw[->] (1) to[out=-110,in=-70,looseness=10] node[midway,below] {$\mathrm b$} (1);
                \end{tikzpicture}
            \end{figure}\noindent
        \end{Example}
        \dfn \undercolor{red}{Детерминированный конечный автомат} --- это набор из: $(\Sigma;Q;s;T;\delta)$, где $\Sigma$ --- алфавит, $Q$ --- множество состояний, $s\in Q$ --- стартовое состояние, $T\subset Q$ --- множество допускающих состояний, $\delta\colon Q\times\Sigma\to Q$ --- функция перехода.
        \begin{Comment}
            Детерминированный --- потому что у него выбора нет, он полностью определяется данной ему строкой (по каждому символу из каждого состояния есть только один переход). Потом мы рассмотрим недетерминированный.
        \end{Comment}
        \dfn \undercolor{red}{Конфигурация конечного автомата} --- это пара $\langle q;w\rangle\in Q\times\Sigma^*$.
        \begin{Comment}
            Тогда мы можем явственно описать, как автомат переходит. Если мы находимся в конфигурации $\langle p;cx\rangle$, то мы можем перейти в $\langle \delta(p;c);x\rangle$. Это обозначается символом $\langle p;cx\rangle\vdash\langle q;x\rangle$.
        \end{Comment}
        \dfn Говорят, что $\langle p;y\rangle\vdash\langle q;x\rangle$, если $y=cx$ и $q=\delta(p;c)$.
        \dfn Понятно, что такое $\vdash^*$.
        \dfn Говорят, что автомат \undercolor{red}{допускает} строку $w$, если $\langle s;w\rangle\vdash^*\langle t;\varepsilon\rangle$, где $t\in T$, а $s$ --- стартовое состояние автомата.
        \begin{Comment}
            Идейно мы хотим доказать, что если язык допускается автоматом, то он порождается некоторым регулярным выражением и наоборот. То есть автоматы и регулярные выражения в некотором смысле эквивалентны.
        \end{Comment}
        \begin{Comment}
            Давайте понемногу избавляться от слов в словосочетании <<детерминированный конечный автомат>>. Сначала попробуем избавиться от слова <<конечный>>. Тогда будет не очень интересно, потому что можно сделать такое дерево, где из каждой вершины ведут рёбра в новые вершины для каждой буквы.
            \begin{figure}[H]
                \begin{tikzpicture}[
                    level 2/.style={sibling distance=1cm},
                    level 3/.style={sibling distance=2cm}
                    ]
                    \node[treenode] {}
                    child {
                        node[treenode] {}
                        child {
                            node[treenode] {}
                            edge from parent[->]
                            edge from parent node[left] {0}
                        }
                        child {
                            node[treenode] {}
                            edge from parent[->]
                            edge from parent node[right] {1}
                        }
                        edge from parent[->]
                        edge from parent node[left] {0}
                    }
                    child {
                        node[treenode] {}
                        child {
                            node[treenode] {}
                            edge from parent[->]
                            edge from parent node[left] {0}
                        }
                        child {
                            node[treenode] {}
                            edge from parent[->]
                            edge from parent node[right] {1}
                        }
                        edge from parent[->]
                        edge from parent node[right] {1}
                    };
                \end{tikzpicture}
            \end{figure}\noindent
            Тогда мы можем просто пометить нужные нам слова в этом дереве. То есть бесконечный детерминированный автомат может принять вообще любой язык. Поэтому эта концепция не интересна.\\
            Теперь уберём слово <<детерминированный>>. Что получим? Получим автомат, где из одной вершины может идти не одно ребро.
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=1.5cm]
                    \node[circle,draw,text width=1em] (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    \node[circle,draw,text width=1em,right=of 2] (3) {};
                    \node[circle,draw,text width=1em,right=of 3] (4) {};
                    \node[circle,draw,text width=1em,right=of 4] (5) {};
                    \node[circle,draw,text width=1ex] at (5) {};
                    
                    \draw[<-] (1) -- ++(-1,0);
                    \draw[->] (1) to[out=70,in=30,looseness=10] node[midway,above] {$\mathrm0$} (1);
                    \draw[->] (1) to[out=150,in=110,looseness=10] node[midway,above] {$\mathrm1$} (1);
                    \draw[->] (1) -- (2) node[midway,above] {$\mathrm0$};
                    \draw[->] (2) -- (3) node[midway,above] {$\mathrm1$};
                    \draw[->] (3) -- (4) node[midway,above] {$\mathrm0$};
                    \draw[->] (4) -- (5) node[midway,above] {$\mathrm1$};
                \end{tikzpicture}
            \end{figure}\noindent
            Когда идёт ноль, мы знаем, что делать, это просто не нарисованное стоковое состояние. А вот что делать, если выходят два ребра с одним символом, непонятно. А вот на самом деле это выбор. Например, автомат на рисунке может четырьмя способами взаимодействовать со строкой $\mathrm{010101}$. Он может сразу пойти направо, но в допускающем состоянии строка не закончится, а значит её принять нельзя. Можно два раза пройти по петлям, а потом пойти направо (тогда строка допускается), а можно походить по петлям побольше, и не дойти до допускающего состояния. И теперь вопрос --- если мы смогли допустить строку одним способом из четырёх, она в итоге принимается или нет? Так вот, говорят что да, если способ допустить есть, то допускается.\\
            В нашем случае автомат принимает то и только то, что кончается на $\mathrm{0101}$. Но ведь он должен ходить по петлям до тех пор, пока не останется 4 символа. Как он понимает, что именно столько осталось? А вот никак. Недетерминированный автомат --- математическая абстракция, если существует последовательность, что строка принимается, она принимается. Теперь формализуем это.
        \end{Comment}
        \dfn Недетерминированный конечный автомат --- это набор из $(\Sigma;Q;s;T;\delta)$, где $\Sigma$ --- алфавит, $Q$ --- множество состояний, $s\in Q$ --- стартовое состояние, $T\subset Q$ --- множество допускающих состояний, $\delta\colon Q\times\Sigma\to2^Q$ --- функция перехода.
        \begin{Comment}
            Несмотря на то, что стартовое состояние одно, можно сделать автомат, у которого их несколько, это было бы логично для концепции детерминированности. В принципе, суть от этого не изменится.
        \end{Comment}
        \dfn Говорят, что $\langle p;y\rangle\vdash\langle q;x\rangle$, если $y=cx$ и $q\in\delta(p;c)$.
        \begin{Comment}
            Если раньше $\vdash$ было функциональным отношением, и не было очень интересно. А тут уже это что-то менее скучное и однозначное.
        \end{Comment}
        \dfn Говорят, что \textbf{недетерминированный} автомат \undercolor{red}{допускает} строку $w$, если $\langle s;w\rangle\vdash^*\langle t;\varepsilon\rangle$, где $t\in T$, а $s$ --- стартовое состояние автомата.
        \thm \undercolor{darkgreen}{Теорема Томпсона}. Пусть $L$ --- формальный язык. Тогда существование ДКА, распознающего его, равносильно существованию НКА.
        \begin{Proof}
            \usemintedstyle{tango}
            Следствие право тривиально, ДКА является случаем НКА. Следствие обратно менее тривиально.\\
            Пусть у нас $\Sigma$ --- это символки типа \mintinline{c}{char}, $Q$ --- это числа от 1 до $n$. Напишем код, который проверяет, принимает ли ДКА строку.
            \begin{minted}{c++}
                int s;
                bool t[n];
                int delta[n][c];

                bool accept(string x)
                {
                    int cur = s;
                    for (char a : x)
                        cur = delta[cur][a];
                    return t[cur];
                }
            \end{minted}
            А теперь давайте попробуем написать то же самое для НКА
            \begin{minted}{c++}
                int s;
                bool t[n];
                set<int> delta[n][c];
                
                bool accept(string x)
                {
                    int L = x.length();
                    bool can[L + 1][n] = {{false}};
                    can[0][s] = true;
                    for (int i = 0; i < L; i++)
                        for (int u = 0; u < n; u++)
                            if (can[i][u])
                                for (int v : delta[u][x[i]])
                                    can[i + 1][v] = true;
                    for (int u = 0; u < n; u++)
                        if (t[u] && can[L][u])
                            return true;
                     return false;
                }
            \end{minted}
            Давайте посмотрим на строки таблицы \mintinline{c++}{can}. Изначально там строка из кучи \mintinline{c++}{false} и одного \mintinline{c++}{true} на месте \mintinline{c++}{s}. А потом мы имели некоторую \textit{битовую маску} в \mintinline{c++}{can[i]}, и мы можем получить по ней следующую битовую маску, которую запишем в \mintinline{c++}{can[i + 1]}. То есть по сути мы имеем функцию
            \begin{minted}{c++}
                bool next(bool cur[n], char c)[n]
                {
                    bool res[n] = { false };
                    for(int u = 0; u < n; u++) {
                        if (cur[u]) {
                            for (int v: delta[u][c])
                            res[v] = true;
                        }
                    }
                    return res;
                }
            \end{minted}
            Тогда нашу функцию \mintinline{c++}{accept} можно переписать как
            \begin{minted}{c++}
                int s;
                bool t[n];
                set<int> delta[n][c];
                
                bool accept(string x)
                {
                    int L = x.length();
                    bool cur[n] = {false};
                    cur[s] = true;
                    for (char a : x)
                        cur = next(cur, a);
                    for (int u = 0; u < n; u++)
                        if (t[u] && cur[u])
                            return true;
                    return false;
                }
            \end{minted}
            Так это же то же самое, что мы делали для ДКА!\\
            Итак, что мы сделали? Мы имели ДКА $(\Sigma;Q;s;T;\delta)$ и построили по нему НКА $(\Sigma;2^Q;\{s\};T';\delta')$, где $T'=\{A\mid A\cap T\neq\varnothing\}$, $\delta'(A;c)=\bigcup\limits_{u\in A}\{\delta(u;c)\}$. И можно видеть, что оба этих автомата распознают одно и то же (что можно при большом желании доказать по индукции).
            \usemintedstyle{native}
        \end{Proof}
        \dfn Алгоритм, написанный выше, называется \undercolor{red}{алгоритмом Томпсона}.
        \begin{Comment}
            Обычно его не пишут так, как написан от тут. Потому что зачастую не все состояния достижимы, и их нет смысла все хранить, а можно просто добавлять всё в приоритетную очередь.
        \end{Comment}
        \begin{Comment}
            Тут мы хотим допустить ещё больший произвол: переходы, которые не едят буквы.
            \begin{figure}[H]
                \begin{tikzpicture}
                    \node[circle,draw,text width=1em] (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    \node[circle,draw,text width=1em,right=of 2] (3) {};
                    \node[circle,draw,text width=1ex] at (3) {};
                    
                    \draw[<-] (1) -- ++(-1,0);
                    \draw[->] (1) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm0$} (1);
                    \draw[->] (2) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm1$} (2);
                    \draw[->] (3) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm2$} (3);
                    \draw[->] (1) -- (2) node[midway,above] {$\varepsilon$};
                    \draw[->] (2) -- (3) node[midway,above] {$\varepsilon$};
                \end{tikzpicture}
            \end{figure}\noindent
            Например эта штука допускает все строки, которые представимы в виде $\mathrm0^*\mathrm1^*\mathrm2^*$ (один хер я это словами запишу).
        \end{Comment}
        \dfn Недетерминированный конечный автомат с $\varepsilon$-переходами ($\varepsilon$-НКА) --- это то же самое, что и просто НКА, но у него $\delta\colon Q\times(\Sigma\cup\{\varepsilon\})\to2^Q$.
        \dfn Говорят, что $\langle p;y\rangle\vdash\langle q;x\rangle$, если $y=cx$ и $q\in\delta(p;c)$ или $y=x$ и $q\in\delta(p;\varepsilon)$.
        \begin{Comment}
            Тут возникает проблема: можно ходить по автомату бесконечно. Торжественно забиваем на это.
        \end{Comment}
        \thm \undercolor{darkgreen}{$\varepsilon$-замыкание}. НКА и $\varepsilon$-НКА также равносильны.
        \begin{Proof}
            Опять же вправо очевидно. Для доказательства влево сделаем вот что. 
            \begin{enumerate}
                \item Возьмём все $\varepsilon$-переходы. Проведём им транзитивное замыкание. Теперь автомат допускает тот же язык, но никогда не ходит по $\varepsilon$-переходу дважды подряд.
                \begin{figure}[H]
                    \begin{tikzpicture}
                        \node[circle,draw,text width=1em] (1) {};
                        \node[circle,draw,text width=1em,right=of 1] (2) {};
                        \node[circle,draw,text width=1em,right=of 2] (3) {};
                        \node[circle,draw,text width=1ex] at (3) {};
                        
                        \draw[<-] (1) -- ++(-1,0);
                        \draw[->] (1) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm0$} (1);
                        \draw[->] (2) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm1$} (2);
                        \draw[->] (3) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm2$} (3);
                        \draw[->] (1) -- (2) node[midway,below] {$\varepsilon$};
                        \draw[->] (2) -- (3) node[midway,below] {$\varepsilon$};
                        \draw[->] (1) to[out=-45,in=-135,looseness=1] node[midway,below] {$\varepsilon$} (3);
                    \end{tikzpicture}
                \end{figure}\noindent
                \item Если $q$ --- терминальное состояние, а $p$ имеет $\varepsilon$-переход в $q$, то сделаем $p$ допускающим. Теперь мы опять не изменили набор допускаемых строк, но последний переход не является $\varepsilon$.
                \begin{figure}[H]
                    \begin{tikzpicture}
                        \node[circle,draw,text width=1em] (1) {};
                        \node[circle,draw,text width=1ex] at (1) {};
                        \node[circle,draw,text width=1em,right=of 1] (2) {};
                        \node[circle,draw,text width=1ex] at (2) {};
                        \node[circle,draw,text width=1em,right=of 2] (3) {};
                        \node[circle,draw,text width=1ex] at (3) {};
                        
                        \draw[<-] (1) -- ++(-1,0);
                        \draw[->] (1) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm0$} (1);
                        \draw[->] (2) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm1$} (2);
                        \draw[->] (3) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm2$} (3);
                        \draw[->] (1) -- (2) node[midway,below] {$\varepsilon$};
                        \draw[->] (2) -- (3) node[midway,below] {$\varepsilon$};
                        \draw[->] (1) to[out=-45,in=-135,looseness=1] node[midway,below] {$\varepsilon$} (3);
                    \end{tikzpicture}
                \end{figure}\noindent
                \item Если $q\in\delta(p;\varepsilon)$, $r\in\delta(q;c)$, то добавим переход из $p$ в $r$ по символу $c$. Теперь мы больше не ходим сначала по $\varepsilon$, потом по букве.
                \begin{figure}[H]
                    \begin{tikzpicture}
                        \node[circle,draw,text width=1em] (1) {};
                        \node[circle,draw,text width=1ex] at (1) {};
                        \node[circle,draw,text width=1em,right=of 1] (2) {};
                        \node[circle,draw,text width=1ex] at (2) {};
                        \node[circle,draw,text width=1em,right=of 2] (3) {};
                        \node[circle,draw,text width=1ex] at (3) {};
                        
                        \draw[<-] (1) -- ++(-1,0);
                        \draw[->] (1) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm0$} (1);
                        \draw[->] (2) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm1$} (2);
                        \draw[->] (3) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm2$} (3);
                        \draw[->] (1) -- (2) node[midway,below] {$\varepsilon$};
                        \draw[->] (2) -- (3) node[midway,below] {$\varepsilon$};
                        \draw[->] (1) to[out=-45,in=-135,looseness=1] node[midway,below] {$\varepsilon$} (3);
                        \draw[->] (1) to[out=45,in=135,looseness=1] node[midway,above] {$\mathrm1$} (2);
                        \draw[->] (2) to[out=45,in=135,looseness=1] node[midway,above] {$\mathrm2$} (3);
                        \draw[->] (1) to[out=-90,in=-90,looseness=1.5] node[midway,below] {$\mathrm2$} (3);
                    \end{tikzpicture}
                \end{figure}\noindent
                \item В итоге мы никогда больше не ходим по $\varepsilon$-переходам. Вырежем их все к чёртовой бабушке.
                \begin{figure}[H]
                    \begin{tikzpicture}
                        \node[circle,draw,text width=1em] (1) {};
                        \node[circle,draw,text width=1ex] at (1) {};
                        \node[circle,draw,text width=1em,right=of 1] (2) {};
                        \node[circle,draw,text width=1ex] at (2) {};
                        \node[circle,draw,text width=1em,right=of 2] (3) {};
                        \node[circle,draw,text width=1ex] at (3) {};
                        
                        \draw[<-] (1) -- ++(-1,0);
                        \draw[->] (1) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm0$} (1);
                        \draw[->] (2) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm1$} (2);
                        \draw[->] (3) to[out=110,in=70,looseness=10] node[midway,above] {$\mathrm2$} (3);
                        \draw[->] (1) -- (2) node[midway,above] {$\mathrm1$};
                        \draw[->] (2) -- (3) node[midway,above] {$\mathrm2$};
                        \draw[->] (1) to[out=-45,in=-135,looseness=1] node[midway,below] {$\mathrm2$} (3);
                    \end{tikzpicture}
                \end{figure}\noindent
            \end{enumerate}
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема Клини}. $L$ --- регулярный язык тогда и только тогда, когда $\exists\varepsilon$-ДКА, принимающий $L$.
        \begin{Proof}
            Докажем право. Давайте сделаем $\varepsilon$-НКА, потому что можем. Делать будем это по индукции по поколению языка. И сначала заметим, что мы будем строить автоматы ровно с одним начальным и допускающим состоянием, причём это ещё и разные состояния.\\
            База: $R_0$. Для пустого языка ответом является
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    \node[circle,draw,text width=1ex] at (2) {};
                    
                    \draw[<-] (1) -- ++(-1,0);
                \end{tikzpicture}
            \end{figure}\noindent
            Для $\varepsilon$ ---
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    \node[circle,draw,text width=1ex] at (2) {};
                    
                    \draw[<-] (1) -- ++(-1,0);
                    \draw[->] (1) -- (2) node[midway,above] {$\varepsilon$};
                \end{tikzpicture}
            \end{figure}\noindent
            Для символа $c$ ---
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    \node[circle,draw,text width=1ex] at (2) {};
                    
                    \draw[<-] (1) -- ++(-1,0);
                    \draw[->] (1) -- (2) node[midway,above] {$c$};
                \end{tikzpicture}
            \end{figure}\noindent
            Теперь переход: надо учиться строить автомат для $R_i$, если мы умеем для $R_{i+1}$. То есть нужно посмотреть, как строить дизъюнкцию, конкатенацию и замыкание Клини. Вот так:
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (A1) {};
                    \draw[<-] (A1) -- ++(-.75,0);
                    \node[circle,draw,text width=1em] at ($(A1)+(1.5,0)$) (A2) {};
                    \node[circle,draw,text width=1ex] at (A2) {};
                    
                    \draw let \p1 = ($(A2.east)-(A1.west)$) in ($(A1.west)+(0,{veclen(\x1,\y1)/2})$) rectangle ($(A2.east)-(0,{veclen(\x1,\y1)/2})$) node[midway,above] {A};
                    
                    \node[circle,draw,text width=1em,below=of A1] (B1) {};
                    \draw[<-] (B1) -- ++(-.75,0);
                    \node[circle,draw,text width=1em] at ($(B1)+(1.5,0)$) (B2) {};
                    \node[circle,draw,text width=1ex] at (B2) {};
                    
                    \draw let \p1 = ($(B2.east)-(B1.west)$) in ($(B1.west)+(0,{veclen(\x1,\y1)/2})$) rectangle ($(B2.east)-(0,{veclen(\x1,\y1)/2})$) node[midway,above] {B};
                \end{tikzpicture}
                \qquad\qquad
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (A1) {};
                    \node[circle,draw,text width=1em] at ($(A1)+(1.5,0)$) (A2) {};
                    
                    \draw let \p1 = ($(A2.east)-(A1.west)$) in ($(A1.west)+(0,{veclen(\x1,\y1)/2})$) rectangle ($(A2.east)-(0,{veclen(\x1,\y1)/2})$) node[midway,above] {A};
                    
                    \node[circle,draw,text width=1em,below=of A1] (B1) {};
                    \node[circle,draw,text width=1em] at ($(B1)+(1.5,0)$) (B2) {};
                    
                    \draw let \p1 = ($(B2.east)-(B1.west)$) in ($(B1.west)+(0,{veclen(\x1,\y1)/2})$) rectangle ($(B2.east)-(0,{veclen(\x1,\y1)/2})$) node[midway,above] {B};
                    
                    \node[circle,draw,text width=1em,left=of $(A1)!.5!(B2)$] (1) {};
                    \draw[<-] (A1) -- ++(-.75,0) to[out=180,in=90] node[midway,left] {$\varepsilon$} (1);
                    \draw[<-] (B1) -- ++(-.75,0) to[out=180,in=-90] node[midway,left] {$\varepsilon$} (1);
                    \draw[<-] (1) -- ++(-.75,0);
                    \node[circle,draw,text width=1em,right=of $(A1)!.5!(B2)$] (2) {};
                    \node[circle,draw,text width=1ex] at (2) {};
                    \draw[->] (A2) to[out=0,in=90] node[midway,above] {$\varepsilon$} (2);
                    \draw[->] (B2) to[out=0,in=-90] node[midway,below] {$\varepsilon$} (2);
                \end{tikzpicture}
                \caption*{Дизъюнкция.}
            \end{figure}\noindent
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (A1) {};
                    \draw[<-] (A1) -- ++(-.75,0);
                    \node[circle,draw,text width=1em] at ($(A1)+(1.5,0)$) (A2) {};
                    \node[circle,draw,text width=1ex] at (A2) {};
                    
                    \draw let \p1 = ($(A2.east)-(A1.west)$) in ($(A1.west)+(0,{veclen(\x1,\y1)/2})$) rectangle ($(A2.east)-(0,{veclen(\x1,\y1)/2})$) node[midway,above] {A};
                    
                    \node[circle,draw,text width=1em,below=of A1] (B1) {};
                    \draw[<-] (B1) -- ++(-.75,0);
                    \node[circle,draw,text width=1em] at ($(B1)+(1.5,0)$) (B2) {};
                    \node[circle,draw,text width=1ex] at (B2) {};
                    
                    \draw let \p1 = ($(B2.east)-(B1.west)$) in ($(B1.west)+(0,{veclen(\x1,\y1)/2})$) rectangle ($(B2.east)-(0,{veclen(\x1,\y1)/2})$) node[midway,above] {B};
                \end{tikzpicture}
                \qquad\qquad
                \begin{tikzpicture}[node distance=1.75cm]
                    \node[circle,draw,text width=1em] (A1) {};
                    \draw[<-] (A1) -- ++(-.75,0);
                    \node[circle,draw,text width=1em] at ($(A1)+(1.5,0)$) (A2) {};
                    
                    \draw let \p1 = ($(A2.east)-(A1.west)$) in ($(A1.west)+(0,{veclen(\x1,\y1)/2})$) rectangle ($(A2.east)-(0,{veclen(\x1,\y1)/2})$) node[midway,above] {A};
                    
                    \node[circle,draw,text width=1em,right=of A2] (B1) {};
                    \node[circle,draw,text width=1em] at ($(B1)+(1.5,0)$) (B2) {};
                    \node[circle,draw,text width=1ex] at (B2) {};
                    
                    \draw let \p1 = ($(B2.east)-(B1.west)$) in ($(B1.west)+(0,{veclen(\x1,\y1)/2})$) rectangle ($(B2.east)-(0,{veclen(\x1,\y1)/2})$) node[midway,above] {B};
                    
                    \draw[<-] (B1) -- (A2) node[midway,above] {$\varepsilon$};
                \end{tikzpicture}
                \caption*{Конкатенация.}
            \end{figure}\noindent
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (A1) {};
                    \draw[<-] (A1) -- ++(-.75,0);
                    \node[circle,draw,text width=1em] at ($(A1)+(1.5,0)$) (A2) {};
                    \node[circle,draw,text width=1ex] at (A2) {};
                    
                    \draw let \p1 = ($(A2.east)-(A1.west)$) in ($(A1.west)+(0,{veclen(\x1,\y1)/2})$) rectangle ($(A2.east)-(0,{veclen(\x1,\y1)/2})$) node[midway,above] {A};
                \end{tikzpicture}
                \qquad\qquad
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (A1) {};
                    \node[circle,draw,text width=1em] at ($(A1)+(1.5,0)$) (A2) {};
                    \node[circle,draw,text width=1em] at ($(A1)-(1.5,0)$) (1) {};
                    \node[circle,draw,text width=1em] at ($(A2)+(1.5,0)$) (2) {};
                    \draw[<-] (1) -- ++(-.75,0);
                    \node[circle,draw,text width=1ex] at (2) {};
                    
                    \draw let \p1 = ($(A2.east)-(A1.west)$) in ($(A1.west)+(0,{veclen(\x1,\y1)/2})$) rectangle ($(A2.east)-(0,{veclen(\x1,\y1)/2})$) node[midway,above] {A};
                    \draw[->] (1) -- (A1) node[midway,above] {$\varepsilon$};
                    \draw[->] (A2) -- (2) node[midway,above] {$\varepsilon$};
                    \draw[->] (1) to[out=90,in=90] node[midway,above] {$\varepsilon$} (2);
                    \draw[->] (2) to[out=-90,in=-90] node[midway,below] {$\varepsilon$} (1);
                \end{tikzpicture}
                \caption*{Замыкание.}
            \end{figure}\noindent
            \begin{Comment}
                Именно так, кстати, работает с регулярными выражениями Java. Она строит по ним НКА, пытается сделать ДКА. И если ДКА получится маленький, то живёт с ним, иначе --- с НКА.
            \end{Comment}
            Хорошо, теперь докажем теорему в другую сторону. У нас есть ДКА, мы хотим сделать по нему выражение. Пронумеруем состояния от 1 до $n$. Тогда запишем $\xi_{i,j,k}$ --- регулярное выражение, которое переходит от $i$ до $j$, причём все вершины на пути имеют номер $\leqslant k$.\\
            Для $k=0$ мы имеем что? Ну, $\xi_{i,i,0}=\varepsilon|c_1|c_2|\cdots$, где $c_1;c_2;\ldots$ --- петли в вершине $i$ (по ним мы можем пройти не больше 1 раза, иначе вершина станет внутренней). В случае $\xi_{i,j,0}$ имеем то же самое, но без $\varepsilon$ (считаем, что мы строим выражение, только если они существуют).\\
            Теперь перейдём от $k$ к $k+1$. Как мы можем попасть из $i$ в $j$, проходя по вершинам с номерами не больше $k+1$? Мы можем не посещать вершину $k+1$ (если можем). А если мы её посещаем, то что имеем? Тогда мы пришли из $i$ в $k+1$ по вершинам $\leqslant k$, потом несколько раз ходили из $k+1$ в $k+1$ (тоже по вершинам $\leqslant k$), а потом пришли в из $k+1$ в $k$. То есть $\xi_{i,j,k+1}=\xi_{i,j,k}|\xi_{i,k+1,k}\xi^*_{k+1,k+1,k}\xi_{k+1,j,k}$. Ну, хорошо, как тогда выглядит выражение для всего автомата? Ну, $\varphi=\xi_{s,t_1,n}|\xi_{s,t_2,n}|\cdots|\xi_{s,t_{|T|},n}$, где $t\in T$. Алгоритм этот просто ужасен, но тем не менее он доказываем нам теорему.
            \begin{Comment}
                Почему этот алгоритм ужасен? А давайте попробуем применить его к автомату <<чётное количество нулей>>.
                \begin{figure}[H]
                    \begin{tikzpicture}[node distance=2cm]
                        \node[circle,draw,text width=1em] (1) {};
                        \node at ($(1)+(0,.5)$) {\textnumero1};
                        \node[circle,draw,text width=1ex] at (1) {};
                        \node[circle,draw,text width=1em,right=of 1] (2) {};
                        \node at ($(2)+(0,.5)$) {\textnumero2};

                        \draw[<-] (1) -- ++(-1,0);
                        \draw[->] (1) to[out=30,in=150,looseness=.75] node[midway,above] {$0$} (2);
                        \draw[->] (2) to[out=-150,in=-30,looseness=.75] node[midway,below] {$0$} (1);
                        \draw[->] (1) to[out=-110,in=-70,looseness=10] node[midway,below] {$1$} (1);
                        \draw[->] (2) to[out=-110,in=-70,looseness=10] node[midway,below] {$1$} (2);
                    \end{tikzpicture}
                \end{figure}\noindent
                \begin{itemize}
                    \item $\xi_{110}=\varepsilon|1$
                    \item $\xi_{120}=0$
                    \item $\xi_{210}=0$
                    \item $\xi_{220}=\varepsilon|1$
                    \item $\xi_{111}=\varepsilon|1|(\varepsilon|1)(\varepsilon|1)^*(\varepsilon|1)$
                    \item $\xi_{121}=0|(\varepsilon|1)(\varepsilon|1)^*0$
                \end{itemize}
                У нас уже на таком этапе получаются не самые простые и красивые выражения, а что дальше-то будет...
            \end{Comment}
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Минимизация и эквивалентность автоматов}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Чем вообще интересны регулярные языки? Этот класс языков, которые обычно можно задать этим способом. Любая булева функция регулярных языков порождает регулярный язык, например. Сейчас мы ещё поговорим о том, что конечный автомат можно легко анализировать и выяснять какие-то их свойства. Более того, если взять регулярный язык, то у него есть единственный автомат с минимальным количеством состояний, причём он легко строится, после чего полученный автомат можно анализировать.\\
            Давайте пока что считать, что у нас нет стартового состояния. Вместо этого будем считать, что мы можем сделать стартовым любое состояние. И тогда мы получаем некий язык, порождаемый уже честным автоматом $L_u=\{x\mid(u;x)\vdash^*(t;\varepsilon),t\in T\}$.
        \end{Comment}
        \dfn $u\sim v\Leftrightarrow L_u=L_v$ \undercolor{red}{эквивалентные состояния} автомата.
        \thm Тривиально, эквивалентность состояний --- отношение эквивалентности.
        \thm Если $A$ --- ДКА, $u,v$ --- эквивалентные состояния $A$, то автомат $A/u,v$ вида $(\Sigma;Q'=Q\setminus\{v\};\cdot;T'=T\setminus\{v\};\delta')$, где $\delta'(p,c)=\begin{cases}
            \delta(p,c) & \delta(p,c)\neq v\\
            u & \delta(p,c)=v
        \end{cases}$ порождает тот же язык для любой стартовой вершины.
        \begin{Proof}
            Вообще говоря, это тривиально.
        \end{Proof}
        \thm Пусть $A_1$, $A_2$ --- ДКА, причём ни $A_1$, ни $A_2$ не содержат эквивалентных состояний. Пусть $s_1$ --- стартовое состояние $A_1$ и любое состояние $A_1$ достижимо из $s_1$. Пусть $s_2$ --- стартовое состояние $A_2$ и любое состояние $A_2$ достижимо из $s_2$. Пусть также $L_{s_1}=L_{s_2}$. Тогда $A_1$ изоморфен $A_2$.
        \dfn Что значит <<два автомата изоморфны>>? Это значит, что существует биекция $\varphi\colon Q_1\to Q_2$, такая что
        \begin{enumerate}
            \item Если $u_2=\varphi(u_1)\land v_1=\delta(u_1;c)\land v_2=\delta(u_2;c)$, то $\varphi(v_1)=v_2$.
            \item Стартовое состояние переводится в стартовое.
            \item Терминальное состояние переводится в терминальное.
        \end{enumerate}
        \begin{Comment}
            Отсюда явно следует равенство алфавитов, но вообще на двух алфавитах тоже можно было бы ввести изоморфизм, что нам делать тупо лень.
        \end{Comment}
        \begin{Proof}
            Давайте снова временно забудем про стартовые состояния. После этого давайте склеим два автомата в один (без стартового состояния). И давайте попробовать искомую биекцию. Биекция будет такой: $\sim$. Во-первых, внутри одного автомата нет эквивалентных по условию. Тогда давайте рассмотрим $u_1\in A_1$. Тогда $u_1$ достижимо из $s_1$ по строке $x$. Давайте из $s_2$ также пойдём по $x$. Тогда мы дойдём до некоторого $u_2$. Хочется сказать, что $u_1\sim u_2$. Почему так? Пусть не так. Что тогда? Тогда их языки не совпадают. То есть есть какое-то слово $z$, которое лежит в $L_{u_1}$, но не лежит в $L_{u_2}$. Но тогда $xz\in L_{s_1}$, но $xz\notin L_{s_2}$, противоречие (автоматы, напомню, детерминированные). Это значит, что (в силу того, что $\sim$ --- отношение эквивалентности) получаем разбиение нашего большого автомата из $A_1$ и $A_2$ на пары состояний: первое из $A_1$, второе --- из $A_2$. Дальше уже тривиально проверить, что полученные пары дают нам изоморфность. 
        \end{Proof}
        \thm Если все состояния $A$ не эквивалентны друг другу и доступны из стартового, то $A$ содержит минимальное количество вершин среди всех автоматов, порождающих тот же язык.
        \begin{Proof}
            Рассмотрим $A'$. Удалим из него эквивалентные состояния и не достижимые из $s$. Тогда по теореме выше $A$ и $A'$ изоморфны, значит $A$ и $A'$ имеют одинаковое количество вершин.
        \end{Proof}
        \begin{Comment}
            Хорошо, как теперь алгоритмически задачу решать? Ну, как-то вообще пока хз. Какие-то языки $L_u$ и $L_v$ на равенство проверять, сложно как-то. Но на самом деле вот что можно делать.
        \end{Comment}
        \dfn Если $u\nsim v$, то $u$ и $v$ называются \undercolor{red}{различимыми} состояниями. И ещё говорят, что $L_u$ и $L_v$ \undercolor{red}{различают} $u$ и $v$.
        \begin{Comment}
            Тогда мы будем строить множества $D_i$ --- пары состояний, различимых строкой длины $\leqslant i$. Тривиально, $D_0$ состоит из пар состояний, где одно терминальное, другое --- нет. А теперь хочется используя $D_i$ построить $D_{i+1}$. Что значит, что $u$ и $v$ различимы строкой $y=cx$? Это значит, что мы из $u$ получили $p$, из $v$ получили $q$ при помощи $c$, а потом пошли по строке $x$ и попали в одном случае в терминальное состояние, в другом --- в нетерминальное. Ну так это значит, что $D_{i+1}=\{(u;v)\mid \exists c:(\delta(u;c);\delta(v;c))\in D_i\}\cup D_i$. Проблема в том, что нам придётся так делать долго.\\
            Тривиально, если $D_i=D_{i+1}$, то $D_{i+1}=D_{i+1}$ (просто из формулы следует). В таком случае наивно можно оценить, что $D_{|Q|^2}$ уж точно будет равно $D_{|Q|^2+1}$.
            \begin{minted}{c++}
                // Q -- множество состояний.
                // T -- множество терминальных состояний.
                std::queue<std::pair<int, int>> Queue;
                bool D[Q.size()][Q.size()] = { { false } };
                for (int u : T)
                    for (int v : Q)
                        if (T.find(v) == T.end())
                        {
                                Queue.push_back(std::make_pair(u, v));
                                Queue.push_back(std::make_pair(v, u));
                                D[u][v] = D[v][u] = true;
                        }
                while (!Queue.empty())
                {
                    auto [p, q] = Queue.pop_front();
                    for (char c : Sigma)
                        for (int u : from[p][c])
                            for (int v : from[q][c])
                                if (!D[u][v])
                                {
                                    Queue.push_back(std::make_pair(u, v));
                                    D[u][v] = true;
                                }
                }
            \end{minted}
            Мы тут только странно храним $\delta$ --- как множество состояний \mintinline{c++}{from[c][p]}, из которых по символу \mintinline{c++}{c} идёт ребро в \mintinline{c++}{p}. Поскольку каждую пару вершин и каждую пару рёбер мы можем рассмотреть один раз, работает это алгоритм за $O(n^2+n^2|\Sigma|)=O(n^2|\Sigma|)$. Если мы в явном виде ищем дополнение отношению эквивалентности (\mintinline{c++}{D} здесь), то лучше ничего и не получится, потому что это какое-то рандомное отношение, в нём и есть $n^2$ пар, которые надо рассмотреть. Но всё же если всё же рассматривать отношение $\sim$ вместо $\nsim$, то его можно и оптимально хранить, и алгоритм получше действительно есть, но он достаточно неприятен.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Нерегулярные языки}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Мы поговорили о достоинствах конечных автоматов, которых много. Но из-за того, что автомат --- достаточно тупое устройство, есть довольно простые языки, для которых нет конечного автомата. Например, язык, содержащий слова, в котором сначала идёт $n$ нулей, а потом --- $n$ единиц.
        \end{Comment}
        \thm Язык, содержащий слова, в котором сначала идёт $n$ нулей, а потом --- $n$ единиц, --- не регулярный.
        \begin{Proof}
            Ну, от противного, пусть автомат существует. Пусть $n$ --- число состояний $A$. Рассмотрим набор строк $\varepsilon,\mathrm0,\mathrm{00},\ldots,\underbrace{\mathrm{00}\cdots\mathrm0}_n$. Скормим все эти строки нашему автомату. Мы попадём в состояния $u_0,u_1,\ldots,u_n$. Поскольку состояний у автомата $n$, какие-то два из них совпадают. Например, $u_i$ и $u_j$. А теперь докормим нашему автомату строку из $i$ единиц. С одной стороны, мы из $u_i$ приходим в терминальное состояние. С другой --- из $u_j$ должны \textbf{не} придти в конечное состояние. Но ведь $u_i=u_j$, а значит мы по строке из $i$ единиц придём в одно и то же место. Противоречие.
        \end{Proof}
        \begin{Comment}
            В чём идеологическая причина этого дела? В том, что у конечного автомата $O(1)$ памяти, а тут надо чиселко помнить.
        \end{Comment}
        \thm Язык, принимающий правильные скобочные последовательности, не регулярный.
        \begin{Proof}
            Делаем то же самое, что и в предыдущем доказательстве.
        \end{Proof}
        \begin{Comment}
            На самом деле, все доказательства будет примерно одинаковыми. Мы будем скармливать автомату какие-то префиксы, а потом что-то сломается. Так давайте вынесем общую часть всех их в лемму.
        \end{Comment}
        \thm \undercolor{darkgreen}{Лемма о накачке}/\undercolor{darkgreen}{лемма о нарастании}/\undercolor{darkgreen}{boost lemma}. Пусть $L$ --- регулярный язык. Тогда
        $$
        \exists n>0~\forall w\in L:|w|\geqslant n~\exists x,y,z:xyz=w~\left\{\begin{aligned}
            &y\neq\varepsilon\\
            &|xy|\leqslant n\\
            &\forall k\geqslant0~xy^kz\in L
        \end{aligned}\right.
        $$
        \begin{Comment}
            Что это за ёбаная хуйня вообще? А суть вот в чём. Если мы берём достаточно длинное слово, мы можем разбить его на $x,y,z$, где $x$ и $y$ достаточно близко к началу, и тогда $y$ можно повторять сколько угодно раз, чтобы слово оставалось в языке.
        \end{Comment}
        \begin{Proof}
            Раз язык регулярный, у него есть ДКА. Назовём его $A$. Пусть $n$ --- количество состояний $A$. Возьмём слово $w$ долины больше $n$, которое в автомате допускается. Тогда мы сначала как-то ходили по первым $n$ его буквам, потом как-то ушли в допускающее состояние. Но тогда в первых $n$ переходах есть $n+1$ состояние, а значит там есть повторения. Ну так и хорошо, то что до этого повторения --- $x$, то что от первого включения повторяющегося состояния до второго --- $y$, всё остальное $z$. И утверждение становится тривиально, мы можем несколько раз пройти по циклу (использовав $y$ столько раз, сколько надо).
        \end{Proof}
        \begin{Example}
            Давайте применим лемму о разрастании для нашего знакомого языка $\{0^n1^n\mid n\in\mathbb N\}$. Лемма говорит нам, что любое слово можно разбить на 3 части такие что... В частности, можно разбить слово $0^n1^n$. Но тогда $x$ и $y$ будут состоять только из нулей. А если мы повторим $y$ хотя бы 1 раз, мы проиграем.
        \end{Example}
        \begin{Proof}
            Очень грустно, что не для любого нерегулярного языка можно так подобрать слово, что оно разрушит лемму.
        \end{Proof}
        \begin{Comment}
            Теперь небольшое отступление на тему того, как смотреть на формулы вида леммы о нарастании. А именно, на игровую интерпретацию. Пусть у нас есть какая-то штука с кванторами вида $\exists x_1\forall x_2\exists x_3\cdots?x_kP(x_1;\ldots;x_k)$. Подобное утверждение можно рассмотреть как игру для двух игроков: первый --- $\exists$, второй --- $\forall$. И они смотрят на строку слева направо и в свой ход подбирают значение соответствующее квантору. Тогда если формула истинна, побеждает $\exists$, иначе --- $\forall$. Тогда верны те и только те формулы, когда $\exists$ имеет выигрышную стратегию. Это по сути даже теоремой не назвать, потому что когда мы распишем, что такое выигрышная стратегия, мы тупо получим ту де самую формулу (существует ход $\exists$, что для любого хода $\forall$ существует ход $\exists$, что...)\\
            Так вот тут по сути мы играем в такую игру за $\forall$. Наш противник походил и выбрал $n$. Мы ходим и даёт ему слово $w=0^n1^n$. Дальше наш противник как-то походил, дал нам $x$, $y$ и $z$, и при этом как бы наш противник не походил, мы можем выбрать $k=2$ (или вообще любое не равное $1$) и победить. Поскольку победили мы вне зависимости от ходов оппонента, утверждение действительно ложно.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Свойства и операции с регулярными языками}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Как мы уже знаем, если $L_1,L_2$ --- регулярные языки, то $L_1\cup L_2$, $L_1L_2$, $L_1^*$ --- также регулярны. А вот регулярно ли, например, $L_1\cap L_2$. Ну, да, легко можно построить автомат из состояний $Q_1\times Q_2$.
        \end{Comment}
        \dfn \undercolor{red}{Прямое произведение автоматов} $(\Sigma;Q_1;s_1;T_1;\delta_1)$ и $(\Sigma;Q_2;s_2;T_2;\delta_2)$ --- автомат $(\Sigma;Q_1\times Q_2;\{(s_1;s_2)\};T_1\times T_2;\delta\colon(q_1;q_2)\mapsto(\delta_1(q_1);\delta_1(q_1)))$,
        \begin{Comment}
            Также мы умеем брать дополнение языка $\overline{L_1}$ в терминах автоматов ($T'=Q\setminus T$), а значит умеем разность: $L_1\setminus L_2=L_1\cap\overline{L_2}$. А можно ли выразить это напрямую? Ну, можно, это то же самое, что и в $\cap$, но только терминальные состояния --- это не $T_1\times T_2$, а $T_1\times(Q_2\setminus T_2)$.\\
            А это значит, что с любыми теоретико-множественными операциями мы можем легко построить автомат. Например, очень легко можем построить автомат для $(L_1\cap L_2)\cup L_3$ --- это произведение трёх автоматов, где терминальтми состояниями являются те, у которых последний элемент тройки --- из $T_3$ или оба первых из $T_1$ и $T_2$ соответственно. То есть любую теоретико-множественную операцию мы можем выразить.
        \end{Comment}
        \begin{Comment}
            Хорошо, что ещё мы можем делать с автоматами? Можем проверять равенство. Минимизировать оба, проверить равенство полученных автоматов. Пока что мы умеем делать её за $O(n^2)$, скоро научимся $O(n\log n)$. Но на самом деле для этой задачи есть более простое решение. Давайте проверим, что $L_1\subset L_2$. Ну, пока не понятно. На самом деле это сводится к задаче проверки равенства следующим образом: $L_1\cap L_2=L_1$. А ещё сводится к задаче проверки на пустоту: $L_1\setminus L_2=\varnothing$. А проверка на пустоту очень легка, надо проверить недостижимость терминальных состояний. Такая простота всех операций --- безусловное преимущество регулярных языков. Для более широкого класса языков это часто бывает нельзя сделать.
        \end{Comment}
        \begin{Comment}
            Давайте возьмём ещё такой язык: $L=\{0^n1^n\mid n\in\mathbb N\}$, про который мы знаем, что он не регулярный. Давайте заменим в нём все нули на $00$, а все единицы --- на $101$. Получим $\{0^{2n}(101)^n\mid n\in\mathbb N\}$. Этот язык также будет нерегулярным. А что будет, если мы проведём такую замену на регулярном языке? Получится ли регулярный язык?\\
            То есть мы берём какую-то функцию $f\colon\Sigma\to\Sigma^*$ и применяем её к каждому символу каждой строки. Так вот, подобное отображение сохраняет регулярность языка на самом деле.
        \end{Comment}
        \dfn Отображение $f\colon\Sigma\to\Sigma^*$ называется \undercolor{red}{гомоморфизмом}.
        \dfn \undercolor{red}{Гомоморфизм от строки} --- конкатенация результатов гомоморфизма от каждого её символа ($f(c_1c_2\ldots c_n)=f(c_1)f(c_2)\cdots f(c_n)$).
        \dfn $f(L)=\{f(w)\mid w\in L\}$.
        \thm Если $L$ --- регулярный, то $f(L)$ --- тоже
        \begin{Proof}
            Ну, берём каждый переход между состояниями $u\to v$ по символу $c$. Символ $c$ переводится в какую-то строку $w$. Давайте между $u$ и $v$ $|w|-1$ состояние, между которыми вставим символы $w$. Автомат может перестать быть регулярным, но автоматом быть не перестанет.
        \end{Proof}
        \begin{Comment}
            В данном случае интереснее обратная задача: является ли регулярным $f^{-1}(L)=\{w\mid f(w)\in L\}$? Ну, на самом деле тоже да, мы просто смотрим существует ли путь из $u$ в $v$ по строке $f(c)$? Если существует, то вставляем переход по $c$.
        \end{Comment}
        \begin{Comment}
            К чему мы это всё? А к тому, что при помощи подобных суждений можно доказывать нерегулярность чего-то. Например, если $L_1\in\mathrm{Reg}$ и $L_1\cap L_2\notin\mathrm{Reg}$, то, очевидно, $L_2\notin\mathrm{Reg}$. И гомоморфизмы также можно для подобных целей использовать.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Уравнения в регулярных выражениях}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Давайте построим выражение, принимающий двоичные записи чисел, кратных трём. Выражение как-то сложно строится, давайте построим автомат.
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=2cm]
                    \node[circle,draw,text width=1em] (1) {};
                    \node at ($(1)+(0,.5)$) {\textnumero0};
                    \node[circle,draw,text width=1ex] at (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    \node at ($(2)+(0,.5)$) {\textnumero1};
                    \node[circle,draw,text width=1em,right=of 2] (3) {};
                    \node at ($(3)+(0,.5)$) {\textnumero2};
                    
                    \draw[<-] (1) -- ++(-1,0);
                    \draw[->] (1) to[out=30,in=150,looseness=.75] node[midway,above] {$1$} (2);
                    \draw[->] (2) to[out=-150,in=-30,looseness=.75] node[midway,below] {$1$} (1);
                    \draw[->] (2) to[out=30,in=150,looseness=.75] node[midway,above] {$0$} (3);
                    \draw[->] (3) to[out=-150,in=-30,looseness=.75] node[midway,below] {$0$} (2);
                    \draw[->] (1) to[out=-110,in=-70,looseness=10] node[midway,below] {$0$} (1);
                    \draw[->] (3) to[out=-110,in=-70,looseness=10] node[midway,below] {$1$} (3);
                \end{tikzpicture}
            \end{figure}\noindent
            Как по нему сделать регулярное выражение? Можно применить общий алгоритм, но он конченый. Проще посмотреть на автомат и построить руками. Пусть $L_0=\{w\mid (0;w)\vdash(0;\varepsilon)\}$, $L_1=\{w\mid (1;w)\vdash(0;\varepsilon)\}$, $L_2=\{w\mid (2;w)\vdash(0;\varepsilon)\}$. То есть все строки, такие что начав из $i$-того состояния мы придём в терминальное (в данном случае нулевое). Такое ощущение, что они как-то связаны. Ну, посмотрим на $L_0$.
            $$
            L_0=\varepsilon+0L_0+1L_0
            $$
            В рамках данной лекции мы будем использовать $+$ вместо $|$ либо $\cup$. Это выглядит как какое-то уравнение в регулярных выражения. Или, точнее, система уравнений:
            \[
            \left\{\begin{aligned}
                L_0&=\varepsilon+0L_0+1L_1\\
                L_1&=0L_2+1L_0\\
                L_2&=1L_2+0L_1
            \end{aligned}\right.
            \]
            Ну так давайте решим её. Например, нижнее можно упросить, чтобы оно не было рекурсивно: $L_2=1^*0L_1$.
        \end{Comment}
        \thm Если $\alpha$, $\beta$ --- регулярные выражения, и $\varepsilon\notin\alpha$. Тогда $L=\alpha L+\beta\Leftrightarrow L=\alpha^*\beta$.
        \begin{Proof}
            Следствие влево очевидно: $\alpha\alpha^*\beta+\beta=\alpha^*\beta$.\\
            Следствие вправо: надо доказать, что $w\in L\Rightarrow w\in\alpha^*\beta$. Ну, если $w\in L$, то $w\in\alpha L+\beta$. Если $w\in\beta$, то понятно. Если $w\in\alpha L$, то пусть $w=u_1u_2$, где $u_1\in\alpha$, $u_2\in L$. Поскольку $u_1\neq\varepsilon$, $|u_2|<|w|$. А значит можно доказать по индукции по длине слова.
        \end{Proof}
        \begin{Comment}
            С помощью такой леммы решим нашу систему уравнений:
            \[
            \left\{\begin{aligned}
                L_0&=\varepsilon+0L_0+1L_1\\
                L_1&=0L_2+1L_0\\
                L_2&=1L_2+0L_1
            \end{aligned}\right.\Leftrightarrow
            \left\{\begin{aligned}
                L_0&=\varepsilon+0L_0+1L_1\\
                L_1&=\underbrace{01^*0}_\alpha L_1+\underbrace{1L_0}_\beta\\
                L_2&=1^*0L_1
            \end{aligned}\right.\Leftrightarrow
            \left\{\begin{aligned}
                L_0&=\varepsilon+0L_0+1(01^*0)^*1L_0\\
                L_1&=(01^*0)^*1L_0\\
                L_2&=1^*0L_1
            \end{aligned}\right.\Leftrightarrow
            \left\{\begin{aligned}
                L_0&=(0+1(01^*0)^*1)L_0+\varepsilon\\
                L_1&=(01^*0)^*1L_0\\
                L_2&=1^*0L_1
            \end{aligned}\right.\Leftrightarrow
            \left\{\begin{aligned}
                L_0&=(0+1(01^*0)^*1)^*\\
                L_1&=(01^*0)^*1L_0\\
                L_2&=1^*0L_1
            \end{aligned}\right.
            \]
            Вот и ответ на задачу: $(0+1(01^*0)^*1)^*$. И это, в целом, видно из автомата. Мы либо идём по нулю, либо идём по 1, там ходим через второе состояние несколько раз, потом возвращаемся обратно в нулевое. И так несколько раз.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Алгоритм Хопкрофта минимизации конечных автоматов}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Тут мы сразу строим разбиение на классы. И эти классы будем разбивать на подклассы, пока не получим корректное разбиение. И в любой момент в разных классах будут пары совершенно точно различимых состояний.\\
            С чего начать? Надо понять, какие как минимум два класса различимы. Ну, $T$ и $Q\setminus T$ различимы как минимум пустой строкой.\\
            Теперь давайте посмотрим на класс и рассмотрим все переходы по некоторому символу $c$. Если все они ведут в один класс, то хорошо. А что если у нас есть две вершины $u$ и $v$, из которых переход $c$ ведёт в разные классы? Ну, вершины в разных классах различимы, а значит вершины $u$ и $v$ также различимы. Т.е. наш класс можно разбить на те вершины, которые по $c$ попадают в один класс, и те, которые в другой. Или ещё в третий, если такие есть.\\
            Окей, в целом уже хорошо, но давайте делать немного по-другому. Рассмотрим класс $A$ и символ $c$. Рассмотрим все входящие переходы откуда-то в $A$ по символу $c$. Пусть есть класс $B$, откуда есть такие переходы. Если их меньше $B$, класс $B$ можно разбить. Это называется разбиением при помощи класса $A$. Также понятно, что если ни один класс не разбивается, то мы получили искомое разбиение на классы эквивалентности. При этом с помощью классов, которыми мы уже проводили разбиение, больше разбиение проводить не надо, бессмысленно получится.\\
            Итак, классы, которыми мы ещё ничего не разбивали, будут лежать в очереди.
            \begin{minted}{c++}
                std::set<std::set<int>> S{T, Q - T};
                std::queue<std::set<int>> q{T, Q - T};
                while (!q.empty())
                {
                    std::set<int> C = q.front();
                    q.pop();
                    for (char d : Sigma)
                        for (const std::set<int>& R : S)
                        {
                            // Пробуем разбить R через delta^-1(d, C)
                            //     (т.е. обратные переходы по d из C)
                            // Если R разбился на R1 и R2, убираем его из очереди
                            //     (если он там был), добавляем R1 и R2,
                            //     а также убираем R из S и добавляем в него R1 и R2.
                        }
                }
            \end{minted}
            Это работает долго и непонятно как. Как это оптимизировать? Ну, во-первых, надо рассматривать сразу всем обратные переходы, а не только из класса $R$. И для каждого класса сохранить, сколько переходов из него ведёт в $C$. После этого пройтись по всем классам и посмотреть, если там положительное число, меньшее размера класса, надо разбить. Главное, это нужно реализовать за линейное время от числа переходов, не меньше. Это пока что не $n\log n$, но!\\
            Фишка алгоритма в следующем. Пусть мы разбивали всех по классу $C$. И когда-то после этого класс $C$ разбился на $C_1$ и $C_2$. Нужно ли нам разбивать что-то по $C_1$ и $C_2$? На самом деле, достаточно будет только $C_1$. После разбиения по $C$, все переходы одного класса целиком ведут либо в $C$, либо вовне. После $C_1$ все переходы целиком ведут в правильную часть $C$, а значит $C_2$ никакие другие классы не разобьёт. А значит, если мы будем класть в очередь только тот класс, что меньше, мы победим и получим асимптотику $O(n\log n)$. Что мы итого имеем? Пусть у нас есть какой-то переход. Если мы его используем второй раз, значит теперь наш класс уменьшился не меньше чем вдвое. А значит общее количество раз, которое используется один переход --- $\leqslant\log_2n$. А поскольку суммарно в автомате $|\Sigma|n$ переходов, асимптотика получается какой надо.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Контекстно-свободные грамматики}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Вспоминаем, что мы хотим. Мы хотим объяснить компьютеру строение некоторого языка. Мы уже смогли описать класс языков, называемых <<регулярными>>. Этот класс довольно мал, потому что даже простые структуры (палиндромы или $0^n1^n$) не описываются. Поэтому есть другой подход.
            \begin{itemize}
                \item $S\to\varepsilon$
            \end{itemize}
            Что это такое? Это значит, что $\varepsilon$ является правильной скобочной последовательностью. Что ещё является таковой? Ну, п.с.п., заключённая в скобки и конкатенация двух п.с.п. То есть добавляем ещё два правила:
            \begin{itemize}
                \item $S\to(S)$
                \item $S\to SS$
            \end{itemize}
            Это и есть контекстно-свободная грамматика. Как ей пользоваться? У нас есть некоторые названия (в данным случае $S$) и есть <<правила>> (называемые \textit{продукциями}). Так вот мы сначала берём $S$, а потом по одной из продукций заменить одно вхождение $S$ на то, что справа от продукции. Например, мы можем получить последовательность $(())()$ так:
            $$
            S\Rightarrow SS\Rightarrow(S)S\Rightarrow((S))S\Rightarrow(())S\Rightarrow(())(S)\Rightarrow(())()
            $$
            КСГ --- множество правил, где слева стоят переменные, а справа --- символы алфавита и переменные. Какая-то переменная выбрана как стартовая, начинаем со строки из неё, потом заменяем одну переменную на то, что справа от некоторой продукции. При этом переменные называются \textit{нетерминалами}, а символы алфавита --- \textit{терминалами}. Множество обычно нетерминалов обозначают $N$.
        \end{Comment}
        \dfn Набор $(\Sigma;N;S;P)$, где $S\in N$, $P\subset N\times(N\cup\Sigma)^*$, $P$ --- конечно называется \undercolor{red}{контекстно-свободной грамматикой}.
        \begin{Comment}
            Почему множество продукций конечно? Ну, по филосовским причинам. Если оно бесконечно, его структуру также надо объяснить компьютеру, непонятно, как.
        \end{Comment}
        \begin{Example}
            Как получить формальный язык $0^n1^n$? Да также, как и со скобочными последовательностями, но проще.
            \begin{itemize}
                \item $A\to\varepsilon$
                \item $A\to0A1$
            \end{itemize}
        \end{Example}
        \dfn $\Rightarrow$ --- бинарное отношение <<\undercolor{red}{выводится за один шаг}>> на $(N\cup\Sigma)^*$, такое что $\alpha\Rightarrow\beta$ тогда и только тогда, когда
        $$
        \exists A\in N~\exists x,y,\xi\in(N\cup\Sigma)^*~\left\{\begin{aligned}
            \alpha&=xAy\\
            \beta&=x\xi y\\
            (A\to\xi)&\in P
        \end{aligned}\right.
        $$
        \dfn Контекстно-свободная грамматика $\Gamma$ \undercolor{red}{принимает язык} $L(\Gamma)$, если $L(\Gamma)=\{w\mid S\Rightarrow^*w\}$.
        \dfn Элементы множества $(N\cup\Sigma)^*$ называются \undercolor{red}{сентенциальными формами}.
        \begin{Comment}
            У определения $\Rightarrow$ есть две степени свободы: мы можем выбрать любой нетерминал из существующих, а также любое правило. Впрочем, на самом деле первое --- иллюзия, потому что наши нетерминалы живут независимыми жизнями. И им индифферентно, в каком порядке мы раскрываем их, получится одно и то же. Например, мы можем раскрывать их всегда в конкретном порядке. А вот вторая степень свободы действительно таковой является.
        \end{Comment}
        \dfn КСГ, в которой у каждого нетерминала есть только одна продукция, называется \undercolor{red}{линейной программой}.
        \begin{Comment}
            Такая программа считается, по сути, алгоритмом сжатия, ведь мы кодируем ровно одно слово какой-то структурой.
        \end{Comment}
        \dfn Преобразование, в процессе которого всегда раскрывается самый первый нетерминал, называется \undercolor{red}{левосторонним выводом}.
        \begin{Comment}
            Когда вывод левосторонний, можно записывать его чуть проще для понимания и чуть компактнее.
            \begin{figure}[H]
                \begin{tikzpicture}[every node/.style={treenode},level 1/.style={sibling distance=3cm},level 2/.style={sibling distance=1cm}]
                    \node {$S$}
                    child {
                        node {$S$}
                        child {
                            node {$($}
                        }
                        child {
                            node {$S$}
                            child {
                                node {$($}
                            }
                            child {
                                node {$S$}
                                child {
                                    node {$\varepsilon$}
                                }
                            }
                            child {
                                node {$)$}
                            }
                        }
                        child {
                            node {$)$}
                        }
                    }
                    child {
                        node {$S$}
                        child {
                            node {$($}
                        }
                        child {
                            node {$S$}
                            child {
                                node {$\varepsilon$}
                            }
                        }
                        child {
                            node {$)$}
                        }
                    };
                \end{tikzpicture}
            \end{figure}\noindent
            Это \textit{дерево разбора}. Если пройтись по его листьям в порядке DFS, получим ответ. Набор листьев называется кроной дерева разбора. В дереве разбора можно заметить, что не сохраняется порядок, в каком мы заменяли нетерминалы. Это и есть причина, почему грамматики контекстно-свободные. Ещё есть контекстно-зависимая грамматика, это когда правила в неё выглядят как $\alpha A\beta\to\alpha\gamma\beta$, то есть нетерминалы можно преобразовывать только в некотором случае. И такие штуки уже более сложные, в частности проверить, лежит ли в неё слово, алгоритмически невозможно.
        \end{Comment}
        \begin{Example}
            Давайте приведём пример чего-то, где больше одного нетерминала. Например, арифметические выражения из сложения и умножения. Хочется как-то отметить приоритеты операций в грамматике, поэтому поступим так:
            \begin{itemize}
                \item $E\to T$
                \item $E\to E+T$
                \item $T\to F$
                \item $T\to F*T$
                \item $F\to n$
                \item $F\to (E)$
            \end{itemize}
        \end{Example}
        \begin{Comment}
            Хорошо, мы обсудили замену нетерминалов в различном порядке. И мы не считаем это неоднозначностью при преобразовании. Есть ли у нас какие-то другие неоднозначности? Определённо, да, ведь строку $()()()$ можно раскрывать как конкатенацию разными способами.
        \end{Comment}
        \dfn Грамматика называется \undercolor{red}{однозначной}, если у любого слова не более одного дерева разбора.
        \begin{Comment}
            У слов из языка одно, у остальных --- ноль.
        \end{Comment}
        \begin{Comment}
            Можем ли мы сделать грамматику скобочных последовательностей однозначной? Да:
            \begin{itemize}
                \item $S\to\varepsilon$
                \item $S\to(S)S$
            \end{itemize}
            Это мы даже делали. Но в общем случае это неверно, существуют грамматики, которые нельзя сделать однозначной.
        \end{Comment}
        \dfn Язык называется \undercolor{red}{контекстно-свободным}, если существует контекстно-свободная грамматика, порождающая его.
        \begin{Comment}
            Вопрос: контекстно-свободные языки лучше регулярных или хуже? А вот первые включают в себя вторые.
        \end{Comment}
        \dfn Грамматика называется \undercolor{red}{праволинейной}, если все её правила имеют либо вид $A\to xB$, либо вид $A\to x$, где $x\in\Sigma^*$.
        \thm $L$ является регулярным тогда и только тогда, когда его можно задать праволинейной грамматикой.
        \begin{Proof}
            Сначала докажем вправо. Если $L\in\mathrm{Reg}$, у него есть детерминированный конечный автомат. Возьмём нетерминал для каждого состояния. И тогда для каждого перехода из $A$ в $B$ по символу $c$ введём правило $A\to cB$. Если состояние $A$ терминальное, добавим правило $B\to\varepsilon$. Стартовый нетерминал --- стартовое состояние автомата. Тривиально вообще понять, что получили мы тот же самый язык, легко индукцией по числу переходов понять, что $(A,x)\vdash^*(B;\varepsilon)$ равносильно $A\Rightarrow^*xB$.\\
            Теперь докажем влево. Пусть есть праволинейная грамматика. Построим по ней $\varepsilon$-НКА. Точно также сделаем состояния для каждого нетерминала (но это будут не все состояния). Тогда для правила $A\to xB$ сделаем переход из $A$ и $B$ по строке $x$. Переход по строке $x=c_1c_2\cdots c_n$ --- переход через $n-1$ вспомогательное состояние. Для правила $A\to x$ делаем то же самое, только берём ещё одно состояние (терминальное), к которому и добавляем переход по строке $x$.
        \end{Proof}
        \begin{Comment}
            Что-то проблемы нам доставляют $A\to xB$, если $x$ не состоит из одного символа. Хочется упростить, чтобы состоял. Как у нас был сложный для обработки НКА и более простой ДКА, так и сейчас мы хотим упростить грамматику. Финалом упрощения грамматики будет грамматика в \textit{нормальной форме Хомского}.
        \end{Comment}
        \dfn Грамматика находится в \undercolor{red}{нормальной форме Хомского}, если её правила имеют одну из двух форм:
        \begin{itemize}
            \item $A\to BC$
            \item $A\to c$
        \end{itemize}
        где $A,B,C\in N$, $c\in\Sigma$. Кроме, возможно, одного правила $S\to\varepsilon$, где $S$ --- стартовое состояние, которое не встречается в правых частях правил.
        \begin{Comment}
            Оговорка про $S\to\varepsilon$ нужна потому, что без него нельзя выразить слово $\varepsilon$.
        \end{Comment}
        \thm У любой грамматики существует нормальная форма Хомского.
        \begin{Comment}
            Сначала пара договорённостей. Пусть $A,B,\ldots,Z$ --- нетерминалы, $a,b,c,d,\ldots$ --- символы $\Sigma$, $\ldots,x,y,z$ --- слова $\Sigma^*$, $\alpha,\beta,\gamma,\delta,\zeta$ --- слова $(N\cup\Sigma)^*$.\\
            Второе --- в грамматиках могут встречаться части, которые не нужны. Как в автомате недостижимые состояния.
        \end{Comment}
        \dfn Нетерминал называется \undercolor{red}{полезным}, если он может встретится в выводе некоторого слова. Иначе --- \undercolor{red}{бесполезным}.
        \begin{Comment}
            Есть две причины, почему символ бывает бесполезным. Первая --- он может быть недостижим. Понятно, что это. Вторая --- нетерминал может быть непорождающим, то есть из него нельзя получить строку из нетерминалов. С недостижимыми нетерминалами всё ясно, вот пример непорождающих:
            \begin{itemize}
                \item $S\to a$
                \item $S\to A$
                \item $A\to bA$
            \end{itemize}
        \end{Comment}
        \thm Если все нетерминалы порождающие и достижимые, то все они полезны.
        \begin{Comment}
            Про каждый конкретный нетерминал так говорить нельзя:
            \begin{itemize}
                \item $S\to a$
                \item $S\to A$
                \item $A\to bAB$
                \item $B\to c$
            \end{itemize}
            Нетерминал $B$ порождающий и достижимый, но бесполезный.
        \end{Comment}
        \begin{Comment}
            А в каком порядке избавляться от символов-то? Сначала удалим непорождающие, потом недостижимые (в.т.ч. те, которые станут недостидимыми после первого шага). После этого, очевидно, новых непорождающих появится не может.\\
            Удалить недостижимые легко (запускаем DFS, всё). Удалить непорождающие непонятно, как. Есть следующий алгоритм: нужно рассмотреть правила $A\to\gamma\in P$. Если $\gamma$ состоит из нетерминалов и порождающих состояний, то $A$ порождающее. Когда новых порождающий состояний не появляется, оставшиеся --- не порождающие. Почему алгоритм работает? Все непорождающие, тривиально, будут удалены. Почему порождающий обязательно будет рано или поздно помечен? Ну, рассмотрим $A$. Он порождающий. То есть $A\Rightarrow^kx$. Пусть $k$ минимально (по всем таким $A$). Ну, лол $A\Rightarrow\alpha\Rightarrow^{k-1}x$, а значит всё в $\alpha$ --- порождающее, причём помеченное, потому что $k$ минимально. Значит и $A$ помечено.
        \end{Comment}
        \begin{Comment}
            Здесь и далее считается, что мы уже удалили все бесполезные символы.
        \end{Comment}
        \thm У любой грамматики существует нормальная форма Хомского.
        \begin{Proof}
            Какого сорта правила нам не подходят? Нам не подходят
            \begin{enumerate}
                \item $A\to a_1A_2a_3a_4A_5\cdots$, то есть в правой части есть как терминалы, так и нетерминалы.
                \item $A\to\varepsilon$ --- $\varepsilon$-правила (если их больше одного или бла-бла-бла).
                \item $A\to B$ --- цепные правила.
                \item $A\to B_1B_2B_3\cdots B_k$, $k\geqslant3$ --- длинные правила.
            \end{enumerate}
            Избавимся от этих правил. Как избавляться от первого? Для каждого символа $c$ заведём $X_c$ --- нетерминал такой что $X_c\to c$. Тогда всё в первом правиле преобразуем в длинное правило так: $A\to X_{a_1}A_2X_{a_3}X_{a_4}A_5\cdots$.\\
            Как справляться с длинными правилами? По индукции. Если $A\to B_1\cdots B_k$, то преобразуем его в $A\to B_1A_1$, $A_1\to B_2A_2$ и так далее.\\
            Избавимся от $\varepsilon$-правил. Будет немного похоже на устранение непорождающих символов. Найдём все $\varepsilon$-порождающие терминалы. То есть $A\Rightarrow^*\varepsilon$. Алгоритм их поиска будет таким:
            \usemintedstyle{tango}
            \begin{minted}{python}
                while changes:
                    changes = False
                    for A, gamma in Gamma:
                        if all(map(epsilon_productive, gamma)):
                            epsilon_productive[A] = True
                            changes = True
            \end{minted}
            \usemintedstyle{native}
            По индукции тривиально доказать, что это найдёт нам все $\varepsilon$-порождающие состояния. Теперь как от них избавиться? Пусть $B$ является $\varepsilon$-порождающим. Тогда какие есть переходы с $B$ в правой части?
            \begin{itemize}
                \item $A\to B$, такие не трогаем.
                \item $A\to BC$ или $A\to CB$, добавляем переход $A\to C$.
            \end{itemize}
            После этого убираем все $\varepsilon$-переходы. Почему ничего не поменяется? Ну, добавление переходов ничего не поломает, а удаление всех $\varepsilon$-переходов работает так. У нас было дерево разбора, которое порождало слово $w\neq\varepsilon$. Какие-то его фрагменты порождали $\varepsilon$. Тогда эти фрагменты дерева можно вырезать, поскольку мы добавили новые правила. Дерево в новой грамматике будет корректным, и породит то же слово. Есть проблема с $w=\varepsilon$. Если изначально наша грамматика не порождала $\varepsilon$, то и так хорошо. Если порождала, породим новый стартовый нетерминал, который переходит в старый стартовый и в $\varepsilon$. У нас специально такой был патч в определении.\\
            Осталось избавиться от цепных правил. Это похоже на $\varepsilon$-переходы в НКА. Построим граф из цепных правил и сделаем ему транзитивное замыкание. Теперь нам нет смысла использовать два цепных перехода подряд. То есть если у слова есть дерево разбора, от у него также есть дерево разбора, где нет двух цепных переходов подряд. Теперь если были переходы $A\to B$ и $B\to CD$, то добавим $A\to CD$, а если были $A\to B$ и $B\to c$, то добавим $A\to c$. Теперь у нас во внутренних вершинах нет смысла использовать цепные переходы до каких-то. Ну, мы их только <<до каких-то>> и используем. Удалим их все.
        \end{Proof}
        \begin{Example}
            Рассмотрим нашу знакомую грамматику:
            \begin{itemize}
                \item $S\to\varepsilon$
                \item $S\to(S)S$
            \end{itemize}
            После первого шага она превратится в
            \begin{itemize}
                \item $S\to\varepsilon$
                \item $A\to{(}$
                \item $B\to{)}$
                \item $S\to ASBS$
            \end{itemize}
            После второго ---
            \begin{itemize}
                \item $S\to\varepsilon$
                \item $A\to{(}$
                \item $B\to{)}$
                \item $S\to AX$
                \item $X\to SY$
                \item $Y\to BS$
            \end{itemize}
            После третьего ---
            \begin{itemize}
                \item $A\to{(}$
                \item $B\to{)}$
                \item $S\to AX$
                \item $X\to SY$
                \item $X\to Y$
                \item $Y\to BS$
                \item $Y\to B$
                \item $S_0\to S$
                \item $S_0\to\varepsilon$
            \end{itemize}
            После четвёртого ---
            \begin{itemize}
                \item $A\to{(}$
                \item $B\to{)}$
                \item $S\to AX$
                \item $X\to SY$
                \item $Y\to BS$
                \item $X\to BS$
                \item $X\to{)}$
                \item $Y\to{)}$
                \item $S_0\to AX$
                \item $S_0\to\varepsilon$
            \end{itemize}
        \end{Example}
        \begin{Comment}
            Теперь вопрос: а зачем это всё? А вот зачем.\\
            Пусть нам дано слово $x$, и мы хотим узнать, выводится ли это слово в грамматике $\Gamma$ в НФХ. Так вот, в НФХ это делать намного приятнее. Причём при большом желании можно для каждого нового правила посмотреть, откуда оно взялось и даже преобразовать дерево разбора из НФХ в оригинальное. Это не очень приятно, но в целом, почему нет.\\
            Так вот, алгоритм Кока --- Янгера --- Касами (CYK). Пусть $\mathrm{dp}_{A,i,j}$ --- порождается ли из нетерминала $A$ фрагмент строки $x$ от $i$-того элемента до $j$-того. Считать это мы будем по возрастанию Чему равно $\mathrm{dp}_{A,i,i}$? Ну, если есть правило $A\to x[i]$, то истине, иначе --- лжи. Что делать, когда $j>i$? Тогда нам нужно пользоваться правилами $A\to BC$. Переберём их все, переберём $k\in[i:j-1]$ и если хотя бы для каких-то $B,C,k$ $\mathrm{dp}_{B,i,k}\land\mathrm{dp}_{C,k+1,j}$, то можно.
            \[\begin{split}
                \mathrm{dp}_{A,i,i}&=\begin{cases}
                    1 & A\to x[i]\in\Gamma\\
                    0 & \mathrm{otherwise}
                \end{cases}\\
                \mathrm{dp}_{A,i,j}&=\bigvee\limits_{A\to BC\in\Gamma}\bigvee\limits_{k=i}^{j-1}(\mathrm{dp}_{B,i,k}\land\mathrm{dp}_{C,k+1,j})
            \end{split}\]
        \end{Comment}
        \dfn \undercolor{red}{Размер грамматики} --- сумма длин правых и левых частей.
        \thm В НФХ размер грамматики --- $O(\text{количество правил})$.
        \begin{Comment}
            Как меняет размер грамматики преобразование в НФХ? На самом деле все шаги, кроме последнего, увеличивают количество размер, а последний шаг --- в худшем случае квадратично.\\
            А за сколько работает CYK? Размер динамики --- $qn^2$, каждое считается за количество правил заданного символа на $n$. То есть CYK работает за $n^3|\Gamma|$. $|\Gamma|$ считается константой, поэтому $n^3$. Это охренеть как долго. Поэтому несколько утверждений без доказательства.\\
            Есть теорема о том, что проверка слова в произвольной грамматике эквивалентно умножению некоторых матриц. Поэтому тривиальный алгоритм получился за $n^3$, а вообще можно быстрее. Второе --- в произвольной грамматике-то да, а в однозначной можно сильно быстрее. Есть алгоритм Эрли (Earley), который работает за куб для произвольной грамматики (в т.ч. не в НФХ), но если чуть аккуратнее оценить время работы в однозначном случае, то получится $n^2$.\\
            А если потребовать более сильные условия (LL- и LR-грамматики), то есть способы разбирать за линейное время.
        \end{Comment}
        \begin{Comment}
            Окей, у регулярных выражений был друг: конечные автоматы. А в КС грамматик друга мы не нашли. Но он такой есть, и мы сейчас его введём. В чём была проблема с конечным автоматом? В малом количестве памяти, он не может помнить числа, потому что чисел бесконечное число. Хочется дать ему память. Самая простая память --- стек.\\
            \textit{МП-автомат}, \textit{автомат с магазинной памятью}, \textit{стековая машина}, \textit{push-down automaton} или \textit{stack machine} --- это вот какая штука:
            Как в ДКА у нас его граф. Помимо графа у нас есть стек. В этом стеке изначально лежит приклеенный элемент --- маркер дна $Z_0$. Его нельзя снимать, иначе у вас перехватит дыхание от кошмара, и перехватит дыхание в прямом смысле слова, так как вы сразу упадёте в обморок на руки одного из ваших застывших без движения товарищей. Чтобы понять, куда переходить, мы смотрим на символ нашей строки, вынимаем первый символ из стека, смотрим на них два, после чего идём в новое состояние и добавляем что-то (несколько символов) в стек. Если мы сняли $Z_0$, обязаны его положить (возможно, вместе с чем-то другим).\\
            При этом автомат не обязательно недетерминированный, если в нём есть $\varepsilon$, он детерминированный, если по заданному символу стека есть либо $\varepsilon$-переход, либо какие-то разные символы. По умолчанию автомат считается недетерминированным.
        \end{Comment}
        \begin{Example}
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=1.5cm]
                    \node[circle,draw,text width=1em] (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    \node[circle,draw,text width=1ex] at (2) {};
                    
                    \draw[<-] (1) -- ++(-1,0);
                    \draw[->] (1) -- (2) node[midway,above] {$\varepsilon,Z_0/Z_0$};
                    \draw[->] (1) to[in=-100,out=-140,looseness=12.5] node[midway,below,sloped] {${(},A/AA$} (1);
                    \draw[->] (1) to[in=-40,out=-80,looseness=12.5] node[midway,below,sloped] {${(},Z_0/AZ_0$} (1);
                    \draw[->] (1) to[in=110,out=70,looseness=12.5] node[midway,above,sloped] {${)},A/\varepsilon$} (1);
                \end{tikzpicture}
            \end{figure}\noindent
            Это недетерминированный автомат для правильных скобочных последовательностей. По открывающей скобке мы в любом случае добавляем символ на стек, по закрывающей убираем символ со стека (а если он там нет, мы уже сломались). Когда стек пуст, можем придти в терминальное состояние. Конкретно этот автомат легко детерминизируется:
            \begin{figure}[H]
                \begin{tikzpicture}[node distance=1.5cm]
                    \node[circle,draw,text width=1em] (1) {};
                    \node[circle,draw,text width=1em,right=of 1] (2) {};
                    \node[circle,draw,text width=1ex] at (2) {};
                    
                    \draw[<-] (1) -- ++(-1,0);
                    \draw[->] (1) -- (2) node[midway,above] {$\varepsilon,Z_0/Z_0$};
                    \draw[->] (1) to[in=-100,out=-140,looseness=12.5] node[midway,below,sloped] {${(},A/AA$} (1);
                    \draw[->] (2) to[in=-45,out=-135,looseness=1] node[midway,below] {${(},Z_0/AZ_0$} (1);
                    \draw[->] (1) to[in=110,out=70,looseness=12.5] node[midway,above,sloped] {${)},A/\varepsilon$} (1);
                \end{tikzpicture}
            \end{figure}\noindent
            Выглядит как некий трюк, который только в данном случае работает. И это действительно так. И мы это ещё обсудим.
        \end{Example}
        \dfn \undercolor{red}{МП-автомат}, \undercolor{red}{автомат с магазинной памятью}, \undercolor{red}{стековая машина}, \undercolor{red}{push-down automaton} или \undercolor{red}{stack machine} --- это набор
        $$
        (\Sigma;Q;\Pi;Z_0;s;T;\delta)
        $$
        Где $\Sigma$ --- алфавит языка, $Q$ --- множество состояний, $\Pi$ --- язык букв, которые мы кладём в стек, $Z_0\in\Pi$ --- маркер дна, $s\in Q$ --- стартовое состояние, $T\subset Q$ --- терминальные состояния,
        $$\delta\colon(\Sigma\cup\{\varepsilon\})\times\Pi\to2^{Q\times\Pi^*}$$
        --- функция перехода, при этом каждое значение $\delta$ --- конечное множество.
        \pagebreak
        \begin{Comment}
            Чтобы определить, что такое детерминированный МП-автомат, придётся поменять определение. Во-первых, по одному символу строки и одному символу стека мы можем перейти только в одно состояние. При этом иногда придётся никуда не переходить. Это мы обозначим символом $\bot$. То есть теперь у нас
            $$\delta\colon(\Sigma\cup\{\varepsilon\})\times\Pi\to Q\times\Pi^*\cup\{\bot\}$$
            Во-вторых, нужно, чтобы по одному символу стека у нас не было одновременно $\varepsilon$-перехода и перехода по символу:
            $$
            \forall p\in Q~\forall A\in\Pi~\left\{\begin{aligned}
                \exists c\in\Sigma~\delta(p;c;A)\neq\bot&\Rightarrow\delta(p;\varepsilon;A)=\bot\\
                \delta(p;\varepsilon;A)\neq\bot&\Rightarrow\forall c\in\Sigma~\delta(p;c;A)=\bot
            \end{aligned}\right.
            $$
        \end{Comment}
        \dfn Набор $(p;x;\gamma)$, где $p\in Q$, $x\in\Sigma^*$, $\gamma\in\Pi^*$ --- \undercolor{red}{конфигурация МП-автомата}.
        \dfn $(p;x;\gamma)\vdash(q;y;\zeta)$, если одно из двух:
        $$
        x=cy\qquad\gamma=A\beta\qquad\zeta=\alpha\beta\qquad(q;\alpha)\in\delta(p;c;A)
        $$
        $$
        x=y\qquad\gamma=A\beta\qquad\zeta=\alpha\beta\qquad(q;\alpha)\in\delta(p;\varepsilon;A)
        $$
        \dfn \undercolor{red}{Язык МП-автомата} --- $L(A)=\{x\mid(s;x;Z_0)\vdash^*(t;\varepsilon;\gamma),t\in T\}$.
        \begin{Comment}
            Мы будем решать задачки, и поймём, что обычно МП-автоматы опустошают стек. И потом докажем, что можно преобразовать МП-автомат в штуку, которая принимает слово, если стек пуст. Это называется МП-автоматом с допуском по пустому стеку.
        \end{Comment}
        \dfn \undercolor{red}{МП-автомат с допуском по пустому стеку} --- это набор из
        $$
        (\Sigma;Q;\Pi;Z_0;s;\delta)
        $$
        (то есть всё то же, что и у обычного, но без терминальных состояний).
        \dfn Говорят, что МП-автомат с допуском по пустому стеку \undercolor{red}{допускает слово} $x$ не если $(s;x;Z_0)\vdash^*(t;\varepsilon;*)\mid t\in T$, а если $(s;x;Z_0)\vdash^*(*;\varepsilon;\varepsilon)$ (то есть мы обязательно снимаем маркер дна).
        \thm $L$ распознаётся МП-автоматом тогда и только тогда, когда распознаётся МП-автоматом по пустому стеку.
        \begin{Proof}
            Вправо. Рассмотрим МП-автомат. У него есть допускающие состояния. Добавим к автомату ещё одно состояние: <<опустошение стека>>. Из него будет переход из самого в себя по $\varepsilon,*/\varepsilon$. И из всех терминальных состояний сделаем переход $\varepsilon,*/\varepsilon$ в <<опустошение>>. Все состояния, кроме <<опустошения>>, не опустошают стек (мы не снимаем $Z_0$), а <<опустошение>> --- опустошает. Несложно заметить, что это будет нужный нам МП-автомат по пустому стеку.\\
            Вправо. Рассмотрим МП-автомат с допуском по пустому стеку. Вместо его стартового состояния сделаем новое, которое имеет переход $\varepsilon,Z_0/Z_0Z_1$, то есть подложим новый маркер дна. И из всех состояний МП-автомата по пустому стеку сделаем переход $\varepsilon,Z_1/Z_0$ в единственное терминальное состояние. Это то что нам нужно.
        \end{Proof}
        \thm $L$ распознаётся МП-автоматом тогда и только тогда, когда он контекстно-свободен.
        \begin{Proof}
            Влево. Пусть у нас слово $x$ распознаётся КС-грамматикой:
            $$
            S\Rightarrow\alpha_1\Rightarrow\alpha_2\Rightarrow\cdots\Rightarrow\alpha_t
            $$
            Пусть у нас вывод левосторонний. Тогда после каждого перехода мы имеем некоторый префикс итоговой строки:
            $$
            S\Rightarrow x_1\beta_1\Rightarrow x_2\beta_2\Rightarrow\cdots\Rightarrow x_t\mid x_t=x,\beta_t=\varepsilon
            $$
            Давайте обозначим за $y_1$ то, что нужно добавить к $x_i$, чтобы получить $x$. Построим автомат (с допуском по пустому стеку) так. $\Sigma$ --- мы знаем что, $\Pi=\Sigma\cup N$, $Z_0=S$, $Q=\{q\}$, $s=q$ (лишь одно состояние). Тогда мы начинаем из конфигурации $(q;x;S)$, и это значит, что $x_0=\varepsilon$, $y_0=x$, $\beta_0=S$. Тогда что делаем? Если $\beta_i$ начинается с терминала, можно убрать его из $y_i$ (тем самым засунуть его в $x_i$). Это переход $c,c/\varepsilon$. Если $\beta_i$ начинается с нетерминала, надо его раскрыть по какому-то правилу: $\varepsilon,A/\alpha$. Итого получаем два правила: $(q;cx';c\gamma')\vdash(q;x';\gamma')$ и $(q;x';A\gamma)\vdash(q;x;\alpha\gamma)$. Несложно по индукции доказать, что
            $$
            S\Rightarrow^*z\beta\quad x=zy\Longleftrightarrow(q;x;S)\vdash(q;y;\beta)
            $$
            Есть некоторое условие на это, $x_i$ должны неубывать (то есть мы не можем перекидывать символы $x_1$ в $\beta_i$).
            \begin{Example}
                Для П.С.П. в однозначной форме мы имеем автомат
                \begin{figure}[H]
                    \begin{tikzpicture}[scale=1.5]
                        \node[circle,draw,text width=1em] (1) {};

                        \draw[->] (1) to[in=67.5,out=22.5,looseness=12.5] node[midway,above,sloped] {${(},{(}/\varepsilon$} (1);
                        \draw[->] (1) to[in=-22.5,out=-67.5,looseness=12.5] node[midway,below,sloped] {${)},{)}/\varepsilon$} (1);
                        \draw[->] (1) to[in=157.5,out=112.5,looseness=12.5] node[midway,above,sloped] {$\varepsilon,S/\varepsilon$} (1);
                        \draw[->] (1) to[in=-112.5,out=-157.5,looseness=12.5] node[midway,below,sloped] {$\varepsilon,S/(S)S$} (1);
                    \end{tikzpicture}
                \end{figure}\noindent
            \end{Example}
            Вправо. Пусть $A$ --- МП-автомат с допуском по пустому стеку. Сделаем из него грамматику. Пусть $Q$ --- состояния, $\Pi$ --- стековый алфавит. Рассмотрим $N=Q\times\Pi\times Q$. Обозначать элементы $N$ мы будем как $[pAq]$. Причём в множество $N$ мы ещё чего-нибудь добавим. Какой смысл мы введём для $[pAq]$? Что делаем автомат? Пусть в какой-то момент мы имеем состояние $p$ и $A$ на стеке. Автомат будет как-то ходить по автомату, он уберёт $A$, добавит $\alpha$, потом снимет верхний символ $\alpha$ и добавит $\beta$. Но поскольку у нас допуск по пустому стеку, автомат рано или поздно уберёт всё, что породил из $A$. Обозначим это состояние за $q$. Несложно заметить, что все пути, которые автомат породит из $p,A$ зависят только из $A$ и $p$ (и строки, разумеется). Ни от чего другого. Так вот, это и будет относится к $[pAq]$. $[pAq]$ --- это набор символов, который мы откусили при проходе от $p$ до $q$, сняв в итоге только $A$:
            $$
            [pAq]\Rightarrow^*z\Longleftrightarrow(p;z;A)\vdash(q;\varepsilon;\varepsilon)
            $$
            Как нам начать КС-грамматику? А вот так:
            \begin{itemize}
                \item $S\to[sZ_0u_1]$
                \item $S\to[sZ_0u_2]$
                \item $\vdots$
                \item $S\to[sZ_0u_n]$
            \end{itemize}
            Где $u_n$ --- состояния МП-автомата, а $S$ --- мы только что добавили в $N$.\\
            Осталось лишь понять, как в КС-грамматике задать правило $(p;z;A)\vdash(q;\varepsilon;\varepsilon)$. Ну, смотрите. Пусть из $p$ мы по символу строки $a$ и символу стека $A$ пришли в $r$ и добавили в стек $\alpha$. Если $\alpha=\varepsilon$, то только для $r=q$ имеем корректный переход, а значит в таком случае создаём правило
            $$
            [pAq]\to a
            $$
            Если $\alpha=V$, где $V$ --- неважно кто, терминал или нет. Тогда
            $$
            [pAq]\to a[rVq]
            $$
            И последний вариант --- $\alpha=V_1V_2\cdots V_k$, $k\geqslant 2$.
            $$
            [pAq]\to a[rV_1u_1][u_2V_2u_3]\cdots[u_{k-1}V_kq]
            $$
            Где $u_i$ --- произвольное состояние $Q$.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Не КС-языки}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            В случае с регулярными языками мы, заметя детали простого концепта под ковёр, получили мутную лемму. Тут так не получится, методы в корне не элементарны.
        \end{Comment}
        \thm \undercolor{darkgreen}{Лемма о разрастании}/\undercolor{darkgreen}{лемма о накачке}/\undercolor{darkgreen}{boost lemma для КС-языков}. Пусть $L$ --- контекстно свободный. Тогда
        $$\exists n>0~\forall w\in L:|w|\geqslant n~\exists u,v,x,y,z:w=uvxyz,vy\neq\varepsilon,|vxy|\leqslant n~\forall k~uv^kxy^kz\in L$$
        \begin{Proof}
            \begin{figure}[H]
                \begin{tikzpicture}[scale=.5]
                    \draw (-1,0) -- (1,0) node[midway,below] {$x$} -- (0,2) node[below=1.5mm] {$A$} -- cycle;
                    \draw (1,0) -- (2,0) node[midway,below] {$y$} -- (0,4) node[below=1.5mm] {$A$} -- (-2,0) -- (-1,0) node[midway,below] {$v$};
                    \draw (2,0) -- (3,0) node[midway,below] {$z$} -- (0,6) node[below=1.5mm] {$S$} -- (-3,0) -- (-2,0) node[midway,below] {$u$};
                \end{tikzpicture}\qquad\qquad
                \begin{tikzpicture}[scale=.5]
                    \draw (-1,-2) -- (1,-2) node[midway,below] {$x$} -- (0,0) node[below=1.5mm] {$A$} -- cycle;
                    \draw (1,-2) -- (2,-2) node[midway,below] {$y$} -- (0,2) node[below=1.5mm] {$A$} -- (-2,-2) -- (-1,-2) node[midway,below] {$v$};
                    \draw (1,0) -- (2,0) node[midway,below] {$y$} -- (0,4) node[below=1.5mm] {$A$} -- (-2,0) -- (-1,0) node[midway,below] {$v$};
                    \draw (2,0) -- (3,0) node[midway,below] {$z$} -- (0,6) node[below=1.5mm] {$S$} -- (-3,0) -- (-2,0) node[midway,below] {$u$};
                \end{tikzpicture}\qquad\qquad
                \begin{tikzpicture}[scale=.5]
                    \draw (-1,-4) -- (1,-4) node[midway,below] {$x$} -- (0,-2) node[below=1.5mm] {$A$} -- cycle;
                    \draw (1,-4) -- (2,-4) node[midway,below] {$y$} -- (0,0) node[below=1.5mm] {$A$} -- (-2,-4) -- (-1,-4) node[midway,below] {$v$};
                    \draw (1,-2) -- (2,-2) node[midway,below] {$y$} -- (0,2) node[below=1.5mm] {$A$} -- (-2,-2) -- (-1,-2) node[midway,below] {$v$};
                    \draw (1,0) -- (2,0) node[midway,below] {$y$} -- (0,4) node[below=1.5mm] {$A$} -- (-2,0) -- (-1,0) node[midway,below] {$v$};
                    \draw (2,0) -- (3,0) node[midway,below] {$z$} -- (0,6) node[below=1.5mm] {$S$} -- (-3,0) -- (-2,0) node[midway,below] {$u$};
                \end{tikzpicture}
            \end{figure}\noindent
            Идейно: для достаточно длинных слов какой-то нетерминал будет повторятся. Надо только обсудить, что такое $n$, почему $vy\neq\varepsilon$ и подобное.\\
            Пусть $\Gamma$ --- КС-грамматика в нормальной форме Хомского для $L$. Пусть количество её нетерминалов равно $m$. Тогда возьмём $n=2^m$.\\
            Рассмотрим дерево разбора. Тривиально, если оно имеет высоту (в рёбрах) $h$, у него не больше $2^h$ листьев. Итак, если мы рассмотрим $w:|w|\geqslant2^m$, то дерево разбора $w$ имеет высоту не меньше $m$. А значит есть путь от корня до листа, в котором существуют два одинаковых нетерминала. Из всех нетерминалов, у которых в поддереве есть такой же, выберем самый далёкий от корня. Несложно заметить, что в этом поддереве нет других повторяющихся пар. Что ж. Давайте то, что выводится до первого $A$, назовём $u$ --- от первого до второго --- $v$, из второго --- $x$, от второго до конца первого --- $y$, после первого --- $z$. Почему $vy\neq\varepsilon$? Ну, у верхнего $A$ есть два ребёнка. Один из идёт в $A$, другой --- куда-то, при этом порождая хотя бы один символ (нормальная форма). Этот символ порождается в $v$ или $y$, значит $vy\neq\varepsilon$. Следующее условие --- $|vxy|\leqslant n$. Ну, смотрите. Несложно заметить, что высота верхнего $A$ не больше $m$ (была бы больше, были бы повторы нетерминалов ниже). А значит листьев у дерева высоты $\leqslant m$ есть меньше либо равно $2^m=m$. И финальное условие про $k$ тривиально.
        \end{Proof}
        \begin{Example}
            Посмотрим $0^a1^a2^a$. Докажем, что это не КС-язык. В игровой трактовке за игрока $\forall$. Пусть нам дали $n$. Мы, недолго думая, выбираем слово $w=0^n1^n2^n$. Наш противник бьёт слово на $uvxyz$. Несложно заметить, что $vxy$ не содержит либо нулей, либо двоек (иначе длина не сходится). После этого мы берём $k=2$, и побеждаем, потому что в полученном слове $uv^2xy^2z$ количество нулей либо двоек не увеличится, а длина увеличится. Значит это точно не будет слово языка $0^a1^a2^a$.
        \end{Example}
        \begin{Comment}
            Почему этот пример важен? А посмотрим на вот такие 2 языка: $0^a1^a2^b$ и $0^a1^b2^b$, где $a$ и $b$ --- не связанные числа. Они, тривиально, контекстно-свободны. А их пересечение --- это не контекстно-свободный $0^a1^a2^a$. В чём идейная проблема с пересечением? А в том, что мы не можем перемножить два стековых автомата, потому что у нас получатся два стека. А на двух стеках, на самом деле, можно вычислить всё что угодно. Кстати, отсюда следует интересное предположение --- пересечение КС- и регулярного языка контекстно-свободно.
        \end{Comment}
        \begin{Example}
            Другой пример не КС-языка --- тандемные повторы в $|\Sigma|\geqslant2$. Тут по числу $n$ нужно взять $0^n1^n0^n1^n$. Cами посмотрите, что так получим победу. При этом так случайно оказывается, что язык \textbf{не}-тандемных повторов контекстно-свободен.
        \end{Example}
        \begin{Comment}
            Почему с идеологической точки зрения это так? Ну, в ДКА мы брали $Q\setminus T$. Тут во-первых из-за недетерминизма такой трюк не проходит, а ещё нам мешает стек.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Операции с КС-языками}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Как мы помним, пересечение КС-языков или разность КС-языком является не всегда. В отличие от объединения, конкатенации и замыкания Клини (что нетрудно доказать). Но оказывается, если проводить бинарные операции с одним КС-языком и одним регулярным, результат будет контекстно-свободен.
        \end{Comment}
        \thm Пусть $L$ --- КС-язык, $R\in\mathrm{Reg}$. Тогда $L\cap R$ контекстно-свободен.
        \begin{Proof}
            Представим пересечение автомата с магазинной памятью и ДКА. Построим МП-автомат для $L\cap R$. Делаем всё тривиально, состояние --- это пара. При этом переходы --- это переход из $L$ плюс переход из $R$ (он стек не трогает). Терминальные состояния --- это понятно что, $T_L\times T_R$. Несложно заметить, что получится именно то, что нам нужно.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Детерминированные МП-автоматы}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Мы уже упоминали, что детерминированные МП-автоматы ущербные и даже не все КС-языки порождают.\\
            Детерминированный МП-автомат с допуском по пустому стеку --- совсем ущербное устройство, потому что стек у нас за проход кончится один раз, а значит ни одно слово не является префиксом другого (а значит даже не все регулярные языки распознаются, $a^*$ не распознаётся). Детерминированный обычный МП-автомат --- это уже менее ущербная штука, как минимум все регулярные языки распознаёт (что как минимум значит, что детерминированные МП-автоматы по пустому стеку не равносильны детерминированным обычным МП-автоматам). Но не все КС-грамматики, увы. Почему распознаёт все регулярные языки --- понятно, давайте возьмём ДКА и скажем, что теперь это детерминированный МП-автомат, который не трогает стек.\\
            Впрочем, вернёмся к неэквивалентности двух типов детерминированных МП-автоматов. Оказывается, если отбросить поправку на беспрефиксность, то они остаются эквивалентными. Более формально:
        \end{Comment}
        \thm Любой язык, который распознаётся ДМП-автоматом по пустому стеку, распознаётся ДМП-автоматом по терминальному состоянию. Любой \textbf{беспрефиксный} язык, который распознаётся ДМП-автоматом по терминальному состоянию, распознаётся ДМП-автоматом по пустому стеку.
        \begin{Proof}
            Как из автомата по пустому стеку сделать автомат по терминальному состоянию? Да точно также, как и раньше, делаем состояние, которое кладёт новый маркер дна, после чего из всех состояний по новому маркеру делаем $\varepsilon$-переход в единственное терминальное состояние. Несложно заметить, что этот автомат будет детерминированным, потому что ни $\varepsilon,Z_0/Z_0Z_1$, ни переход $\varepsilon,Z_1/\varepsilon$ из всех остальных состояний детерминированность не нарушат.\\
            А как наоборот? Да тоже то же самое, что было раньше, но не совсем. Пусть мы имеем автомат с допуском по терминальному состоянию. Мы хотим из всех терминальных состояний сделать $\varepsilon$-переход в состояние, которое опустошит стек. Почему это нам ничего не поломает? А вот почему. Нас язык беспрефиксный, то есть придя в терминальное состояние единожды, больше мы в него не придём. В таком случае можно безнаказанно обрезать все переходы, которые из терминальных состояний были раньше, и теперь из каждого терминального состояния будет только один переход, а значит детерминированность.
        \end{Proof}
        \thm Существует КС-язык, не распознаваемый ДМП-автоматом.
        \begin{Example}
            Рассмотрим $L=\{0^n1^n\}\cup\{0^n1^{2n}\}$. Предположим, что такая штука распознаётся ДМП-автоматом. В чём с этим языком идейно проблема? В том, что мы сначала едим $n$ нулей (и кладём в стек что-то, что даст нам считать единицы), потом едим $n$ единиц, и, скорее всего, опустошаем стек, чтобы считать единицы. Как мы потом снова посчитаем до $n$, непонятно. Это, разумеется, не доказательство, а идея, но всё же.\\
            А теперь докажем.
            \begin{Proof}
                Очевидно, этот язык контекстно-свободен (как минимум как объединение двух КС-языков).\\
                Предположим, что для него существует ДМП-автомат. Тогда рассмотрим язык $L'=L\cup\{0^n1^n2^n\}$. Язык $L'$ является не контекстно-свободным, что можно несложно доказать при помощи леммы о разрастании почти также, как и для просто $\{0^n1^n2^n\}$. Но можно также доказать и при помощи доказанной ранее теоремы о пересечении КС- и регулярного языка. Если $L'$ контекстно-свободен, то его пересечение с $00^*11^*22^*$ также контекстно-свободно, а оно, очевидно, нет.\\
                Итак, $L'$ --- не КС-язык. А давайте теперь на основе ДМП для $L$ сделаем МП для $L'$. Как сделаем? Да тривиально вообще. Как выглядит ДМП? Он по строке $0^n1^n$ идёт в терминальное состояние, из которого после ещё $1^n$ идёт в другое (или то же самое) терминальное. Теперь давайте лёгким движением руки превратим автомат для $\{0^n1^n\}\cup\{0^n1^{2n}\}$ в автомат для $\{0^n2^n\}\cup\{0^n2^{2n}\}$. А теперь давайте возьмём все терминальные состояния первого автомата, и добавим из них $\varepsilon,*/*$-переход в соответствующее терминальное состояние второго. Получим уже недетерминированный, но всё ещё МП-автомат, который будет распознавать что? Ну, $L'$, на самом деле. Почему? У нас точно принимаются все слова $L$. Но ещё есть слова, получаемые так: мы сначала ходим в первом автомате, приходим в терминальное состояние, спускаемся во второй, и ходим после этого в нём. Но что тогда? Тогда на нашем пути мы дошли до терминального состояния, в потом из него до другого. Тривиально, если бы мы так в первом автомате ходили, мы бы получили сначала строку $0^n1^n$, а потом ещё $1^n$, потому что только такие у нас есть возможности для префиксов. Но ведь вторую часть пути мы во втором ходили, а значит к $0^n1^n$ у нас приписалось $2^n$, а не $1^n$. Это как раз и есть язык $L'$. Он не КС, значит его задать автоматом нельзя, а мы задали, то есть получили противоречие.
            \end{Proof}
        \end{Example}
        \begin{Comment}
            В чём тут глобально идея? В том что МП-автомат для $\{0^n1^n\}\cup\{0^n1^{2n}\}$ очень сильно использует недетерминированность. Мы с самого начала делаем недетерминированный выбор, какое слово мы принимаем, и не можем сделать этот выбор в середине слова.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Неоднозначные грамматики}. \undercolorblack{orange}{Неоднозначные языки}.}
    \begin{itemize}
        \dfn Как мы помним, \undercolor{red}{грамматика является однозначной}, если для любого слова существует не более одного дерева разбора.
        \dfn \undercolor{red}{Существенно неоднозначный язык} --- КС-язык, для которого не существует однозначной грамматики.
        \begin{Comment}
            Прежде чем привести пример такого (тем самым доказав из существование), рассмотрим некую модификацию леммы о разрастании.
        \end{Comment}
        \thm \undercolor{darkgreen}{Лемма Огдена}.
        \[\begin{split}
            \exists&n\in\mathbb N~\forall w:|w|\geqslant n~\text{Для любого способа \textit{пометить} }\geqslant n\text{ позиций в слове }w\\
            &\exists u,v,x,y,z:w=uvxyz:\left\{\begin{aligned}
                &x\text{ содержит помеченную позицию}\\
                &uv\text{ либо }yz\text{ содержит помеченную позицию}\\
                &vxy\text{ содержит не более }n\text{ помеченных позиций}
            \end{aligned}\right.\forall k\in\mathbb N~uv^kxy^kz\in L
        \end{split}\]
        \begin{Comment}
            В частности, лемма о разрастании КС-языка (почти) является частным случаем леммы Огдена, где мы пометили все символы вообще.
        \end{Comment}
        \begin{Proof}
            Доказывать это нам впадлу, но идейно лемма доказывается также, как и лемма о разрастании КС-языков.
        \end{Proof}
        \begin{Comment}
            При помощи этой леммы можно значительно проще доказать, что $0^n1^n2^n$ не является контекстно-свободным. Достаточно лишь рассмотреть строку $0^n1^n2^n$, где мы пометили все нули. Тогда $u$ и $v$ будут полностью из нулей состоять, а значит $y$ должен накачивать одновременно единицы и двойки, а он не может.
        \end{Comment}
        \begin{Example}
            Отсюда легко доказать существенную неоднозначность языка $\{0^n1^n2^m\}\cup\{0^n1^m2^m\}$. Он, тривиально, КС. Значит для него выполнена лемма Огдена. Рассмотрим слово $0^n1^n2^{n!+n}$. И всё также пометим в нём все нули. Тогда $x$ имеет помеченную позицию, значит $uv$ состоит только из нулей. Если $uv$ пусто, то $y$ содержит нули, а значит мы уже проиграли. Значит $v$ состоит из ненулевого количества нулей $i$. Что тогда можно сказать про $y$? Что оно состоит из такого же количества единиц (можно просто случаи поразбирать и понять, что иначе не работает). А значит можно накачать это слово на $k=\frac{n!}i+1$, получив слово $0^{n!+n}1^{n!+n}2^{n!+n}$. А значит слово $0^{n!+n}1^{n!+n}2^{n!+n}$ имеет дерево разбора какое-то такое:
            \begin{figure}[H]
                \begin{tikzpicture}[scale=.5]
                    \draw (-.5,0) -- (.5,0) -- (0,1) node[below=1.5mm] {$A$} -- cycle;
                    \draw (.5,0) -- (3,0) -- (0,6) node[below=1.5mm] {$A$} node[below=6mm] {$A$} node[below=10.5mm] {$A$} node[below=15mm] {$A$} node[below=19.5mm] {$A$} -- (-3,0) -- (-.5,0);
                    \draw (3,0) -- (10,0) -- (0,8) node[below=1.5mm] {$S$} -- (-4,0) -- (-3,0);
                    \draw (-4,-1) rectangle node[midway] {$0$} (-.5,0) rectangle (.5,-1) rectangle node[midway] {$1$} (5,0) rectangle node[midway] {$2$} (10,-1);
                \end{tikzpicture}
            \end{figure}\noindent
            Ту же самую операцию можно сделать со словом $0^{n!+n}1^n2^n$, получив другую структуру в дереве разбора слова $0^{n!+n}1^{n!+n}2^{n!+n}$. Но эти структуры могут быть в одном дереве разбора, нет?
            \begin{figure}[H]
                \begin{tikzpicture}[scale=.5]
                    \draw (-.5,0) -- (.5,0) -- (0,1) node[below=1.5mm] {$A$} -- cycle;
                    \draw (.5,0) -- (3,0) -- (0,6) node[below=1.5mm] {$A$} node[below=6mm] {$A$} node[below=10.5mm] {$A$} node[below=15mm] {$A$} node[below=19.5mm] {$A$} -- (-3,0) -- (-.5,0);
                    \draw (4.5,0) -- (5.5,0) -- (5,1) node[below=1.5mm] {$B$} -- cycle;
                    \draw (5.5,0) -- (8,0) -- (5,6) node[below=1.5mm] {$B$} node[below=6mm] {$B$} node[below=10.5mm] {$B$} node[below=15mm] {$B$} node[below=19.5mm] {$B$} -- (2,0) -- (4.5,0);
                    \draw (3,0) -- (9,0) to[in=-30,out=110] (2.5,9) node[below=1.5mm] {$S$} to[out=-150,in=70] (-4,0) -- (-3,0);
                    \draw (-4,-1) rectangle node[midway] {$0$} (-.5,0) rectangle (.5,-1) rectangle node[midway] {$1$} (4.5,0) rectangle (5.5,-1) rectangle node[midway] {$2$} (9,0);
                \end{tikzpicture}
            \end{figure}\noindent
            Тогда у нас есть цепочка из $A$, которая наращивает 0 и 1, и есть цепочка из $B$, которая наращивает 1 и 2. Тогда мы можем $A$ и $B$ ещё раз повторить. Давайте повторим их оба. Тогда количество единиц вырастет на $i+j$, количество нулей --- на $i$, количество двоек --- на $j$. Но как-то слово $0^{n!+n+i}1^{n!+n+i+j}2^{n!+n+j}$ не принадлежит нашему языку, хотя дерево разбора для него имеется. Значит это всё-таки два разных дерева разбора, а значит язык существенно неоднозначен.
        \end{Example}
    \end{itemize}
\end{document}