\documentclass{article}
\input{Headers/header}
\input{Headers/old-formal}

\newcommand*{\ab}[1][a;b]{{\left\langle #1\right\rangle}}
\newcommand*{\Var}[2]{{\overset{#2}{\underset{#1}{\scalebox{1.2}{V}}}}}

\makeatletter
\newcommand*{\romanic}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\geometry{legalpaper, paperheight=16383pt, margin=1in}
\setcounter{totalnumber}{100}
\pagestyle{empty}

\begin{document}
    \section{Интегральное счисление функций одной вещественной переменной.}
    \paragraph{\undercolorblack{orange}{Первообразная, неопределённый интеграл.}}
    \begin{itemize}
        \dfn Пусть $f,F\colon\langle a;b\rangle\to\mathbb R$. Тогда $F$ --- \undercolor{red}{первообразная} $f$ на $\langle a;b\rangle$, если $F'=f$ на $\langle a;b\rangle$.
        \begin{Comment}
            Пусть дана $f$. Возникают вопросы: существует ли $F$, если да, то сколько, и как их найти.\\
            Ответ на первый в простых терминах неизвестен. Но мы уже знаем, что уж точно не всякая функция имеет первообразную. Из теоремы Дарбу мы знаем, что производная не имеет разрывов 1 рода. Так, $\sign$ не имеет первообразной на промежутке, содержащем $0$.
        \end{Comment}
        \thm Всякая непрерывная на промежутке функция имеет на нём первообразную.
        \begin{Comment}
            Доказательство это довольно сложное, сейчас мы неспособны это сделать. Докажем мы в следующем параграфе, где зададим определённый интеграл, после чего построим первообразную.
        \end{Comment}
        \begin{Comment}
            Однако мы знаем, что $f(x)=\begin{cases}
                x^2\sin\frac1x & x\neq0\\0 & x=0
            \end{cases}$ имеет производную, причём разрывную.\\
            То есть на первый вопрос мы ответить не можем.
        \end{Comment}
        \begin{Comment}
            А что с единственностью? Ну, если мы уже знаем, что $F$ --- первообразная $f$, то совершенно справедливо, что $\forall C\in\mathbb R~F(x)+C$ --- также первообразная $f$. Более сложное утверждение --- других первообразных нет.
        \end{Comment}
        \thm Если $F$ --- первообразная $f$ и $\Phi$ --- первообразная $f$, то $\exists C\in\mathbb R~\forall x\in\langle a;b\rangle~\Phi=F+C$.
        \begin{Proof}
            Составим разность $\Phi-F$. Тогда $(\Phi-F)'=0$, что значит, что $\Phi-F$ --- постоянная функция (по признаку постоянства функции). Эта постоянная и есть $C$.
        \end{Proof}
        \dfn Пусть $f$ имеет первообразную на $\ab$. Её \undercolor{red}{неопределённым интегралом} называется множество всех её первообразных.
        \begin{Comment}
            Обозначается это так: $\int f$. Тогда $f$ называется подынтегральной функцией. Промежуток $\langle a;b\rangle$ тут не участвует, однако понятно, что подразумевается.\\
            Можно также писать более традиционное выражение: $\int f(x)~\mathrm dx$. Здесь мы \textbf{пока что} не вводим отдельного смысла для $\mathrm dx$, но потом мы будем обращаться с ним как с обычным дифференциалом.\\
            Также имеет смысл заметить следующее. По сути $\int f$ --- это множество вида $\{F+C\mid C\in\mathbb R\}$. Но обычно фигурные скобки опускают и пишут что-то такое: $\int f=F+C$, подразумевая при этом равенство множеств.\\
            Для проверки правильности выражения $\int f=F+C$ достаточно проверить равенство $F'=f$.
        \end{Comment}
        \begin{Comment}
           Аналогично таблице производных, мы можем написать таблицу интегралов:
           Полностью переписывать я её не буду, а лишь дам некоторые замечания.\\
           Например, $\int\frac{\mathrm dx}x=\ln x+C$ при $x>0$. Но при $x<0$ $\int\frac{\mathrm dx}x=\ln(-x)+C$, что даёт нам право написать $\int\frac{\mathrm dx}x=\ln|x|+C$, но с одной оговоркой. Это верно только при $x$ на интервале, не содержащем нуля. Однако у данного тезиса есть проблема, если мы хотим полный ответ на вопрос, чему равно $\int\frac{\mathrm dx}x$. Ведь мы можем сдвинуть $\ln x$ и $\ln(-x)$ на разные константы $C$ на луче $(0;+\infty)$ и луче $(-\infty;0)$. Что значит, что $\int\frac{\mathrm dx}x=\ln|x|+\begin{cases}
               C_1 & x>0\\
               C_2 & x<0
           \end{cases}$. Или же можно по-другому трактовать $C$ в обозначении $\int\frac{\mathrm dx}x=\ln|x|+C$. Можно просто сказать, что $C$ — это не совсем константа, а своя константа на каждом промежутке области определения. Эта трактовка поможет нам в интегрировании $\int\frac{\mathrm dx}{\cos^2x}$. Это, вроде как, $\tan x+C$, но тут Таких констант $C_1$, $C_2$ и так далее будет счётное множество.\\
           Другой комментарий касается $\int\frac{\mathrm dx}{\sqrt{1-x^2}}$. С одной стороны это равно $\arcsin x+C$. С другой --- $-\arccos x+C$. Но несмотря на то, что $\arcsin x+C=\int\frac{\mathrm dx}{\sqrt{1-x^2}}=-\arccos x+C$ и $\arcsin x+C=-\arccos x+C$, неверно что $\arcsin x=-\arccos x$. А всё потому, что константы $C$ разные (отличаются на $\frac\pi2$), поэтому немного проще для понимания будет такое написание: $\int\frac{\mathrm dx}{\sqrt{1-x^2}}=\arcsin x+C_1=-\arccos x+C_2$, где $C_2=C_1+\frac\pi2$. Далее мы будем просто писать $C$, не уточняя связь между $C_1$, $C_2$ и прочими, если есть.\\
           Ещё можно написать вот такие два выражения, которые просто часто бывают нужны:
           $$\int\frac{\mathrm dx}{\sqrt{x^2\pm1}}=\ln|x+\sqrt{x^2\pm1}|+C$$
           $$\int\frac{\mathrm dx}{1-x^2}=\frac12\ln\left|\frac{1+x}{1-x}\right|+C$$
           Их проверка очевидна и заключается в дифференцировании правых частей.
        \end{Comment}
        \thm \undercolor{darkgreen}{Арифметические действия над неопределёнными интегралами}. Пусть $f$ и $g$ имеют первообразные на промежутке $\ab$, а $\alpha\in\mathbb R$. Тогда.
        \begin{enumerate}
            \item $f+g$ имеет первообразную на $\ab$ и $\int f+g=\int f+\int g$.
            \item $\alpha f$ имеет первообразную на $\ab$ и при $\alpha\neq0$ $\int\alpha f=\alpha\int f$.
        \end{enumerate}
        \begin{Proof}
            Обозначим за $F$ и $G$ --- первообразные $f$ и $g$ соответственно. Тогда из свойств производных $F+G$ --- первообразная $f+g$, а $\alpha F$ --- первообразная $\alpha f$. Этот факт даёт нам наличие первообразных. Теперь проверим формулы.\\
            Что у нас написано в формуле $\int f+g=\int f+\int g$? Что $\{F+G+C\mid C\in\mathbb R\}=\{F+C_1\mid C_1\in\mathbb R\}+\{G+C_2\mid C_2\in\mathbb R\}$. То есть справа написана сумма множеств (являющаяся также множеством). Тогда обозначим левое множество за $\Lambda$, а правое --- за $\Pi$. Если $h\in\Lambda$, то взяв $C_1=C$, $C_2=0$ получим, что $h\in\Pi$. Если $h\in\Pi$, то возьмём $C=C_1+C_2$ и получим, что $h\in\Lambda$.\\
            Аналогично доказывается второе утверждение: $\{\alpha F+C\mid C\in\mathbb R\}=\{\alpha F+\alpha C_1\mid C_1\in\mathbb R\}$. Понятно, что можно взять $C=\alpha C_1$ и аналогично доказать.
        \end{Proof}
        \begin{Comment}
            А что с $\alpha=0$? Ну, тогда не выполняется, ведь $0\int f$ --- множество из одного нуля, а $\int 0f$ --- множество постоянных функций.
        \end{Comment}
        \begin{Comment}
            Первую формулу можно переписать так: $\int f+g=F+\int g$ (вместо $\int f+g=F+C+\int g$). Очевидно, почему.
        \end{Comment}
        \thm \undercolor{darkgreen}{Замена переменной в неопределённом интеграле}. Пусть $f\colon\ab\to\mathbb R$, а $\varphi\colon\ab[\alpha;\beta]\to\ab$. Пусть $f$ имеет первообразную, а $\varphi$ дифференцируема. Тогда
        $$\int f(\varphi(t))\varphi'(t)~\mathrm dt=\int f(x)~\mathrm dx\big|_{x=\varphi(t)}$$.
        \begin{Proof}
            Обозначим первообразную $f$ за $F$. Тогда $F\circ\varphi$ --- первообразная $(f\circ\varphi)\varphi'$ (из производной композиции). Тогда обе части равенства очевидно равны $F(\varphi(t))+C$.
        \end{Proof}
        \begin{Example}
            Правило подстановки можно применять и справа налево, и слева направо. Справа налево --- совершенно бесхитростно.
            $$\int e^{\sin t}\cos t~\mathrm dt=\int e^x~\mathrm dx\big|_{x\sin t}=e^{\sin t}+C$$
            А как наоборот? Пусть $\exists\varphi^{-1}$. Тогда
            $$\int f(x)~\mathrm dx=\int f(\varphi(t))\varphi'(t)~\mathrm dt\big|_{t=\varphi^{-1}(x)}\overset{(*)}=G(t)+C\big|_{t=\varphi^{-1}(x)}=G(\varphi^{-1}(x))+C$$
            А значит, если левая часть равенства $(*)$ проще, чем был исходный интеграл (т.е. там что-то упростилось), то такая замена вполне возможна и осмысленна.
        \end{Example}
        \begin{Example}
            Простым примером теоремы о замене переменной является вот такая формула:
            $$\int f(\alpha t+\beta)~\mathrm dt=\frac1\alpha F(\alpha t+\beta)+C$$
        \end{Example}
        \thm \undercolor{darkgreen}{Интегрирование по частям в неопределённом интеграле}. Пусть $f$ и $g$ дифференцируемы на $\ab$ и $f'g$ имеет первообразную. Тогда $fg'$ имеет первообразную и $\int fg'=fg-\int f'g$.
        \begin{Comment}
            Если у нас функции непрерывно дифференцируемы, то вопрос о наличии первообразной однозначно решается, потому что тогда $fg'$ непрерывно, следовательно имеет первообразную.
        \end{Comment}
        \begin{Proof}
            Мы уже знаем, что $(fg)'=f'g+fg'$. Тогда $fg'=(fg)'-f'g$. Ну, собственно, всё: интеграл разности --- разность интегралов, а константу из $\int(fg)'$ можно не приписывать, она уже есть из $\int f'g$.
        \end{Proof}
        \begin{Comment}
            Также эту формулу часто пишут в дифференциалах: $\int f~\mathrm dg=fg-\int g~\mathrm df$.\\
            Ещё по историческим причинам в данной формуле любят обозначать функции не $f$ и $g$, а $u$ и $v$.
        \end{Comment}
        \begin{Example}
            $$\int\ln xx^\alpha~\mathrm dx$$
            Возьмём $f=\ln x$, $g'=x^\alpha$. Тогда (при $\alpha\neq-1$) $f'=\frac1x$, $g=\frac{x^{\alpha-1}}{\alpha+1}$.
            Тогда
            $$\int\ln xx^\alpha~\mathrm dx=\frac{x^{\alpha-1}}{\alpha+1}\ln x-\int\frac{x^\alpha}{\alpha+1}~\mathrm dx=\frac{x^{\alpha-1}}{\alpha+1}\ln x-\frac{x^{\alpha+1}}{(\alpha+1)^2}+C$$
            А что в случае $\alpha=-1$? Ну, то же самое, но $f=\ln x$. Тогда
            $$\int\frac{\ln x}x~\mathrm dx=\ln^2x-\int\frac{\ln x}x~\mathrm dx$$
            Мы свели интеграл к самому себе, его можно рассмотреть как переменную и решить уравнение. Тогда
            $$\int\frac{\ln x}x~\mathrm dx=\frac12\ln^2x+C$$
            Этот способ называется сведением интеграла к самому себе. Впрочем, этот интеграл при $\alpha=-1$ проще взять заменой переменной $\ln x=t$.
        \end{Example}
        \begin{Comment}
            Поговорим вот о чём. Производные элементарных функций были также элементарными функциями. Для интегралов это неверно. Такие интегралы называются неберущимися.
        \end{Comment}
        \begin{Example}
            \begin{tabular}{|c|c|}
                \hline
                $\int\frac{\sin x}x~\mathrm dx$ & интегральный синус\\
                $\int\frac{\cos x}x~\mathrm dx$ & интегральный косинус\\
                $\int\frac{e^x}x~\mathrm dx$ & интегральная экспонента\\
            $\int\frac{\mathrm dx}{\ln x}$ & интегральный логарифм\\
                $\int e^{-x^2}~\mathrm dx$ & функция Гаусса, функция ошибок, функция распределения вероятности\\
                $\int\sqrt{P_{3,4}(x)}~\mathrm dx$ & эллиптический интеграл ($P_{3,4}$ --- некий многочлен 3 либо 4 степени)\\
                \hline
            \end{tabular}
        \end{Example}
        \begin{Comment}
            Неберущиеся интегралы выходят далеко за пределы математического анализа (и изучаются интегральной алгеброй), поэтому доказывать, что они неберущиеся мы уж точно не будем.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Интеграл Римана, интегрируемые функции}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Мы хотим найти площадь под графиком функции на отрезке $[a;b]$. Как подойти к этой задаче? Можно разбить отрезок на $n$ частей, снабдить каждый произвольной точкой $\xi_k$ на нём, сказать, что площадь примерно равна суммой площадей прямоугольников с высотой $f(\xi_k)$, а дальше сказать, что при стремлении разбиения к бесконечно мелкому эта штука стремится к константе.
            \begin{center}
                \pgfmathsetseed{795339314}
                \begin{tikzpicture}
                    \begin{axis}[
                        width = 10cm,
                        height = 10cm,
                        grid = none,
                        xmin = 0,
                        xmax = 4.5,
                        ymin = .5,
                        ymax = 5,
                        axis equal,
                        axis x line = middle,
                        axis y line = middle,
                        axis line style = {->},
                        xtick = \empty,
                        ytick = \empty,
                        xlabel = \empty,
                        ylabel = \empty,
                        ]
                        \addplot[domain=.5:3.5,samples=50,red]{-1/2*x^4+47/12*x^3-10*x^2+115/12*x};
                        
                        \foreach\i in {0,...,12}
                        {
                            \pgfmathsetmacro{\x}{{\i/4+.5}}
                            \pgfmathsetmacro{\func}{{-1/2*\x^4+47/12*\x^3-10*\x^2+115/12*\x}}
                            \edef\temp{\noexpand\node at (axis cs:\x,.6) {$x_{\i}$};}
                            \temp
                        }
                        \foreach\i in {0,...,11}
                        {
                            \pgfmathsetmacro{\x}{{\i/4+.5}}
                            \pgfmathsetmacro{\xn}{{\x+.25}}
                            \pgfmathsetmacro{\rnd}{{random(\x * 64,\xn * 64) / 64}}
                            \pgfmathsetmacro{\func}{{-1/2*\rnd^4+47/12*\rnd^3-10*\rnd^2+115/12*\rnd}}
                            \edef\temp{\noexpand\draw (axis cs:\x,\func) -- (axis cs:\xn,\func);}
                            \temp
                            
                            \edef\temp{\noexpand\draw (axis cs:\x,.75) -- (axis cs:\x,\func);}
                            \temp
                            \edef\temp{\noexpand\draw (axis cs:\xn,.75) -- (axis cs:\xn,\func);}
                            \temp
                        }
                    \end{axis}
                \end{tikzpicture}
            \end{center}
        \end{Comment}
        \dfn $\tau=\{x_k\}_{k=0}^n$, где $x_0=a$, $x_n=b$ --- \undercolor{red}{дробление} (либо \undercolor{red}{разбиение}) отрезка $[a;b]$. В разбиение обозначение $\Delta x_k$ --- это $x_{k+1}-x_k$.
        \dfn \undercolor{red}{Мелкость дробления} $\tau$ --- число $\lambda_\tau=\max_{0\leqslant k<n}\Delta x_k$.
        \dfn Если $\tau$ --- дробление, а $\xi=\{\xi_k\}_{k=0}^{n-1}$, где $\xi\in[x_k;x_{k+1}]$, то $\xi$ --- \undercolor{red}{оснащение} дробления $\tau$.
        \dfn Дробление с оснащением --- \undercolor{red}{оснащённое дробление}.
        \dfn Пусть задана $f\colon[a;b]\to\mathbb R$. Тогда суммы $\sigma=\sigma_\tau(f;\xi)=\sum\limits_{k=0}^{n-1}f(\xi_k)\Delta x_k$ называются \undercolor{red}{интегральными суммами} (либо \undercolor{red}{суммами Римана}), отвечающим оснащённому дроблению $(\tau;\xi)$.
        \dfn Число $I\in\mathbb R$ называется \undercolor{red}{пределом интегральных сумм при $\lambda\to0$}, если $\forall\varepsilon>0~\exists\delta>0~\forall\tau:\lambda_\tau<\delta~\forall\xi~|\sigma_\tau(f,\xi)-I|<\varepsilon$.
        \begin{Comment}
            По крайней мере формально, эта конструкция не вкладывается в определение предела, которое у нас было в метрическом пространстве. Не вкладывается потому, что $\sigma$ много от чего зависит (точнее, от $\tau$ и $\xi$), и странно считать её функцией. Поэтому математики придумали особую конструкцию предела (предел по базе), который и эту конструкцию в себя включает. Но в нашем учебном курсе настолько высокая абстракция будет иметь мало применений (данный --- чуть ли не единственный), а значит проще будет разобраться с каждым отдельно. Конкретно сейчас можно вообще свести всё к последовательностям (индексы которых му напишем сейчас сверху): $\forall\{(\tau^{(j)};\xi^{(j)})\}_{j=1}^\infty:\lambda_{\tau^{(j)}}\to0~\sigma_{\tau^{(j)}}(f;\xi^{(j)})\to I$, что даёт нам право использовать базовые свойства пределов. Доказательство равносильности определений аналогично доказательству равносильности непрерывности по Гейне и по Коши.
        \end{Comment}
        \dfn Если существует $\lim\limits_{\lambda\to0}\sigma=I\in\mathbb R$, то функция $f$ называется \undercolor{red}{интегрируемой по Риману} на $[a;b]$, а $I$ --- её определённым интегралом. Множество функций, интегрируемых на отрезке $[a;b]$ по Риману обозначается $R[a;b]$
        \begin{Comment}
            Аналогично с неопределённым интегралом, есть минималистичное обозначение $\int_a^bf$ и традиционное $\int_a^bf(x)~\mathrm dx$. Значок интеграла --- видоизменённая буква S, от слова <<сумма>>.\\
            Также мы будем говорить <<интегрируемая>> вместо <<интегрируемая по Риману>>, потому что у нас пока нет интегрируемости по Лебегу.
        \end{Comment}
        \begin{Comment}
            Тут мы хотим узнавать, какие функции являются интегрируемыми, а какие --- нет. Пока введём ещё несколько обозначений.
        \end{Comment}
        \dfn Пусть $f\colon[a;b]\to\mathbb R$, $\tau$ --- дробление $[a;b]$. Пусть
        $$M_k=\sup\limits_{[x_k;x_{k+1}]}f(x)\qquad\qquad m_k=\inf\limits_{[x_k;x_{k+1}]}f(x)$$
        Тогда $S=S_\tau(f)=\sum\limits_{k=0}^{n-1}M_k\Delta x_k$ и $s=s_\tau(f)=\sum\limits_{k=0}^{n-1}m_k\Delta x_k$ называются \undercolor{red}{верхней} и \undercolor{red}{нижней}\\\undercolor{red}{интегральными суммами} соответственно (или \undercolor{red}{верхними} и \undercolor{red}{нижними суммами Дарбу}).
        \begin{Comment}
            \begin{center}
                \begin{tikzpicture}
                    \begin{axis}[
                        width = 10cm,
                        height = 10cm,
                        grid = none,
                        xmin = 0,
                        xmax = 4.5,
                        ymin = .5,
                        ymax = 5,
                        axis equal,
                        axis x line = middle,
                        axis y line = middle,
                        axis line style = {->},
                        xtick = \empty,
                        ytick = \empty,
                        xlabel = \empty,
                        ylabel = \empty,
                        ]
                        \addplot[domain=.5:3.5,samples=50,red]{-1/2*x^4+47/12*x^3-10*x^2+115/12*x};
                        
                        \foreach\i in {0,...,12}
                        {
                            \pgfmathsetmacro{\x}{{\i/4+.5}}
                            \pgfmathsetmacro{\func}{{-1/2*\x^4+47/12*\x^3-10*\x^2+115/12*\x}}
                            \edef\temp{\noexpand\node at (axis cs:\x,.6) {$x_{\i}$};}
                            \temp
                        }
                        \foreach\i in {0,...,11}
                        {
                            \pgfmathsetmacro{\x}{{\i/4+.5}}
                            \pgfmathsetmacro{\xn}{{\x+.25}}
                            \pgfmathsetmacro{\mx}{{max(-1/2*\x^4+47/12*\x^3-10*\x^2+115/12*\x,-1/2*\xn^4+47/12*\xn^3-10*\xn^2+115/12*\xn)}}
                            \edef\temp{\noexpand\draw (axis cs:\x,\mx) -- (axis cs:\xn,\mx);}
                            \temp
                            
                            \edef\temp{\noexpand\draw (axis cs:\x,.75) -- (axis cs:\x,\mx);}
                            \temp
                            \edef\temp{\noexpand\draw (axis cs:\xn,.75) -- (axis cs:\xn,\mx);}
                            \temp
                        }
                    \end{axis}
                \end{tikzpicture}
                
                \begin{tikzpicture}
                    \begin{axis}[
                        width = 10cm,
                        height = 10cm,
                        grid = none,
                        xmin = 0,
                        xmax = 4.5,
                        ymin = .5,
                        ymax = 5,
                        axis equal,
                        axis x line = middle,
                        axis y line = middle,
                        axis line style = {->},
                        xtick = \empty,
                        ytick = \empty,
                        xlabel = \empty,
                        ylabel = \empty,
                        ]
                        \addplot[domain=.5:3.5,samples=50,red]{-1/2*x^4+47/12*x^3-10*x^2+115/12*x};
                        
                        \foreach\i in {0,...,12}
                        {
                            \pgfmathsetmacro{\x}{{\i/4+.5}}
                            \pgfmathsetmacro{\func}{{-1/2*\x^4+47/12*\x^3-10*\x^2+115/12*\x}}
                            \edef\temp{\noexpand\node at (axis cs:\x,.6) {$x_{\i}$};}
                            \temp
                        }
                        \foreach\i in {0,...,11}
                        {
                            \pgfmathsetmacro{\x}{{\i/4+.5}}
                            \pgfmathsetmacro{\xn}{{\x+.25}}
                            \pgfmathsetmacro{\mn}{{min(-1/2*\x^4+47/12*\x^3-10*\x^2+115/12*\x,-1/2*\xn^4+47/12*\xn^3-10*\xn^2+115/12*\xn)}}
                            \edef\temp{\noexpand\draw (axis cs:\x,\mn) -- (axis cs:\xn,\mn);}
                            \temp
                            
                            \edef\temp{\noexpand\draw (axis cs:\x,.75) -- (axis cs:\x,\mn);}
                            \temp
                            \edef\temp{\noexpand\draw (axis cs:\xn,.75) -- (axis cs:\xn,\mn);}
                            \temp
                        }
                    \end{axis}
                \end{tikzpicture}
            \end{center}
            Первая картинка --- верхние суммы Дарбу, вторая --- нижние. Как видно, есть небольшое отличие от изображённых в начале параграфа произвольных интегральных сумм.
        \end{Comment}
        \thm Очевидно, что $S<+\infty\Leftrightarrow f\text{ ограничена сверху}$ и $s>-\infty\Leftrightarrow f\text{ ограничена снизу}$.
        \begin{Comment}
            Ст\'{о}ит заметить, что суммы Дарбу не обязаны являться суммами Римана. В случае непрерывной $f$ --- обязаны, а в общем случае --- не обязательно.
        \end{Comment}
        \thm \undercolor{darkgreen}{Свойства сумм Дарбу}.
        \begin{enumerate}
            \item $S_\tau(f)=\sup\limits_\xi\sigma_\tau(f;\xi)$, $s_\tau(f)=\inf\limits_\xi\sigma_\tau(f;\xi)$.
            \item При добавлении новых точек в дробление верхняя сумма Дарбу не увеличится, а нижняя --- не уменьшится.
            \item Верхняя сумма Дарбу всегда не меньше нижней (даже для другого дробления).
        \end{enumerate}
        \begin{Proof}
            \begin{enumerate}
                \item
                $\forall t\in[x_k;x_{k+1}]~f(t)\leqslant M_k\Rightarrow f(\xi_k)\leqslant M_k\Rightarrow f(\xi_k)\Delta x_k\leqslant M_k\Delta x_k$. У нас получится $n$ таких неравенств, их все можно сложить. Тогда мы получим, что $S_\tau(f)$ --- какая-то верхняя грань $\sigma_\tau(f;\xi)$.\\
                Докажем, что точная, отдельно для ограниченных и неограниченных сверху функций. Сначала для ограниченных. Возьмём произвольное $\varepsilon>0$ и докажем, что $S_\tau(f)-\varepsilon$ --- не верхняя грань. Для этого рассмотрим оснащение $\xi$, что $f(\xi_k)>M_k-\frac\varepsilon{b-a}$. Такое, очевидно, существует. А для него уже очевидно, что оно является противоречит тому, что $S_\tau(f)-\varepsilon$ --- верхняя грань.\\
                Теперь что делать, если $f$ не ограничена? Тогда, очевидно, она не ограничена на каком-то отрезке разбиения $[x_\nu;x_{\nu+1}]$. Возьмём $E$, для которого хочется доказать, что оно --- не верхняя грань. Раз уж $f$ не ограничена на $[x_\nu;x_{\nu+1}]$, на нём можно выбрать такое $\xi_\nu$, что $f(\xi_\nu)>\frac1{\Delta x_\nu}(E-\sum\limits_{k\neq\nu}f(\xi_k)\Delta x_k)$.
                \item
                Пусть у нас есть разбиение $\tau$ и есть $\tau'=\tau\cup\{c\}$. Хочется доказать, что $S=S_\tau\leqslant S_{\tau'}=S'$. Чем у нас отличаются по сути $S$ и $S'$? Большинство слагаемых у них одинаковы, а отличаются они тем, что в $S$ было некоторое $M_\nu\Delta x_\nu$, а в $S'$ вместо него поставили $M'(c-x_\nu)+M''(x_{\nu+1}-c)$. Несложно заметить, что $M'\leqslant M_\nu$ и $M''\leqslant M_\nu$. Но тогда $M'(c-x_\nu)+M''(x_{\nu+1}-c)\leqslant M_\nu(c-x_\nu)+M_\nu(x_{\nu+1}-c)=M_\nu\Delta x_\nu$. Ч.Т.Д.
                \item У нас есть два дробления $\tau_1$ и $\tau_2$. Мы хотим доказать, что $s_{\tau_1}\leqslant S_{\tau_2}$. Возьмём $\tau=\tau_1\cup\tau_2$. В общем-то всё. $s_{\tau_1}\leqslant s_\tau\leqslant S_\tau\leqslant S_{\tau_2}$.
            \end{enumerate}
        \end{Proof}
        \thm Интегрируемая на отрезке функция ограничена.
        \begin{Proof}
            Пусть $f$ не ограничена сверху. Докажем, что интегральные суммы не имеют предела (то есть что произвольное $I$ не является пределом). Из первого свойства следует, что $\forall\tau~\exists\xi~\sigma_\tau(f;\xi)>I+1$. Что противоречит тому, что $I$ --- предел $\sigma$.
        \end{Proof}
        \dfn Обозначим $I^*=\inf\limits_\tau S_\tau$ и $I_*=\sup\limits_\tau s_\tau$. $I^*$ и $I_*$ называются \undercolor{red}{верхним} и \undercolor{red}{нижним интегралами Дарбу} соответственно.
        \thm Очевидно, $I_*\leqslant I^*$.
        \thm Очевидно, что $I^*<+\infty\Leftrightarrow f\text{ ограничена сверху}$ и $I_*>-\infty\Leftrightarrow f\text{ ограничена снизу}$.
        \thm \undercolor{darkgreen}{Критерий интегрируемости функции}. Пусть $f\colon[a;b]\to\mathbb R$. Тогда $f\in R[a;b]\Leftrightarrow S_\tau(f)-s_\tau(f)\underset{\lambda\to0}\longrightarrow0$. Иначе говоря
        $$\forall\varepsilon>0~\exists\delta>0~\forall\tau:\lambda_\tau<\delta~\forall\xi~|S_\tau(f)-s_\tau(f)|<\varepsilon$$
        \begin{Proof}
            \rightimp. Пусть $I=\int_a^bf$. Надо доказать ту страшную штуку. Зафиксируем $\varepsilon$, для которого хочется найти $\delta$. Мы знаем, что $I=\int_a^bf$, что значит, что мы можем подобрать $\delta$, что $\forall\tau:\lambda_\tau<\delta~\forall\xi~I-\frac\varepsilon3<\sigma_\tau(f;\xi)<I+\frac\varepsilon3$. Тогда из свойства 1 $\forall\tau:\lambda_\tau<\delta~\forall\xi~I-\frac\varepsilon3\leqslant s_\tau(f)\leqslant S_\tau(f)\leqslant I+\frac\varepsilon3$. А это значит, что $S_\tau(f)-s_\tau(f)\leqslant\frac{2\varepsilon}3<\varepsilon$.\\
            \leftimp. Заметим, что $I_*=I^*\in\mathbb R$. Докажем, что это $I$. Мы совершенно точно знаем, что $s_\tau(f)\leqslant\sigma_\tau(f;\xi)\leqslant S_\tau(f)$. А ещё знаем, что $s_\tau(f)\leqslant I_*=I=I^*\leqslant S_\tau(f)$. А это значит, что $|\sigma_\tau(f;\xi)-I|\leqslant S_\tau(f)-s_\tau(f)$. А мы умеем для произвольного $\varepsilon$ подбирать $\delta$, что $S_\tau(f)-s_\tau(f)<\varepsilon$. Ну, вот и всё, тут это $\delta$ тоже подойдёт.
        \end{Proof}
        \thm Следствие: если $f\in R[a;b]$, то $S_\tau(f)\underset{\lambda\to0}\longrightarrow\int_a^bf$ и $s_\tau(f)\underset{\lambda\to0}\longrightarrow\int_a^bf$.
        \begin{Proof}
            $0\leqslant S_\tau(f)-I\leqslant S_\tau(f)-s_\tau(I)\underset{\lambda\to0}\longrightarrow0$
        \end{Proof}
        \dfn Обозначим $\omega_D(f)=\sup\limits_Df-\inf\limits_Df$ --- \undercolor{red}{колебание} функции $f$ на $D$.
        \thm Так можно переформулировать критерий так: $f\in R[a;b]\Leftrightarrow\sum\limits_{k=0}^{n-1}\omega_{[x_k;x_{k+1}]}(f)\Delta x_k\underset{\lambda\to0}\longrightarrow0$.
        \thm Верхние и нижние интегралы Дарбу определялись как грани верхних и нижних сумм. Оказывается, что это ещё и пределы (причём для любой функции):
        $$\forall f\colon[a;b]\to\mathbb R~S_\tau(f)\underset{\lambda\to0}\longrightarrow I^*\land s_\tau(f)\underset{\lambda\to0}\longrightarrow I_*$$
        \begin{Comment}
            Доказывать мы это не будем, как и пользоваться этим. Аналогично со следующими двумя утверждениями.
        \end{Comment}
        \thm Критерий Дарбу:
        $$f\in R[a;b]\Leftrightarrow f\text{ ограничена}\land I^*=I_*$$
        \thm Критерий Римана:
        $$f\in R[a;b]\Leftrightarrow\forall\varepsilon>0~\exists\tau\text{ --- дробление }[a;b]~S_\tau(f)-s_\tau(f)<\varepsilon$$
        \begin{Comment}
            То есть достаточно подобрать только одно дробление, а не нужно проверять все.\\
            А ещё у этого критерия есть геометрический смысл: мы берём дробление и исходя из него заключаем график в прямоугольники.
            \begin{center}
                \begin{tikzpicture}
                    \begin{axis}[
                        width = 10cm,
                        height = 10cm,
                        grid = none,
                        xmin = 0,
                        xmax = 4.5,
                        ymin = .5,
                        ymax = 5,
                        axis equal,
                        axis x line = middle,
                        axis y line = middle,
                        axis line style = {->},
                        xtick = \empty,
                        ytick = \empty,
                        xlabel = \empty,
                        ylabel = \empty,
                        ]
                        \addplot[domain=.5:3.5,samples=50,red]{-1/2*x^4+47/12*x^3-10*x^2+115/12*x};
                        
                        \foreach\i in {0,...,12}
                        {
                            \pgfmathsetmacro{\x}{{\i/4+.5}}
                            \pgfmathsetmacro{\func}{{-1/2*\x^4+47/12*\x^3-10*\x^2+115/12*\x}}
                            \edef\temp{\noexpand\node at (axis cs:\x,.6) {$x_{\i}$};}
                            \temp
                        }
                        \foreach\i in {0,...,11}
                        {
                            \pgfmathsetmacro{\x}{{\i/4+.5}}
                            \pgfmathsetmacro{\xn}{{\x+.25}}
                            \pgfmathsetmacro{\mn}{{min(-1/2*\x^4+47/12*\x^3-10*\x^2+115/12*\x,-1/2*\xn^4+47/12*\xn^3-10*\xn^2+115/12*\xn)}}
                            \pgfmathsetmacro{\mx}{{max(-1/2*\x^4+47/12*\x^3-10*\x^2+115/12*\x,-1/2*\xn^4+47/12*\xn^3-10*\xn^2+115/12*\xn)}}
                            
                            \edef\temp{\noexpand\draw (axis cs:\x,\mn) rectangle (axis cs:\xn,\mx);}
                            \temp
                            
                            \edef\temp{\noexpand\draw[dashed] (axis cs:\x,.75) -- (axis cs:\x,\mn);}
                            \temp
                            \edef\temp{\noexpand\draw[dashed] (axis cs:\xn,.75) -- (axis cs:\xn,\mn);}
                            \temp
                        }
                    \end{axis}
                \end{tikzpicture}
            \end{center}
            И хочется, чтобы у них была произвольно маленькая суммарная площадь.
        \end{Comment}
        \thm Непрерывная на отрезке функция интегрируема на нём ($C[a;b]\supset R[a;b]$).
        \begin{Proof}
            Будем на $\varepsilon$-языке проверять критерий интегрируемости. Зафиксируем $\varepsilon$. Как мы знаем по теореме Кантора, непрерывная на отрезке функция равномерно непрерывна. А значит мы можем по числу $\frac\varepsilon{b-a}$ подобрать такое $\delta$, что $\forall t';t''\in[a;b]:|t'-t''|<\delta~|f(t')-f(t'')|<\frac\varepsilon{b-a}$. Утверждается, что это $\delta$ нам подойдёт. То есть что для любого дробления, что $\lambda_\tau<\delta$ выполнено условие критерия. Почему так? Понятно, что колебание $\omega_{[x_k;x_{k+1}]}(f)<\frac\varepsilon{b-a}$. А это значит, что $\sum\limits_{k=0}^{n-1}\omega_{[x_k;x_{k+1}]}(f)\Delta x_k<\sum\limits_{k=0}^{n-1}\frac\varepsilon{b-a}\Delta x_k=\varepsilon$.
        \end{Proof}
        \thm Монотонная на отрезке функция интегрируема.
        \begin{Proof}
            Не умаляя общности $f\uparrow$. Если $f$ --- константа, то предыдущая теорема. Иначе $f(a)<f(b)$. Снова возьмём $\varepsilon>0$ и будем подбирать $\delta$. Утверждается, что $\delta=\frac\varepsilon{f(b)-f(a)}$ подойдёт. Возьмём дробление с рангом $<\delta$ и оценим сумму $\sum\limits_{k=0}^{n-1}\omega_{[x_k;x_{k+1}]}\Delta x_k$. Понятно, что на правом конце значение функции не меньше, чем на левом. А значит $\omega_{[x_k;x_{k+1}]}=f(x_{k+1})-f(x_k)$. Тогда $\sum\limits_{k=0}^{n-1}\omega_{[x_k;x_{k+1}]}\Delta x_k=\sum\limits_{k=0}^{n-1}(f(x_{k+1})-f(x_k))\Delta x_k\overset{(*)}<\sum\limits_{k=0}^{n-1}(f(x_{k+1})-f(x_k))\delta=\sum\limits_{k=0}^{n-1}(f(x_{k+1})-f(x_k))\frac\varepsilon{f(b)-f(a)}=\varepsilon$.\\
            Про $(*)$. Почему неравенство строгое? Потому что все $\Delta x_k$ не больше $\delta$, а хотя бы одно из $f(x_{k+1})-f(x_k)$ строго больше нуля.
        \end{Proof}
        \thm При изменении функции на конченом множестве интегрируемость не нарушится, а интеграл не изменится.
        \begin{Proof}
            Пусть у нас есть $f,\hat f\colon[a;b]\to\mathbb R$. И $f(x)=\hat f(x)$ для всех $x\in[a;b]\setminus\{x_1;\ldots;x_m\}$. Пусть $f$ интегрируема. Тогда она ограничена ($|f|\leqslant A$), а тогда, ясно, $|\hat f|\leqslant\max\{A;|\hat f(x_1)|;\ldots;|\hat f(x_m)|\}=\hat A$. Тогда $|\sigma_\tau(\hat f)-\sigma_\tau(f)|\leqslant 2m(A+\hat A)\lambda_\tau$ (количество отрезков на число, не меньшее их длины). А это выражение стремится к нулю при $\lambda_\tau\to0$.
        \end{Proof}
        \begin{Comment}
            Это свойство позволяет определить интеграл функции, заданной на $[a;b]$ без конечного числа точек. Для этого можно произвольным образом её доопределить: как мы показали, совершенно не важно как именно. Это позволяет нам интегрировать ступеньку. Или набор ступенек.
        \end{Comment}
        \thm \undercolor{darkgreen}{Интегрируемость функции и её сужения}
        \begin{enumerate}
            \item Если $f\in R[a;b]$, а $[\alpha;\beta]\supset[a;b]$, то $f\in R[\alpha;\beta]$ интегрируема.
            \item Если $c\in(a;b)$, а $f\in R[a;c]$ и $f\in R[c;b]$, то $f\in R[a;b]$.
        \end{enumerate}
        \begin{Proof}
            Снова будем проверять предел.
            \begin{enumerate}
                \item Возьмём $\varepsilon>0$ и подберём $\delta>0~\forall\tau_0\text{ --- дробление }[a;b]:\lambda_{\tau_0}<\delta~S_{\tau_0}-s_{\tau_0}<\varepsilon$. Докажем, что это же $\delta$ подходит для $[\alpha;\beta]$ тоже. Рассмотрим дробление $\tau$ отрезка $[\alpha;\beta]$, у которого ранг $<\delta$, и дополним его до $\tau_0$ --- дробления $[a;b]$ и рангом тоже $<\delta$. Тогда $S_{\tau_0}-s_{\tau_0}<S_{\tau}-s_{\tau}$, потому что в первой сумме (которая $\sum\limits_{k=0}^{n-1}\omega_{[x_k;x_{k+1}]}\Delta x_k$) будет тупо больше слагаемых.
                \item По данному $\varepsilon$ подберём $\delta_1$ и $\delta_2$ для обоих подотрезков по числу $\frac\varepsilon2$. Это было бы так просто, если бы $c$ всегда принадлежало дроблению $[a;b]$. Поэтому нужно подставить костыль.\\
                Сначала давайте подберём $\delta_1$ и $\delta_2$ по числу $\frac\varepsilon3$. Положим, что $f$ не постоянна. Тогда рассмотрим $\delta=\min\left\{\delta_1;\delta_2;\frac\varepsilon{3\omega_{[a;b]}(f)}\right\}$. Если $c\in\tau$, то всё понятно ($S_{\tau_1}-s_{\tau_1}+S_{\tau_2}-s_{\tau_2}<\frac{2\varepsilon}3<\varepsilon$). Если не попало, то есть отрезок разбиения $[x_\nu;x_{\nu+1}]\ni c$. Тогда понятно, что $\omega_{[x_\nu;x_{\nu+1}]}\Delta x_\nu<\omega_{[a;b]}\delta\leqslant\frac\varepsilon3$. Ну, теперь всё.
            \end{enumerate}
        \end{Proof}
        \thm При этом второй пункт предыдущего утверждения можно по индукции расширить на любое конечное количества точек.
        \dfn $f\colon[a;b]\to\mathbb R$ называется \undercolor{red}{кусочно-непрерывной}, если множество её точек разрыва конечно (либо пусто), при этом все её разрывы первого рода.
        \thm Кусочно-непрерывная функция интегрируема.
        \begin{Proof}
            Смотри несколько предыдущих утверждений.
        \end{Proof}
        \begin{Comment}
            Ещё есть критерий, связанный с мерой. Его мы доказывать не будем, как и нем будем вводить понятие меры, потому что мера у нас начнётся осенью. А пока нам достаточно будет сказать, что такое мера длины 0.
        \end{Comment}
        \dfn Говорят, что $e\subset\mathbb R$ \undercolor{red}{имеет нулевую меру}, если $\forall\varepsilon>0$ множество $e$ можно покрыть не более чем счётным количеством интервалов суммарной длины меньше $\varepsilon$.
        \begin{Comment}
            Что такое сумма конечного количества слагаемых, мы знаем, а сумма счётного количества слагаемых --- предел частичных сумм.
        \end{Comment}
        \begin{Example}
            Разумеется, пустое, одноточечное и конечное множества имеют нулевую меру. Чуть менее тривиально, что счётное --- тоже ($k$-тую точку заключаем в $\varepsilon2^{-k}$-окрестность). Но бывают и несчётные множества нулевой меры. Если кто-то что-то слышат о Канторовом множестве, то это оно, но детально о нём мы поговорим осенью.
        \end{Example}
        \thm \undercolor{darkgreen}{Критерий Лебега}: функция $f\colon[a;b]\to\mathbb R$ интегрируема по Риману на $[a;b]$ тогда и только когда она ограничена, и множество её точек разрыва имеет нулевую меру.
        \begin{Comment}
            <<Множество её точек разрыва имеет нулевую меру>> обычно сокращают до <<непрерывна \textit{почти всюду}>>.
        \end{Comment}
        \begin{Example}
            Так, $\sin\frac1x$ интегрируема на $[0;1]$ (ограничена и имеет одну точку разрыва).
        \end{Example}
        \begin{Comment}
            Докажем это утверждение мы, как и всё с мерой, осенью.
        \end{Comment}
        \thm \undercolor{darkgreen}{Арифметические действия над интегрируемыми функциями}. Пусть $f$ и $g$ интегрируемы на $[a;b]$, $\alpha\in\mathbb R$. Тогда $f+g$, $f-g$, $fg$, $\alpha f$, $|f|$ интегрируемы на $[a;b]$. Если кроме того $\inf\limits_{[a;b]}|g|>0$, то $\frac fg$ также интегрируема на $[a;b]$.
        \begin{Proof}
            Всё доказывается по одной схеме.
            \begin{enumerate}
                \item[$+$] У нас есть неравенство: $|(f+g)(x)-(f+g)(y)|\leqslant|f(x)-f(y)|+|g(x)-g(y)|$. Отсюда $\omega_{[x_k;x_{k+1}]}(f+g)\leqslant\omega_{[x_k;x_{k+1}]}(f)+\omega_{[x_k;x_{k+1}]}(g)$. Это домножим на $\Delta x_k$ и просуммируем.
                \item[$\cdot$] Тут у нас есть такое неравенство: $|(fg)(x)-(fg)(y)|\leqslant|(f(x)-f(y))g(x)|+|f(y)(g(x)-g(y))|$. Поскольку интегрируемые функции $f$ и $g$ ограничены (пусть числами $A$ и $B$). отсюда мы знаем $\omega_{[x_k;x_{k+1}]}(fg)\leqslant A\omega_{[x_k;x_{k+1}]}(f)+B\omega_{[x_k;x_{k+1}]}(g)$.
                \item[$\alpha$] Частный случай предыдущего.
                \item[$-$] Следствие из первого и третьего.
                \item[$|\cdot|$] $\big||f(x)|-|f(y)|\big|\leqslant|f(x)-f(y)|$, отсюда $\omega_{[x_k;x_{k+1}]}(|f|)\leqslant\omega_{[x_k;x_{k+1}]}(f)$.
                \item[$/$] Достаточно доказать, что $\frac1g$ интегрируема. $\left|\frac1{g(x)}-\frac1{g(y)}\right|=\left|\frac{g(y)-g(x)}{g(x)g(y)}\right|\leqslant\frac{|g(y)-g(x)|}{m^2}$, где $m=\inf\limits_{[a;b]}|g|$, откуда $\omega_{[x_k;x_{k+1}]}\left(\frac1g\right)\leqslant\frac1{^2}\omega_{[x_k;x_{k+1}]}(f)$.
            \end{enumerate}
        \end{Proof}
        \begin{Comment}
            Окей, но брать интегралы нас это не учит. А брать хочется. Впрочем, один интеграл мы всё же можем взять.
        \end{Comment}
        \begin{Example}
            $$\int_0^bx^2~\mathrm dx=?$$
            Возьмём конкретное разбиение (ведь функция непрерывна, она точно интегрируема, нам надо лишь число узнать). Разбиение такое: $x_k=\frac{kb}n$. Тогда $\Delta x_k=\frac bn$. В качестве $\xi_k$ возьмём $\frac{(k+1)b}n$. Тогда:
            $$\sum\limits_{k=0}^{n-1}f(\xi_k)\Delta x_k=\sum\limits_{k=0}^{n-1}\frac{(k+1)^2b^2}{n^2}\frac bn=\frac{b^3}{n^3}\sum\limits_{k=1}^{n}k^2=\frac{b^3}{n^3}\frac{n(n-1)(2n-1)}6\underset{n\to\infty}\longrightarrow\frac{b^3}3$$
        \end{Example}
        \begin{Example}
            Теперь посмотрим на $\chi(x)=\begin{cases}
                1 & x\in\mathbb Q\\
                0 & x\notin\mathbb Q
            \end{cases}$. Что мы про неё знаем? Ну, она не интегрируема нигде, у неё на любом отрезке колебание равно 1.
        \end{Example}
        \begin{Example}
            А что с функцией Римана $\psi(x)=\begin{cases}
                0 & x\notin\mathbb Q\\
                \frac1q & x=\frac pq,p\in\mathbb Z,q\in\mathbb N
            \end{cases}$? Докажем, что её интеграл на любом отрезке равен 0. Понятно, что $s_\tau(\psi)=0$. Теперь заметим следующий факт: на любом отрезке для любого натурального $N$ есть конечное количество дробей со знаменателем не больше $N$. Обозначим это количество $c_N$. Тогда $S_\tau(\psi)=\sum\limits_{k=0}^{n-1}M_k\Delta x_k\leqslant2C_N\delta+\frac1N$. берём $\frac1N<\frac\varepsilon2$, после этого выбираем хорошее $\delta$.
        \end{Example}
        \begin{Example}
            Теперь вот на что посмотрим. Пусть $f(x)=\begin{cases}
                1 & x\in(0;1]\\
                0 & x=0
            \end{cases}$. Совершенно очевидно, что $f$ интегрируемо. Также очевидно, что $f\circ\psi=\chi$. То есть мы получили, что композиция интегрируемых функций не интегрируема.
        \end{Example}
        \thm Если $f\in C[A;B]$, а $g\colon[\alpha;\beta]\to[A;B]$ --- интегрируема, то $f\circ g$ интегрируема на $[\alpha;\beta]$.
        \begin{Proof}
            Доказывать это честно мы не будем, а скажем лишь, что из критерия Лебега очевидно. Ограниченность у нас есть (из непрерывности $f$), а композиция непрерывно везде, где непрерывно $g$, а значит множество точек разрыва $f\circ g$ --- подмножество точек разрыва $g$, уж и подавно тот имеет меру 0.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Свойства интеграла}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Тут мы будем часто делать предельный переход, потому что когда мы уже знаем интегрируемость, мы можем рассмотреть последовательность интегральных сумм.
        \end{Comment}
        \begin{Comment}
            Также примем такое соглашение: если $a>b$ и $f\in R[b;a]$, то $\int_a^bf(x)~\mathrm dx$ считается равным $-\int_b^af(x)~\mathrm dx$. также считаем $\int_a^af(x)~\mathrm dx=0$, если $f$ определено в точке $a$.
        \end{Comment}
        \thm \undercolor{darkgreen}{Аддитивность по отрезку}. Пусть $a, b, c\in\mathbb R$ и $f\in R[\min\{a;b;c\};\max\{a;b;c\}]$. Тогда
        $$\int_a^bf=\int_a^cf+\int_c^bf$$
        \begin{Proof}
            Если $a<c<b$, то мы можем взять наше любимое разбиение отрезков, оснастить их нашим любимым способом, содержащим $c$, после чего получить $\sigma=\sigma^{(1)}+\sigma^{(2)}$. Остальные случаи взаимного расположения $a;b;c$ являются следствием из этого.
        \end{Proof}
        \thm Если $K=\mathrm{const}$, то $\int_a^bK=K(b-a)$.
        \begin{Proof}
            Достаточно взять $b>a$; все интегральные суммы равны $K(b-a)$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Линейность интеграла}. $\int_a^bf+g=\int_a^bf+\int_a^bg$ и $\int_a^b\alpha f=\alpha\int_a^bf$.
        \begin{Proof}
            Ну, справедливо, $\sigma(\alpha f;\xi)=\alpha\sigma(f;\xi)$ и $\sigma(f+g;\xi)=\sigma(f;\xi)+\sigma(g;\xi)$.
        \end{Proof}
        \begin{Comment}
            Поскольку тут тоже есть аддитивность, и это не то же самое, что аддитивность по отрезку, аддитивность тут --- аддитивность по функции.
        \end{Comment}
        \thm \undercolor{darkgreen}{Монотонность интеграла}. Если $a\leqslant b$ и $f,g\in R[a;b]$, а также $f\leqslant g$, то $\int_a^bf\leqslant\int_a^bg$.
        \begin{Comment}
            Почему монотонность? Ну, потому что тут мы можем понимать интеграл как функцию одной переменной, и тогда тут мы по сути имеем, что она возрастает.
        \end{Comment}
        \begin{Proof}
            Ну, тоже всё понятно, $\sigma(f;\xi)\leqslant\sigma(g;\xi)$, в чём мы делаем предельный переход.
        \end{Proof}
        \thm Есть полезный частный случай, когда $g=\mathrm{const}$. Тогда мы получим, что если $f\leqslant M$, то $\int_a^bf\leqslant M(b-a)$ или если $f\geqslant m$, то $\int_a^bf\geqslant m(b-a)$. В частности, интеграл неотрицательной функции неотрицателен.
        \thm Пусть $a<b$, $f\in R[a;b]$, $f$ неотрицательна и $f$ положительна хотя бы в одной точке непрерывности. Тогда $\int_a^bf>0$.
        \begin{Proof}
            Пусть та самая точка непрерывности --- $x_0$. Тогда очевидно, что существует окрестность $x_0$, в которой $f(x)\geqslant\frac{f(x_0)}2$. Тогда $\int_a^bf$ можно разбить на эту окрестность и всё остальное. Во <<всём остальном>> $f$ неотрицательна (значит интеграл неотрицателен), а в <<этой окрестности>> интеграл равен положительному числу.
        \end{Proof}
        \begin{Comment}
            Понятно, что без непрерывности $f$ в $x_0$ не получится (функция, равная нулю везде, кроме одной точки, обломает нам жизнь).
        \end{Comment}
        \thm Если $a<b$, $f$ и $g$ интегрируемы на $[a;b]$, $f\leqslant g$ и обе функции непрерывны в $x_0$, то $\int_a^bf<\int_a^bg$.
        \thm Если $a<b$, $f\in R[a;b]$, $f>0$, то $\int_a^bf>0$.
        \begin{Proof}
            Нам нужно доказать, что хоть в одной точке интегрируемая функция непрерывна. Это является следствием признака Лебега или ка-нибудь доказывается руками.
        \end{Proof}
        \thm Если $a\leqslant b$ и $f\in R[a;b]$, то $\left|\int_a^bf\right|\leqslant\int_a^b|f|$.
        \begin{Proof}
            Доказывается либо предельным переходом в $|\sigma(f;\xi)|\leqslant\sigma(|f|;\xi)$, либо рассуждением о том, что $-|f|\leqslant f\leqslant|f|$.
        \end{Proof}
        \thm Если $f\in R[a;b]$, то $\left|\int_a^bf\right|\leqslant\left|\int_a^b|f|\right|$.
        \thm \undercolor{darkgreen}{Первая (обобщённая) теорема о среднем интегрального счисления}. Пусть $f,g\in R[a;b]$, $m\leqslant f\leqslant M$, а $g$ не меняет знак. Тогда $\exists\mu\in[m;M]~\int_a^bfg=\mu\int_a^bg$.
        \begin{Proof}
            Известно, что $m\int_a^bg\leqslant \int_a^bfg\leqslant M\int_a^bg$. Когда $\int_a^bg=0$, подходит любое $\mu$. А иначе на него можно поделить и получить $\frac{\int_a^bfg}{\int_a^bg}=\mu$.
        \end{Proof}
        \thm Если $f\in C[a;b]$, $g\in R[a;b]$, $g$ не меняет знак, то $\exists c\in[a;b]~\int_a^bfg=f(c)\int_a^bg$.
        \begin{Proof}
            Пусть $m=\min\limits_{[a;b]}f$, $M=\max\limits_{[a;b]}f$ (такие есть по непрерывности $f$). Тогда возьмём в предыдущей теореме $\mu=f(c)$.
        \end{Proof}
        \thm Если $f\in C[a;b]$, $m\leqslant f\leqslant M$ то $\exists \mu\in[m;M]~\int_a^bf=\mu(b-a)$.
        \thm Если $f\in C[a;b]$, то $\exists c\in[a;b]~\int_a^bf=(b-a)f(c)$.
        \begin{Comment}
            Величину $\frac1{b-a}\int_a^bf$ называют интегральным средним арифметическим $f$. Это потому, что $\sum\limits_{k=0}^{n-1}f(x_k)\frac{b-a}n=\frac{b-a}n\sum\limits_{k=0}^{n-1}f(x_k)\underset{n\to\infty}\longrightarrow\frac{b-a}{b-a}\int_a^bf$.
        \end{Comment}
        \dfn Пусть $E$ --- промежуток (в $\mathbb R$). Тогда $f$ \undercolor{red}{локально интегрируемо} на $E$ ($f\in R_{\mathrm{loc}}(E)$), если $f$ интегрируема, на любом подотрезке $E$.
        \dfn Пусть $f\in R_{\mathrm{loc}}(E)$, $a\in E$. Функция $\Phi(x)=\int_a^xf$, где $x\in E$ называется \undercolor{red}{интегралом с переменным верхним пределом}.
        \thm \undercolor{darkgreen}{Об интеграле с переменным верхним пределом}. В условиях определения 1 $\Phi\in C(E)$ и, если $f$ непрерывно в $x\in E$, то $\Phi$ дифференцируема в $x$ и $\Phi'(x)=f(x)$. Вторая часть также называется \undercolor{darkgreen}{теоремой Барроу}.
        \begin{Proof}
            Зафиксируем $x\in E$ и докажем непрерывность $\Phi$ в точке $x$. То есть $\Phi(x+h)-\Phi(x)\underset{h\to0}\longrightarrow0$ (при $h\neq0\land x+h\in E$). Разность равна $\int_x^{x+h}f$. Поставим модуль в разности, и в интеграле тоже поставим. Понятно, что длина промежутка равна $h$, а подынтегральную функцию хочется как-то ограничить. Впрочем, понятно, что $f$ ограничена на каждом отрезке $E$. То есть если мы возьмём $\delta>0:|h|<\delta$, то на отрезке $[x-\delta;x+\delta]\cap E$ (при условии, что это отрезок (надо так выбрать $\delta$)) $|f|<M$. Тогда, понятно, модуль нашего интеграла меньше $M|h|\longrightarrow0$.\\
            Теперь теорема Барроу. Докажем, что $\frac{\Phi(x+h)-\Phi(x)}h-f(x)\underset{h\to0}\longrightarrow0$. Обозначим левую часть за $R$, чтобы не переписывать. Тогда $|R|=\left|\frac1h\int_x^{x+h}f-f(x)\right|=\left|\frac1h\int_x^{x+h}f(t)-f(x)~\mathrm dt\right|\leqslant\left|\frac1h\int_x^{x+h}|f(t)-f(x)|~\mathrm dt\right|$. Тогда мы знаем, что $\forall\varepsilon>0~\exists\delta>0:|x-t|<\delta~|f(t)-f(x)|<\varepsilon$ Из этого $\int_x^{x+h}|f(t)-f(x)|~\mathrm dt\leqslant\varepsilon h$, а значит $|R|<\varepsilon$. Что нам в общем-то и хотелось.
        \end{Proof}
        \thm Всякая непрерывная на промежутке функция имеет первообразную (собственно, $\Phi$).
        \thm $\Phi$ локально лившицевая на $E$ (то есть функция не больше, чем константа на приращение аргумента). Только константа на каждом отрезке своя, поэтому и <<локально>>.
        \thm Можно вместо $\Phi$ взять $\Psi(x)=\int_x^bf$, и получилось бы, что $\Psi'(x)=-f(x)$.
        \thm \undercolor{darkgreen}{Формула Ньютона-Лейбница}. Пусть $f\in R[a;b]$, а $F$ --- первообразная $f$ на $[a;b]$. Тогда $\int_a^bf=F(b)-F(a)$.
        \begin{Proof}
            Посмотрим на $F(b)-F(a)$. Возьмём произвольное дробление $[a;b]$ (например, на $n$ равных частей): $x_k=a+\frac{k(b-a)}n$. Тогда $F(b)-F(a)=\sum\limits_{k=0}^{n-1}F(x_{k+1})-F(x_k)$. Дальше применим формулу Лагранжа ($\exists\xi_k\in[x_k;x_{k+1}]$): $\sum\limits_{k=0}^{n-1}F'(\xi_k)\Delta x_k=\sum\limits_{k=0}^{n-1}f(\xi_k)\Delta x_k$. Да это же наш старый знакомый, интегральная сумма для $f$! А значит при $n\to\infty$ $F(b)-F(a)$ стремится к $\int_a^bf$. Но $F(b)-F(a)$ --- стационарная последовательность, значит её предел равен $F(b)-F(a)$. Всё.
        \end{Proof}
        \dfn \undercolor{red}{Двойная подстановка} $F$ --- это $F(b)-F(a)$. Обозначается так: $F\big|_a^b$, так: $F(x)\big|_{x=a}^b$ или так: $[F(x)]_{x=a}^b$.
        \thm Видно, что двойная подстановка не зависит от выбора первообразной.
        \begin{Comment}
            Пусть мы хотим найти $\int_{-1}^1\frac{\mathrm dx}{x^2}=\left[-\frac1x\right]_{-1}^1$, так? Нет, ни разу не так, получится $-2$, хотя функция положительна. Проблема в том, что $\frac1{x^2}$ не интегрируема на $[-1;1]$, потому что не ограничена. А к тому же у неё нет первообразной на всём отрезке, есть на отрезке без точки. Впрочем, второе можно ослабить.
        \end{Comment}
        \thm Пусть $f\in R[a;b]$, а $F$ --- первообразная $f$ на $[a;b]$ без конечного количества точек, и $F$ непрерывна. Тогда $\int_a^bf=F(b)-F(a)$.
        \begin{Proof}
            Обозначим исключительные точки за $\alpha_1;\alpha_2;\ldots;\alpha_{m-1}$. не умаляя общности, $\alpha_1<\alpha_2<\cdots<\alpha_{m-1}$. Ещё положим $\alpha_0=a$, $\alpha_m=b$. Тогда $\int_a^bf=\sum\limits_{k=0}^{m-1}\int_{\alpha_k}^{\alpha_{k+1}}f=\sum\limits_{k=0}^{m-1}\lim\limits_{\varepsilon\to0+}\int_{\alpha_k+\varepsilon}^{\alpha_{k+1}-\varepsilon}f\overset{(*)}=\sum\limits_{k=0}^{m-1}\lim\limits_{\varepsilon\to0+}F\big|_{\alpha_k+\varepsilon}^{\alpha_{k+1}-\varepsilon}\overset{(**)}=\sum\limits_{k=0}^{m-1}F\big|_{\alpha_k}^{\alpha_{k+1}}=F\big|_{a}^{b}$.\\
            $(*)$ --- непрерывность интеграла с переменным верхним/нижним пределом.\\
            $(**)$ --- непрерывность $F$.
        \end{Proof}
        \begin{Example}
            Когда нам это поможет? Например, $|x|'=\sign x,\quad x\neq0$, значит $\int_{-1}^2\sign x~\mathrm dx=|x|\big|_{-1}^2=1$. И это правда.
        \end{Example}
        \begin{Comment}
            Понятно, что без непрерывности $F$ получится не хорошо. $\sign'x=0,\quad x\neq0$. Тогда $\int_{-1}^20\neq\sign x\big|_{-1}^2=2$. А это бред.
        \end{Comment}
        \thm Пусть $F$ дифференцируема на $[a;b]$, а $F'\in R[a;b]$, тогда $\int_a^b F'=F(b)-F(a)$.
        \begin{Comment}
            Опустить условие $F'\in R[a;b]$ нельзя. Производная может не быть интегрируемой. $F(x)=\begin{cases}
                x^2\sin\frac1{x^2} & x\neq 0\\
                0 & x=0
            \end{cases}$. Тогда $F'(0)=0$ (из предела разностного отношения), а при $x\neq 0$ $F'(x)=2x\sin\frac1{x^2}-x^2\cos\frac1{x^2}\cdot\left(\frac{-2}{x^3}\right)$. Эта штука не ограничена ни в какой окрестности нуля, а значит по Риману её сложно интегрировать, например, на $[0;1]$.
        \end{Comment}
        \begin{Comment}
            Мы видим некоторый недостаток интеграла Римана. У нас была теорема Ньютона-Лейбница, которая связывает определённый и неопределённый интегралы. И вот эта связь не полная. Функция имеет производную, но восстановить её при помощи интегрирования по Риману не получается. На непрерывных функциях интеграл Римана работает замечательно, а на разрывных нужен другой инструмент.\\
            Кстати, для непрерывных функций формула Ньютона-Лейбница является тривиальным следствием теоремы Барроу. Как именно? $\Phi=\int_a^xf$. По Барроу это первообразная. А для этой первообразной формула Ньютона-Лейбница тривиальна. А тогда она верна и для любой другой первообразной.
        \end{Comment}
        \thm Условие интегрируемости и наличие у неё первообразной не влекут друг друга.
        \begin{Proof}
            Пример в одну сторону мы привели только что, а в другую --- чуть раньше: ступенька (произвольная не может иметь разрывов 1 рода).
        \end{Proof}
        \begin{Comment}
            Полностью эту задачу решили в 1910-х годах Данжуа и Перрон, которые построили такие интегралы, которые решали задачу о восстановлении функции по производной. И довольно быстро было доказано, что их интегралы (построенные по-разному) дают один и тот же класс интегрируемых функций, и их интегралы также совпадают. Их мы рассматривать не будем. Потом появился интеграл Хенстока, который наследовал интеграл Римана. У нас был предел интегральных сумм (берём оснащённое дробление...) И вот функция Дирихле оказывается не интегрируемой, в неё есть рациональные и иррациональные точки. Вторых больше, а значит почти везде --- 0. Если бы разумный интеграл существовал, он был бы 0. И в интеграле Хенстока по $\varepsilon$ ищут не число, а функцию. И идейно мы разрешаем точкам быть неравноправными, рациональным точкам давая маленькие отрезки по сравнению с иррациональными. Потом выяснилось, что этот интеграл также совпадает с интегралами Данжуа и Перрона.\\
            А у нас будут интегралы Лебега и теорией меры. Помимо увеличения класса интегрируемых функций, мы перенесём всё на многомерный случай, что с интегралами Римана делать можно, но затруднительно.\\
            Пока мы рассматриваем непрерывные функции, нам ничего этого не надо. А для них есть ещё два подхода, как смотреть на интегралы. Первый --- принять формулу Ньютона-Лейбница за определение (что иногда называют интегралом Ньютона-Лейбница). Тут будет только одна проблема --- существование первообразной (мы доказывали через интеграл Римана), а всё остальное следует вообще легко. Второй --- аксиоматический подход. Детали отличаются в зависимости от того, какой класс функций мы интегрируем, а в случае непрерывных функций мы получаем три аксиомы.\\ Пусть $I$ --- функция, которая паре из отрезка и непрерывной на нём функции сопоставляет вещественное число. Для краткости $I([a;b];f)$ обозначается как $\int_a^bf$. Тогда $I$ --- интеграл, если
            \begin{enumerate}
                \item Выполнена аддитивность по отрезку: $\forall a<b<c~\int_a^bf+\int_b^cf=\int_a^cf$.
                \item Выполнена монотонность: $f\leqslant g\Rightarrow\int_a^bf\leqslant\int_a^bg$.
                \item Нормировка: $\forall K\in\mathbb R~\int_a^bK=K(b-a)$.
            \end{enumerate}
            Легко доказать, что такое отображение $I$ если существует, то только одно. И нетрудно доказать для неё теорему Барроу. Но тут существует проблема с существованием $I$ в принципе (может, аксиомы внутренне противоречивы). И да, существует, например, барабанная дробь, интеграл Римана, для которого мы доказали выполнение аксиом.
        \end{Comment}
        \thm \undercolor{darkgreen}{Интегрирование по частям в неопределённом интеграле}. Пусть $f$ и $g$ дифференцируемы на $[a;b]$, а из производные интегрируемы по Риману. Тогда $\int_a^bfg'=fg\big|_a^b-\int_a^bf'g$.
        \begin{Proof}
            Несложно заметить, что всё тут написанное интегрируемо. А потом возьмём $g'f=(fg)'-f'g$ и возьмём интеграл обеих частей от $a$ до $b$.
        \end{Proof}
        \begin{Comment}
            У нас обычно условие на функции будет уже, кусочная непрерывность, но вообще можно пожадничать и сильнее, сказав, что достаточно существования одного из интегралов.
        \end{Comment}
        \thm \undercolor{darkgreen}{Замена переменной в определённом интеграле}. Пусть $f\in C[A;B]$, $\varphi\colon[\alpha;\beta]\to[A;B]$ --- дифференцируема, а $\varphi'\in R[\alpha;\beta]$. Тогда $\int_\alpha^\beta(f\circ\varphi)\varphi'=\int_{\varphi(a)}^{\varphi(b)}f$.
        \begin{Proof}
            $f$ непрерывна, значит имеет первообразную ($F$). Тогда мы уже знаем первообразную $(f\circ\varphi)\varphi$ на $[\alpha;\beta]$, она равна $F\circ\varphi$. Остаётся применить функцию Ньютона-Лейбница:\\
            $
            \int_\alpha^\beta(f\circ\varphi)\varphi'=F\circ\varphi\big|_\alpha^\beta=F(\varphi(\beta))-F(\varphi(\alpha))=F\big|_{\varphi(a)}^{\varphi(b)}=\int_{\varphi(a)}^{\varphi(b)}f
            $
        \end{Proof}
        \begin{Comment}
            Заметим, что тут, в отличие от неопределённого интеграла, не обязательно возвращаться к изначальной переменной, потому что нам нужно найти число. Но тут нужно не забыть поменять пределы интегрирования.
        \end{Comment}
        \begin{comment}
            $\int_a^bf~\mathrm dg=fg\big|_a^b-\int_a^bg~\mathrm df$.
        \end{comment}
        \begin{Comment}
            При этих условиях значения функции $\varphi$ могут выходить за пределы $[\varphi(a);\varphi(b)]$, главное, чтобы нем выходили за $[A;B]$.
        \end{Comment}
        \thm От $\varphi$ можно потребовать побольше, а от $f$ --- поменьше. Пусть $\varphi$ не только дифференцируема на $[\alpha;\beta]$, но ещё и монотонна. А $f$ пусть интегрируема по Риману на $[\varphi(\alpha);\varphi(\beta)]$ (понятно, границы могут и наоборот стоять). Тогда всё то же самое.
        \begin{Proof}
            Это мы доказывать не будем, это сложно, но желающие могут почитать Фихтенгольца.
        \end{Proof}
        \begin{Example}
            \[
            \begin{split}
                \int_0^1\sqrt{1-x^2}~\mathrm dx\overset{x=\cos t}=\int_{\pi/2}^0-\sin t\sqrt{1-\cos^2t}~\mathrm dt=\int_0^{\pi/2}\sin^2t~\mathrm dt=\\
                \int_0^{\pi/2}\frac{1-\cos2t}2~\mathrm dt=\frac12\left(\frac\pi2+\left[\frac{\sin2t}2\right]_0^{\frac\pi2}\right)=\frac\pi4
            \end{split}
            \]
        \end{Example}
        \begin{Example}
            Если $f\in R[-a;a]$ чётно, то $\int_{-a}^af=2\int_0^af$, а если нечётна, то 0. Это элементарно доказывается заменой $x=-t$ в $\int_{-a}^0f$.
        \end{Example}
        \thm \undercolor{darkgreen}{Вторая теорема о среднем} (\undercolor{darkgreen}{теорема Бонне}). Пусть $f\in C[a;b]$, $g\in C^{(1)}[a;b]$, $g$ монотонна. Тогда $\exists c\in[a;b]~\int_a^bfg=g(a)\int_a^cf+g(b)\int_a^bf$.
        \begin{Proof}
            Пусть $F(x)=\int_a^xf$. $\int_a^bfg=\int_a^bgF'=gF\big|_a^b-\int_a^bg'F=g(b)\int_a^bf-\int_a^bg'F$. Теперь применим первую теорему о среднем. Там нужна функция, не меняющая знак --- это $g'$. Так вот, по первой теореме о среднем $\exists c\in[a;b]~\int_a^bg'F=F(c)\int_a^bg'$. То есть $\int_a^bfg=g(b)\int_a^bf-F(c)\int_a^bg'=g(b)\int_a^bf-(g(b)-g(a))\int_a^cf=g(a)\int_a^cf+g(b)\int_a^bf$
        \end{Proof}
        \begin{Comment}
            Вообще говоря, в данной теореме можно наложить более слабые условия, но тогда доказательство усложнится. Но нам эта теорема нахер не нужна (и вообще редко используется), так что нам она скорее для общего развития. Так что мы просто скажем, как конкретно можно ослабить условие: достаточно потребовать, чтобы $f$ была интегрируемы по Риману, а $g$ --- монотонна.
        \end{Comment}
        \begin{Comment}
            Иногда эту формулу можно упростить, оставив в правой части одно слагаемое из двух. Упростить можно, если $g\downarrow$ и $g\geqslant0$. Тогда можно переопределить $g(b)$ нулём: убывание сохранится, интеграл слева не поменяется, а одно из слагаемых правой части испарится. Аналогично если $g\uparrow$ и неотрицательно, её можно переопределить в точке $a$.\\
            Хрен его знает, правда, стало лучше или хуже.
        \end{Comment}
        \begin{Example}
            Рассмотрим $I=\int_{100\pi}^{200\pi}\frac{\sin x}x~\mathrm dx$ (то есть колеблющаяся функция на длинном промежутке). Интеграл не берущийся, его не посчитать по формуле Ньютона-Лейбница. Но хочется примерно посчитать, что это. Что нам даёт первая теорема о среднем? $|I|\leqslant \int_{100\pi}^{200\pi}\frac{\mathrm dx}{x}=\ln2$. Но ясно, что оценка очень грубая, хочется как-то учесть взаимное уничтожение положительных и отрицательных частей функции, поэтому применим вторую теорему о среднем с $f(x)=\sin x$, $g(x)=\frac1x$. Она даст нам, что $I=\frac1{100\pi}\int_{100\pi}^c\sin x~\mathrm dx+\frac1{200\pi}\int_c^{200\pi}\sin x~\mathrm dx$. Тут уже интегралы берутся, поэтому $I=\frac{1-\cos c}{100\pi}-\frac{\cos c-1}{200\pi}=\frac{\cos c-1}{200\pi}$. И отсюда ясно видно следующее: $0\leqslant I\leqslant\frac1{100\pi}$.\\
            Но вообще можно руками проинтегрировать по частям: $I=\int_{100\pi}^{200\pi}\frac{\mathrm d(1-\cos x)}x=\int_{100\pi}^{200\pi}\frac{1-\cos x}{x^2}~\mathrm dx\overset{\theta\in(0;2)}=\theta\int_{100\pi}^{200\pi}\frac{\mathrm dx}{x^2}=\frac\theta{200\pi}\in\left(0;\frac1{100\pi}\right)$.\\
            Именно поэтому вторую теорему о средней считают не очень полезной (за редким исключением, когда она верна в ослабленных условиях).
        \end{Example}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Формулы Тейлора и Валлиса}. \undercolorblack{orange}{Интегральные неравенства}.}
    \begin{itemize}
        \thm \undercolor{darkgreen}{Формула Тейлора с интегральным остатком} (a.k.a. \undercolor{darkgreen}{формула Якоби}). пусть $n\in\mathbb Z_+$, $f\in C^{(n+1)}\ab$, $x_0;x\in\ab$. Тогда $f(x)=\sum\limits_{k=0}^n\frac{f^{(n)}(x_0)}{k!}(x-x_0)^k+\frac1{n!}\int_{x_0}^xf^{(n+1)}(t)(x-t)^n~\mathrm dt$.
        \begin{Comment}
            В параграфе про Тейлора говорилось, что из всех формул записи остатка важны три: Пеано, Лагранжа и вот эта.
        \end{Comment}
        \begin{Proof}
            Докажем по индукции. База ($n=0$). $f(x)=f(x_0)+\int_{x_0}^xf'(t)~\mathrm dt$. Видим формулу Ньютона-Лейбница.\\
            Переход ($n-1\to n$). Преобразуем остаток:
            \[
            \begin{split}
                \frac1{(n-1)!}\int_{x_0}^xf^{(n)}(t)(x-t)^{n-1}~\mathrm dt=-\int_{x_0}^xf^{(n)}(t)~\mathrm d\frac{(x-t)^n}{n!}=\\
                -f^{n}(t)\frac{(x-t)^n}{n!}\bigg|_{t=x_0}^x+\frac1{n!}\int_{x_0}^xf^{(n+1)}(t)(x-t)^n~\mathrm dt
            \end{split}
            \]
            Это ли не то, что нам надо.
        \end{Proof}
        \begin{Comment}
            Форма Лагранжа из этой формы следует в силу первой теоремы о среднем. Ну, действительно, $\exists c\in[x_0;x]~\frac1{n!}\int_{x_0}^xf^{(n+1)}(t)(x-t)^n~\mathrm dt=\frac{f^{(n+1)}(c)}{n!}\int_{x_0}^x(x-t)^n~\mathrm dt=\frac{f^{(n+1)}(c)}{(n+1)!}(x-x_0)^{n+1}$. За тем лишь исключением следует, что тут чуть более жёсткие условия на функцию. Но интегральная форма чем удобна --- что там нет неизвестной точки
        \end{Comment}
        \thm Пусть $n\in\mathbb Z_+$. Пусть $J_m=\int_0^{\pi/2}\sin^mx~\mathrm dx$. Очевидно, $J_0=\frac\pi2$, $J_1=1$. Также мы уже посчитали $J_2=\frac\pi4$. Хочется узнать рекуррентную формулу $J_m$.
        \begin{Proof}
            \[
            \begin{split}
                J_m=\int_0^{\pi/2}\sin^{m-1}x~\mathrm d(-\cos x)=\\
                -\sin^{m-1}x\cos x\big|_0^{\pi/2}+\int_0^{\pi/2}(m-1)\sin^{m-2}x\cos^2x~\mathrm dx=\\
                0+(m-1)\int_0^{\pi/2}\sin^{m-2}x(1-\sin^2x)~\mathrm dx=(m-1)(J_{m-2}-J_m)
            \end{split}
            \]
            Это рекуррентная формула: $J_m=\frac{m-1}mJ_{m-2}$.
        \end{Proof}
        \dfn $m!!$ (\undercolor{red}{двойной факториал}) --- $\prod\limits_{\substack{k=1\\k\equiv m(\mathrm{mod} 2)}}^mk$, $0!!=(-1)!!=1$.
        \thm $J_m=\frac{(m-1)!!}{m!!}\begin{cases}
            1 & m\mathop{\not\divby}2\\
            \frac\pi2 & m\divby2
        \end{cases}$.
        \thm \undercolor{darkgreen}{Формула Валлиса} $\pi=\lim\limits_{n\to\infty}\frac1n\left(\frac{(2n)!!}{(2n-1)!!}\right)^2$.
        \begin{Proof}
            Известно, что $\forall x\in(0;\frac\pi2)~\sin^{2n+1}x<\sin^{2n}x<\sin^{2n-1}x$.
            Проинтегрируем это от 0 до $\frac\pi2$. Получим $J_{2n-1}<J_{2n}<J_{2n+1}$ или $\frac{(2n)!!}{(2n+1)!!}<\frac{(2n-1)!!}{(2n)!!}\frac\pi2<\frac{(2n-2)!!}{(2n-1)!!}$. Пусть $x_n=\frac1n\left(\frac{(2n)!!}{(2n-1)!!}\right)^2$. Тогда из левого неравенства $x_n<\frac{2n+1}{2n}\pi$, а из правого $x_n>\pi$. Ну, и всё, обе части стремятся к $\pi$, по лемме о двух милиционерах $x_n$ также стремится к $\pi$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Формула Стирлинга для факториала}. $n!\sim\sqrt{2\pi n}\left(\frac ne\right)^n$.
        \begin{Proof}
            Это мы докажем, но не сейчас.
        \end{Proof}
        \thm \undercolor{darkgreen}{Интегральное неравенство Йенсена}. Пусть $a<b$, $f\in C\ab[A;B]$ --- выпукла вниз, $\varphi\colon[a;b]\to\ab[A;B]$ --- также непрерывна. Пусть $\lambda\in C([a;b]\to[0;+\infty))$ и $\int_a^b\lambda=1$. Тогда $f\left(\int_a^b\lambda\varphi\right)\leqslant\int_a^b\lambda\cdot(f\circ\varphi)$.
        \begin{Proof}
            Повторим рассуждения из обычного неравенства Йенсена.\\
            Пусть $E=\{x\in[a;b]\mid\lambda(x)>0\}$. Пусть $M=\sup\limits_E\varphi$, $m=\inf\limits_E\varphi$. Поскольку $\varphi$ непрерывна на $[a;b]$, она ограничена, а значит, $m$ и $M$ конечны. Также пусть $c=\int_a^b\lambda\varphi$. Во-первых, заметим, что $c\in\ab[A;B]$.\\
            Если $m=M$, то $\varphi$ --- константа на $E$, а значит $c=m=M$. Неравенство Йенсена обращается в равенство (и слева, и справа $f(m)$).\\
            Когда же $m<M$, $m<c<M$, тем самым $A<c<B$. Тогда в точке $c$ у функции $f$ существует опорная прямая, задаваемая уравнением $l(x)=\alpha x+\beta$. В таком случае $l(c)=\alpha c+\beta=\int_a^b\lambda\cdot(\alpha\varphi+\beta)\leqslant\int_a^b\lambda\circ\varphi$. Последнее равенство из монотонности интеграла и определения опорной прямой.
        \end{Proof}
        \begin{Comment}
            Если $\varphi\neq\mathrm{const}$ на $E$, а $f$ строго выпукла, то неравенство Йенсена строгое.
        \end{Comment}
        \begin{Comment}
            Для выпуклой вверх функции неравенство выполняется в противоположную сторону.
        \end{Comment}
        \thm \undercolor{darkgreen}{Интегральное неравенство Гёльдера}. Пусть $a<b$, $f;g\in R[a;b]$, $\frac1p+\frac1q=1$ --- сопряжённые показатели. Тогда $\left|\int_a^bfg\right|\leqslant\left(\int_a^b|f|^p\right)^{1/p}\left(\int_a^b|g|^q\right)^{1/q}$.
        \begin{Proof}
            Почему указанные функции интегрируемы? Ну, из критерия Лебега мы выводили свойства композиции, а если не хочется им пользоваться, то будем считать $f$ и $g$ непрерывными.\\
            Поскольку $\left|\int_a^bfg\right|\leqslant\int_a^b|fg|$, можно считать $f$ и $g$ неотрицательными.\\
            Как выглядит неравенство Юнга? $\forall u;v\geqslant0~uv\leqslant\frac{u^p}p+\frac{v^q}q$. Его мы доказывали 100 лет назад.
            Пусть $A=\int_a^bf^p$, $B=\int_a^bg^q$. Тогда $\int_a^bfg=\int_a^b\frac{f(t)}{A^{1/p}}\frac{g(t)}{B^{1/q}}~\mathrm dt\cdot A^{1/p}B^{1/q}\leqslant\int_a^b\left(\frac{f^p}{AP}+\frac{g^p}{BP}\right)\cdot A^{1/p}B^{1/q}=A^{1/p}B^{1/q}$.
        \end{Proof}
        \begin{Comment}
            Неравенство Гёльдера обращается в равенство, когда все произведения $fg$ нестрого одного знака, а во-вторых, $f^p$ и $g^q$ пропорциональны.
        \end{Comment}
        \thm К---Б---Ш для интегралов. Если $a<b$, $f,g\in R[a;b]$, то $\left|\int_a^bfg\right|\leqslant\sqrt{\int_a^bf^2}\sqrt{\int_a^bg^2}$.
        \begin{Proof}
            Неравенство Гёльдера, $p=q=2$.
        \end{Proof}
        \thm $\dotprod fg=\int_a^bfg$ --- скалярное произведение.
        \begin{Proof}
            Независимо от того, на каком пространстве мы рассматриваем это скалярное произведение, у нас всё уже есть для проверки аксиом.
        \end{Proof}
        \begin{Comment}
            А если мы решим определить интеграл функции $[a;b]\to\mathbb C$, то ничего не изменится, кроме $\dotprod fg=\int_a^b f\overline g$.
        \end{Comment}
        \thm \undercolor{darkgreen}{Интегральное неравенство Минковского}. Пусть $a<b$, $f,g\in R[a;b]$, $p\in[1;+\infty)$. Тогда $\left(\int_a^b|f+g|^p\right)^{1/p}\leqslant\left(\int_a^b|f|^p\right)^{1/p}\left(\int_a^b|g|^p\right)^{1/p}$.
        \begin{Proof}
            Возьмём дробление (например, равномерное), оснастим его левыми концами. Получим
            \[
            \begin{split}
                \left(\sum\limits_{k=0}^{n-1}|(f+g)(x_k)|^p\Delta x_k\right)^{1/p}=\left(\sum\limits_{k=0}^{n-1}|\underbrace{f(x_k)(\Delta x_k)^{1/p}}_{a_k}+\underbrace{g(x_k)(\Delta x_k)^{1/p}}_{b_k}|^p\right)^{1/p}\leqslant\\
                \left(\sum\limits_{k=0}^{n-1}|a_k|^p\right)^{1/p}+\left(\sum\limits_{k=0}^{n-1}|b_k|^p\right)^{1/p}=\left(\sum\limits_{k=0}^{n-1}|f(x_k)|^p\Delta x_k\right)^{1/p}+\left(\sum\limits_{k=0}^{n-1}|g(x_k)|^p\Delta x_k\right)^{1/p}
            \end{split}
            \]. (Здесь было применено сумматорное неравенство Минковского.) Тогда левая часть стремится к левому интегралу, а правая --- к сумме интегралов в правой части.
        \end{Proof}
        \thm $a<b$, $f;g\in R[a;b]$. Тогда $\sqrt{\int_a^b|f+g|^2}\leqslant\sqrt{\int_a^b|f|^2}+\sqrt{\int_a^b|g|^2}$.
        \thm При $p\geqslant1$ $\left(\int_a^b|f|^p\right)^{1/p}$ --- норма в $R[a;b]$ и $C[a;b]$.
        \begin{Comment}
            Случай $p=2$ самый важный, потому что только эта норма порождается скалярным произведением (а именно скалярным произведением $\dotprod fg=\int_a^bfg$). И пространство с такой нормой является одним иж важнейших. Точнее, не совсем оно, а ещё то, что из него получается.\\
            Дело тут вот в чём. Есть такое свойство как полнота пространства (фундаментальная последовательность сходится). Это же применимо к метрическим и в частности нормированным пространствам. К сожалению, функциональные пространства полными не получаются, что создаёт некоторые помехи. С другой же стороны, есть теорема о пополнении теории метрических пространств, которая говорит, что любое метрическое пространство можно вложить в полное метрическое пространство, так что оно будет плотным подпространством (а в нормированном случае --- вложить линейным способом). При этом пополнение единственно с точностью до изометрии. И вот это пополнение функционального пространства содержит те же достоинства, но лишается неполноты.\\
            А вообще к этому есть и абстрактный подход: есть пространство со скалярным произведением, им порождается норма, и если полученное пространство полно, то нормированное пространство называется гильбертовым. Весной в 4 семестре мы будем таковые изучать, и тогда расширим пространство интегрируемых функций до интегрируемых по Лебегу, что уже даст нам полноту.
        \end{Comment}
        \begin{Comment}
            Хочется неравенство о средних (арифметическом и геометрическом). Арифметическое мы уже имеем: $\frac1{b-a}\int_a^bf$, а как определить геометрическое? Да легко, $\sqrt[n]{x_1\cdots x_n}=e^{\frac1n(x_1+\cdots+x_n)}$.
        \end{Comment}
        \thm \undercolor{darkgreen}{Неравенство для интегральных средних}. Пусть $f\in R[a;b]$, $f>0$. Тогда $e^{\frac1{b-a}\int_a^bf}\leqslant\frac1{b-a}\int_a^bf$. Если $f$ не константа, неравенство строгое.
        \begin{Proof}
            Как и всё остальное доказывается либо через неравенство Йенсена для логарифма, либо как предельный переход сумматорного неравенства для средних.
        \end{Proof}
        \thm \undercolor{darkgreen}{Интегральное неравенство Чебышёва}. Пусть $f\uparrow[a;b]$, $g\downarrow[a;b]$. Тогда $\frac1{b-a}\int_a^bfg\leqslant\frac1{b-a}\int_a^bf\cdot\frac1{b-a}\int_a^bg$.
        \begin{Proof}
            Пусть $A=\frac1{b-a}\int_a^bf$. Пусть $c=\sup\{x\in[a;b]\mid f(x)\leqslant A\}$. Рассмотрим $\int_a^bg(f-A)$. Убедимся, что нужно доказать неположительность этого. Тогда $\int_a^bg(f-A)=\int_a^c\underbrace{g}_{\geqslant g(c)}\underbrace{(f-A)}_{\leqslant0}+\int_c^b\underbrace{g}_{\leqslant g(c)}\underbrace{(f-A)}_{\geqslant0}\leqslant g(c)\int_a^bf-A=0$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Сумматорное неравенство Чебышёва}. Пусть $a;b\in\mathbb R^n$, $a_1\leqslant\cdots\leqslant a_n$, $b_1\geqslant\cdots\geqslant b_n$. Тогда $\frac1n\sum\limits_{k=1}^na_kb_k\leqslant\frac1n\sum\limits_{k=1}^na_k\cdots\frac1n\sum\limits_{k=1}^nb_k$.
        \begin{Proof}
            Положим $f(x)=a_k$, $g(x)=b_k$ при $x\in[\frac{k-1}n;\frac kn)$. Интегральное неравенство Чебышёва для этих функций совпадает с тем, что мы хотим получить.
        \end{Proof}
        \begin{Comment}
            Для одноимённо монотонных функций/последовательностей неравенства Чебышёва противоположны.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Несобственные интегралы}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Геометрический смысл интеграла --- площадь её подграфика, а что если функция задана на луче и мы хотим посчитать площадь такой неограниченной фигуры. Как мы поступим? Можем посчитать функцию на $\int_a^x$ и устремить $x$ к бесконечности. Или сама функция может быть не ограничена, тогда берём точку, в которой она стремится к $\infty$, отступаем её на $\varepsilon$, который устремляем к нулю.
        \end{Comment}
        \dfn Пусть $-\infty<a<b\leqslant+\infty$, $f\in R_{\mathrm{loc}}[a;b)$. Тогда $\int_a^Af$, где $A\in[a;b)$ --- \undercolor{red}{частичные интегралы}, а символ $\int_a^bf$ называют \undercolor{red}{несобственным интегралом}.\\
        Если $\exists\lim\limits_{A\to b-}\int_a^Af=I\in\mathbb R$, то говорят, что \undercolor{red}{несобственный интеграл сходится} и символу приписывают значение $I$. В противном случае говорят, что \undercolor{red}{несобственный интеграл расходится} (если предел есть в $\overline{\mathbb R}$, то символу всё равно приписывают значение предела).
        \begin{Comment}
            Также несобственный интеграл могут обозначать как $\int_a^{\to b}f$, чтобы было понятно, на каком конце переходить к пределу.
        \end{Comment}
        \begin{Comment}
            Аналогично определяется несобственный интеграл в симметричной ситуации. Но непонятно, что если хочется взять несобственный интеграл с обеих сторон. Тогда можно положить $\int_{\to a}^{\to b}$ равным $\int_{\to a}^{c}+\int_{c}^{\to b}$ (и доказать, что определение не зависит от $c$), а можно взять двойной предел.
        \end{Comment}
        \begin{Comment}
            Давайте посмотрим, появляется ли тут что-нибудь новое по сравнению с интегралом Римана (который, как мы помним, также является пределом).
        \end{Comment}
        \thm Если у нас $b$ конечно и $f\in R[a;b]$, то несобственный интеграл совпадает с собственным: $\int_a^bf=\int_a^{\to b}f$.
        \begin{Proof}
            $\int_a^A f$ --- непрерывная функция переменной $A$.
        \end{Proof}
        \begin{Comment}
            А значит новизна появляется в двух случаях. Либо $b=+\infty$, либо $b<+\infty$, но $f\notin R[a;b]$. На второй случай посмотрим подробнее. Почему так может быть (то есть на каждом $[a;A]$, где $A<b$, $f$ интегрируема, а на $[a;b]$ --- нет). Легко заметить, что это происходит, если $f$ не ограничена ни в какой окрестности $b$. Почему? По критерию Лебега. Если бы $f$ была ограничена, то множество точек её разрыва на каждом отрезке имеет меру 0. А счётное объединение множеств нулевой меры имеет нулевую меру (это мы не доказали ещё, но это правда). Так что причина неинтегрируемости на $[a;b]$ может быть только неограниченность. Эти случаи можно свести один к другому, но тем не менее мы будем писать общий случай.\\
            Самый основной вопрос с несобственным интегралом --- его сходимость. И про это будут несколько теорем этого параграфа.
        \end{Comment}
        \thm \undercolor{darkgreen}{Критерий Больцано-Коши для несобственных интегралов}. Пусть $-\infty<a<b\leqslant+\infty$, $f\in R_{\mathrm{loc}}[a;b)$. Для того, чтобы $\int_a^bf$ сходился необходимо и достаточно, чтобы $\forall\varepsilon>0~\exists\Delta\in(a;b)~\forall A;B\in(\Delta;b)~\left|\int_A^Bf\right|<\varepsilon$.
        \begin{Proof}
            Пусть $F(A)=\int_a^Af$. Сходимость несобственного интеграла равносильна существованию конечного $\lim\limits_{A\to b-}F(A)$. А это $\forall\varepsilon>0~\exists\Delta\in(a;b)~\forall A,B\in(\Delta;b)~|F(B)-F(A)|<\varepsilon$. Последняя разность равна тому, чему надо.
        \end{Proof}
        \begin{Comment}
            Обычно этот критерий применяется для выяснения расходимости интеграла. То есть проверяют отрицание этого утверждения. Например, проверяют в таком виде: если существуют две последовательности $A_n$ и $B_n$, принадлежащие $(a;b)$, которые стремятся к $b$, и при этом $\int_{A_n}^{B_n}f\nrightarrow0$, то несобственный интеграл расходится.
        \end{Comment}
        \begin{Comment}
            Как мы знаем, $\int_a^{\to b}f=\lim\limits_{A\to b-}\int_a^Af=\lim\limits_{A\to b-}F\big|_a^A=F\big|_a^b$, где под $F(b)$ подразумевается односторонний предел.
        \end{Comment}
        \pagebreak
        \begin{Example}
            $\int_1^{+\infty}\frac1{x^\alpha}~\mathrm dx$. Действуем по определению. Это равно $\begin{cases}
                \ln x\big|_1^{+\infty} & \alpha=1\\
                \frac{x^{1-\alpha}}{1-\alpha}\big|_1^{+\infty} & \alpha\neq1
            \end{cases}$. В верхнем случае получаем бесконечность. А во втором при $\alpha\leqslant1$ получаем тоже бесконечность, а иначе --- число. То есть
            $$\int_1^{+\infty}\frac1{x^\alpha}~\mathrm dx=\begin{cases}
                \frac1{\alpha-1} & \alpha>1\\
                +\infty & \alpha\leqslant1
            \end{cases}$$
        \end{Example}
        \begin{Example}
            $$\int_0^1\frac1{x^\alpha}~\mathrm dx=\begin{cases}
                \ln x\big|_0^1 & \alpha=1\\
                \frac{x^{1-\alpha}}{1-\alpha}\big|_0^1 & \alpha\neq1
            \end{cases}=\begin{cases}
                \frac1{1-\alpha} & x<1\\
                +\infty & x\geqslant1
            \end{cases}$$
        \end{Example}
        \begin{Example}
            Из теоремы выше $\int_0^{+\infty}\frac1{x^\alpha}~\mathrm dx$ расходится всегда.
        \end{Example}
        \begin{Comment}
            В нескольких следующих утверждениях (свойствах собственного интеграла) мы полагаем выполненными условия из определения несобственного интеграла.
        \end{Comment}
        \thm \undercolor{darkgreen}{Аддитивность несобственного интеграла}. Если $c\in(a;b)$, то сходимость $\int_a^bf$ и $\int_c^bf$ равносильна, и в случае сходимости $\int_a^bf=\int_a^cf+\int_c^bf$.
        \begin{Proof}
            Возьмём $A\in(c;b)$. Тогда $\int_a^Af=\int_a^cf+\int_c^Af$. Устремим $A$ к $b$ слева. Профит!
        \end{Proof}
        \dfn $\int_c^{\to b}f$ называется \undercolor{red}{остатком} $\int_a^{\to b}f$.
        \thm В условии предыдущего свойства остаток сходящегося несобственного интеграла бесконечно мал при $c\to b-$.
        \begin{Proof}
            Ну, если сходится, то предел равенства $\int_a^Af=\int_a^cf+\int_c^Af$ превращается в $\int_a^bf=\int_a^cf+\int_c^bf$, где остаток стремится к $\int_b^bf=0$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Линейность несобственного интеграла}. Пусть $\int_a^{\to b}f$ и $\int_a^{\to b}g$ сходятся. Тогда $\int_a^{\to b}f+g$ сходится и равен сумме исходных интегралов и $\int_a^{\to b}\alpha f$ тоже сходится и равен $\alpha\int_a^{\to b}f$.
        \begin{Proof}
            Объявляется очевидным. (Доказывается через линейность предела.)
        \end{Proof}
        \thm Пусть $\int_a^{\to b}f$ сходится, а $\int_a^{\to b}g$ расходится. Тогда $\int_a^{\to b}f+g$ расходится.
        \begin{Proof}
            Если бы он сходится, что сходится бы и $\int_a^{\to b}g$ как разность $\int_a^{\to b}f+g$ и $\int_a^{\to b}f$.
        \end{Proof}
        \begin{Comment}
            Про сумму двух расходящихся интегралов, понятно, сказать ничего нельзя.
        \end{Comment}
        \thm \undercolor{darkgreen}{Монотонность несобственного интеграла}. Пусть $\int_a^{\to b}f$ и $\int_a^{\to b}g$ существуют (не обязательно сходятся) и $f\leqslant g$. Тогда $\int_a^{\to b}f\leqslant\int_a^{\to b}g$.
        \begin{Proof}
            Предельный переход в неравенстве.
        \end{Proof}
        \thm Сохраняются интегральные неравенство Йенсена, Гёльдера и Минковского.
        \begin{Proof}
            Для частичных интегралов есть, предельный переход есть.
        \end{Proof}
        \begin{Comment}
            На несобственный интеграл произведение интегралов не переносится. То есть если $\int_a^{\to b}f$ и $\int_a^{\to b}g$ сходятся, то не факт, что $\int_a^{\to b}fg$ сходится. Можно взять $f=g=\frac1{\sqrt x}$ на $(0;1]$. Более подробно про это --- примеры сверху.
        \end{Comment}
        \begin{Comment}
            Про формулу Ньютона-Лейбница мы уже говорили в начале.
        \end{Comment}
        \thm \undercolor{darkgreen}{Интегрирование по частям (для неопределённых интегралов)}. Пусть $-\infty<a<b\leqslant+\infty$, $f,g$ дифференцируемы на $[a;b)$ и $f',g'\in R_{\mathrm{loc}}[a;b)$. Тогда $\int_a^{\to b}fg'=fg\big|_a^{\to b}-\int_a^{\to b}f'g$.
        \begin{Comment}
            Эту формулу надо понимать как <<если существуют и конечны два предела из трёх, то существует и третий и выполняется равенство>>.
        \end{Comment}
        \begin{Proof}
            В очередной раз запишем равенство для частичных интегралов и устремим $A\to b-$.
        \end{Proof}
        \begin{Comment}
            Перед формулировкой замены переменных, напомним соглашение о том, что при $a<b$ $\int_b^af=-\int_a^bf$. Оно будет действовать и для несобственных интегралов.
        \end{Comment}
        \thm \undercolor{darkgreen}{Замена переменных в несобственном интеграле}. Пусть $f\in C[A;B)$, $\varphi\colon[\alpha;\beta)\to[A;B)$ --- дифференцируема и имеет локально интегрируемую на $[\alpha;\beta)$ производную, и $\exists\varphi(b-)\in\overline{\mathbb R}$. Тогда
        $$\int_\alpha^{\to\beta}f(\varphi(t))\varphi'(t)~\mathrm dt=\int_{\varphi(\alpha)}^{\to\varphi(\beta-)}f(x)~\mathrm dx$$
        \begin{Comment}
            Это также следует понимать как <<если существует один интеграл, то существует и другой, и верно равенство>>.
        \end{Comment}
        \begin{Comment}
            Не исключено, что один или оба этих интеграла собственные. Т.е. замена переменной в собственном интеграле может привести к несобственному и наоборот. Примеры будут дальше.
        \end{Comment}
        \begin{Proof}
            Докажем при чуть более ограниченных условиях, для строго монотонной $\varphi$ (н.у.о., возрастающей). В книжке можно найти доказательство в полном объёме.\\
            Возьмём $\gamma\in(\alpha;\beta)$. Обозначим $C=\varphi(\gamma)\in(A;B)$ Докажем, что $\int_\alpha^\gamma f(\varphi(t))\varphi'(t)~\mathrm dt=\int_{\varphi(\alpha)}^Cf(x)~\mathrm dx$.\\
            Пусть существует $I=\int_{\varphi(\alpha)}^{\to\varphi(\beta-)}f(x)~\mathrm dx$. Нужно доказать, что другой интеграл тоже существует и равен $I$. Докажем на языке последовательностей. Рассмотрим последовательность $\gamma_n\in(\alpha;\beta)$, обозначим $C_n=\varphi(\gamma_n)$. Тогда для $\gamma_n\to\beta$ $\varphi(\gamma_n)\to\varphi(\beta-)$. А это, собственно, и значит, что $\int_\alpha^{\gamma_n}f(\varphi(t))\varphi'(t)~\mathrm dt\to I$, поскольку равен $\int_{\varphi(\alpha)}^{C_n}f(x)~\mathrm dx$.\\
            Теперь докажем в другую сторону. Пусть существует $\int_\alpha^{\to\beta}f(\varphi(t))\varphi'(t)~\mathrm dt$. Тогда рассмотрим $C_n$, обозначаем $\gamma_n=\varphi^{-1}(C_n)$ и проходит то же самое.
        \end{Proof}
        \begin{Comment}
            В условии теоремы можно сделать и б\'{о}льшую общность, но мы за ней гнаться не будем.\\
            А ещё можно взять монотонность, но перестать требовать непрерывность $f$.
        \end{Comment}
        \begin{Example}
            Тот самый пример с заменой и сменой собственности интеграла.\\
            $\int_0^\pi\frac1{2+\cos x}~\mathrm dx$. Можно сделать универсальную подстановку $t=\tan\frac x2$, резко получить интеграл по неограниченному промежутку $\int_0^{+\infty}\frac2{(1+t^2)(2+\frac{1-t^2}{1+t^2})}~\mathrm dx$. Это можно раскрыть и взять интеграл обычными методами. Получится $\frac2{\sqrt3}\atan\frac t{\sqrt3}\bigg|_0^{+\infty}=\frac\pi{\sqrt3}$.
        \end{Example}
        \begin{Comment}
            Вообще, случай неограниченного промежутка и неограниченной функции можно интуитивно перевести друг в друга, в интеграле $\int_a^{\to b}f(x)~\mathrm dx$ замену $t=\frac1{b-x}$, получив $\int\limits_{\frac1{b-a}}^{+\infty}\frac{f(b-\frac1t)}{t^2}~\mathrm dt$. Это не всегда приводит к упрощению, но всё же можно. А это значит, что все теоремы можно формулировать только для одного из типов несобственных интегралов.
        \end{Comment}
    \end{itemize}
    \subparagraph{\undercolorblack{orange}{Интегралы неотрицательный функций}.}
    \begin{itemize}
        \thm Пусть $-\infty<a<b\leqslant+\infty$, $f\in R_{\mathrm{loc}}[a;b)$, $f\geqslant0$. Тогда $\int_a^{\to b}f$ сходится тогда и только тогда, когда $F(A)=\int_a^Af$ ограничено сверху.
        \begin{Proof}
            Очевидно, $F$ возрастает. Чтобы это проверить, возьмите $F(B)-F(A)$, раскройте как разность интегралов и получите, что хотите. А как мы знаем, возрастающая функция $F$ имеет предел тогда, когда она ограничена.
        \end{Proof}
        \begin{Comment}
            А что будет, если $F$ будет не ограничена? Тогда, очевидно, несобственный интеграл равен $+\infty$. Так что для несобственного интеграла неотрицательной функции есть только два варианта --- или сходится, или расходится к $+\infty$. В обоих случаях несобственный интеграл равен $\sup\limits_{A\in[a;b)}F(A)$.
        \end{Comment}
        \begin{Comment}
            Для проверки сходимости достаточно ограниченности сверху какой-то последовательности частичных интегралов $f(A_n)$ (где $A_n\in[a;b)$ и $A_n\to b$).
        \end{Comment}
        \thm \undercolor{darkgreen}{Признак сравнения сходимости несобственных интегралов}. Пусть $-\infty<a<b\leqslant+\infty$, $f,g\in R_{\mathrm{loc}}[a;b)$, $f,g\geqslant0$, $f(x)=O(g(x))$ при $x\to b-$. Тогда
        \begin{enumerate}
            \item Если $\int_a^{\to b}g$ сходится, что $\int_a^{\to b}f$ тоже сходится.
            \item Если $\int_a^{\to b}f$ расходится, что $\int_a^{\to b}g$ тоже расходится.
        \end{enumerate}
        \begin{Proof}
            Заметим сначала, что достаточно доказать первое, потому что они равносильны.\\
            Распишем условие $f(x)=O(g(x))$. Это равносильно $\exists K>0,\Delta\in(a;b)~\forall x\in(\Delta;b)~f(x)\leqslant Kg(x)$. Рассмотрим остаток интеграла $f$: $\int_\Delta^{\to b}f\leqslant K\int_\Delta^{\to b}g<+\infty$. Вот и всё.
        \end{Proof}
        \thm \undercolor{darkgreen}{Признак сравнения в предельной форме}. Пусть $-\infty<a<b\leqslant+\infty$, $f,g\in R_{\mathrm{loc}}[a;b)$, $f\geqslant0$, $g>0$, $\exists\lim\limits_{x\to b-}\frac{f(x)}{g(x)}=l\in[0;+\infty]$. Тогда если $l$ конечно и $\int_a^{\to b}g$ сходится, то $\int_a^{\to b}f$ тоже сходится. Если $l>0$ и $\int_a^{\to b}f$ сходится, то $\int_a^{\to b}g$ тоже сходится. Если же $l\in(0;+\infty)$, то несобственные интегралы либо оба сходятся, либо оба расходятся.
        \begin{Proof}
            Если $l\neq+\infty$, то $\exists\Delta\in(a;b)~\forall x\in(\Delta;b)~\frac{f(x)}{g(x)}<l+1\Leftrightarrow f(x)<(l+1)g(x)$.\\
            Если $l\neq0$, то $f(x)$ в какой-то окрестности $b$ не ноль, а значит на неё можно делить, а значит можно применить первое утверждение, поменяв ролями $f$ и $g$.\\
            Третье следует тривиально из первых двух.
        \end{Proof}
        \thm Интегралы эквивалентных неотрицательных функций сходятся или расходятся одновременно.
        \begin{Proof}
            $l=1$.
        \end{Proof}
        \begin{Example}
            $\int_2^{+\infty}\frac1{x^\alpha\ln^\beta x}~\mathrm dx$. Без логарифма мы знаем, что происходит. А ещё мы знаем, что $\frac{\ln x}{x^\varepsilon}$ стремится к нулю при любом положительном $\varepsilon$. А раз это такая мелочь по сравнению со степенью. А значит при $\alpha<1$ он не поможет интегралу сойтись, в при $\alpha>1$ --- не помешает. Теперь формально.\\
            Пусть $\alpha>1$. Тогда $\alpha=\frac{\alpha+1}2+\frac{\alpha-1}2$. Первое $>1$, второе --- $>0$.  А это значит $\frac1{x^\alpha\ln^\beta x}=\frac1{x^{\frac{\alpha+1}2}}\frac1{x^{\frac{\alpha+1}2}\ln^\beta x}$. Второе стремится к нулю при $x\to+\infty$, а значит это произведение равно $O\left(\frac1{x^{\frac{\alpha+1}2}}\right)$. А значит изначальный интеграл сходится, так как $\int_2^{+\infty}\frac1{x^{\frac{\alpha+1}2}}$ сходится.\\
            Пусть $\alpha<1$. Делаем то же самое, но тут получается $\frac1{x^{\frac{\alpha+1}2}}=O\left(\frac1{x^{\frac{\alpha+1}2}\ln^\beta x}\right)$, а значит интеграл расходится.\\
            А при  $\alpha=1$ получаем $\int_2^{+\infty}\frac1{x\ln^\beta x}~\mathrm dx$, где можно обозначить $t=\ln x$ и получить $\int_{\ln 2}^{+\infty}\frac1{t^\beta}~\mathrm dt$, который сходится мы знаем когда.\\
            Итого интеграл сходится тогда и только тогда, когда $\left[\begin{aligned}
                &\alpha>1\\
                \alpha=1\land\beta>0
            \end{aligned}\right.$
        \end{Example}
        \begin{Comment}
            Таким же образом можно ещё сильнее это обобщать, получая функцию, которая стремится у нулю всё медленнее и медленнее, там будет всё также, что в не пограничных случаях степень у нового множителя не важна, а в пограничном всё решает.\\
            Но на самом деле, не обязательно, чтобы $f(x)$ стремилась к нулю. Причём даже если $f$ непрерывно, это не обязательно.
        \end{Comment}
        \begin{Example}
            Возьмём функцию, которая равна нулю очень много где, а в целочисленных точках имеет узкие высокие пики. Как конкретно мы будем это делать? А так:\\
            $f(k)=k$ при $k\in\mathbb Z_+$, $f(x)=0$ при $x\notin\bigcup\limits_{k=2}^\infty\left(k-\frac1{k^2(k+1)};k+\frac1{k^2(k+1)}\right)$. Во всех остальных точках $f$ линейна. Тогда чему равен интеграл? Он равен пределу таких частичных сумм: $\sum\limits_{k=1}^N\frac k2\cdot\frac2{k^2(k+1)}=\sum\limits_{k=1}^N\frac1{k(k+1)}=\sum\limits_{k=1}^N\frac1k-\frac1{k+1}=1-\frac1{N+1}$. Так что тут интеграл сходится не потому, что функция стремится к нулю быстро, а потому, что она не равна нулю очень мало где.
        \end{Example}
    \end{itemize}
    \subparagraph{\undercolorblack{orange}{Функции произвольного знака}.}
    \begin{itemize}
        \thm Из ограниченности частичных интегралов не следует сходимость несобственных интегралов.
        \begin{Proof}
            $\int_0^A\cos x~\mathrm dx=\sin A$, однако нет предела при $A\to+\infty$.
        \end{Proof}
        \dfn Если $\int_a^b|f|$ сходится, то говорят, что $\int_a^{\to b}f$ \undercolor{red}{абсолютно сходится}.
        \begin{Comment}
            У нас было определение сходимости, и было --- абсолютной сходимости. И там, и там, есть слово <<сходимость>>, но тем не менее следствие из второго в первое нужно доказать.
        \end{Comment}
        \thm Если $\int_a^{\to b}f$ и $\int_a^{\to b}g$ абсолютно сходятся, то $\int_a^{\to b}f+g$ также абсолютно сходится.
        \begin{Proof}
            $|f+g|\leqslant|f|+|g|$.
        \end{Proof}
        \thm Если существует $\int_a^{\to b}f$, то $\left|\int_a^{\to b}f\right|\leqslant\int_a^{\to b}|f|$.
        \begin{Proof}
            Для частичных интегралов запишите, примените предельный переход.
        \end{Proof}
        \thm Если интеграл абсолютно сходится, то он сходится.
        \begin{Proof}
            Первый способ --- критерий Больцано-Коши.\\
            Второй способ содержит вспомогательные функции, которые в течение всего курса нам будут полезны.
        \end{Proof}
        \dfn Пусть $x\in\mathbb R$. Тогда \undercolor{red}{положительная часть} $x$ --- $x_+=\begin{cases}
            x & x\geqslant0\\
            0 & x<0
        \end{cases}$, а \undercolor{red}{отрицательная часть} $x$ --- $x_-=\begin{cases}
            -x & x\leqslant0\\
            0 & x>0
        \end{cases}$.
        \thm Тривиальные свойства: $0\leqslant x_\pm\leqslant|x|$, $x_++x_-=|x|$, $x_+-x_-=x$.
        \thm $x_+=\frac{|x|+x}2=\max\{x;0\}$, $x_-=\frac{|x|-x}2=\max\{-x;0\}$.
        \dfn Положительная и отрицательная часть функции $f$ --- $f_\pm(x)=(f(x))_\pm$.
        \begin{Proof}
            Доказательство следствия из абсолютной сходимости в сходимость.\\
            Поскольку $\int_a^{\to b}|f|$ сходится, оба $\int_a^{\to b}f_\pm$ также сходятся. А это уж точно значит, что $\int_a^{\to b}f$ сходится как разность сходящихся.
        \end{Proof}
        \thm Обратное утверждение неверно, пример будет позже.
        \dfn Если $\int_a^{\to b}f$ сходится, но не абсолютно сходится, то говорят, что $\int_a^{\to b}$ \undercolor{red}{условно сходится}.
        \begin{Comment}
            Вместо этого термина часто говорят \undercolor{red}{неабсолютно сходится}.
        \end{Comment}
        \thm Если $\int_a^{\to b}f$ сходится условно, а $\int_a^{\to b}g$ --- абсолютно, то $\int_a^{\to b}f+g$ сходится условно.
        \begin{Proof}
            Мы знаем, что он сходится, а если бы он абсолютно сходится, то разность двух абсолютно сходящихся абсолютно сходится.
        \end{Proof}
        \thm \undercolor{darkgreen}{Признаки Дирихле и Абеля сходимости несобственных интегралов}. Пусть $-\infty<a<b\leqslant+\infty$. Пусть $f\in C[a;b)$, а $g\in C^{(1)}[a;b)$ и $g$ монотонна. Тогда
        \begin{itemize}
            \item[\undercolor{darkgreen}{Признак Дирихле}.] Если частичные интегралы $F(A)=\int_a^Af$ ограничены, а $g\underset{x\to b-}\longrightarrow0$, то $\int_a^{\to b}fg$ сходится.
            \item[\undercolor{darkgreen}{Признак Абеля}.] Если $\int_a^{\to b}f$ сходится, а $g$ ограничена, то $\int_a^{\to b}fg$ сходится.
        \end{itemize}
        \begin{Proof}
            Можно интегрировать по частям, а можно применить вторую теорему о среднем.\\
            $\int_A^Bfg=g(A)\int_a^Cf+g(B)\int_C^Bf$, где $C\in(A;B)$. Теперь возьмём $\varepsilon>0$.\\
            Дирихле: Тогда, если $F$ ограничены числом $K$, то подберём $\Delta$ по числу $\frac\varepsilon{4K}$: $\exists\Delta\in(a;b)~\forall x\in(\Delta;b)~|g(x)|>\frac\varepsilon{4K}$. Также тривиально, $\forall u,v\in(a;b)~\left|\int_u^vf\right|\leqslant 2K$. А тогда $\forall A,B\in(\Delta;b)~\left|\int_A^Bfg\right|<\frac\varepsilon{4K}\cdot2K+\frac\varepsilon{4K}\cdot 2K=\varepsilon$.\\
            Абеля: $\exists L>0~\forall x\in[a;b)~|g(x)|\leqslant L$ и $\exists\Delta\in(a;b)~\forall c\in(\Delta;b)~\left|\int_a^{\to b}f\right|\leqslant\frac\varepsilon{4L}$. Тогда $\forall A,B\in(\Delta;b)~\left|\int_A^Bfg\right|\leqslant L\frac\varepsilon{2L}+L\frac\varepsilon{2L}=\varepsilon$. В обоих случаях получаем условие теоремы Больцано-Коши.
        \end{Proof}
        \begin{Comment}
            Достаточно, чтобы $f$ и $g$ были локально интегрируемы на $[a;b)$ (с сохранением условия монотонности). (При этих условиях было обещано, но не доказано, что верна теорема о среднем.)
        \end{Comment}
        \begin{Example}
            Рассмотрим $\int_0^{+\infty}g(x)\sin\lambda x~\mathrm dx$ и $\int_0^{+\infty}g(x)\cos\lambda x~\mathrm dx$, где $g$ монотонна, а $\lambda\in\mathbb R$. Для определённости рассмотрим первый, со вторым аналогично.\\
            Во-первых, из нечётности $\lambda<0$ сводится к $\lambda>0$. Во-вторых, при $\lambda=0$ первый интеграл вообще скучный, а второй --- непонятный. А в случае $\lambda>0$ можно провести замену $t=\lambda x$, избавившись от $\lambda$.\\
            Так как $g$ монотонна, можно считать, что она не меняет знак. И не умаляя общности, этот знак --- $+$.\\
            Итак, рассматриваем $\int_0^{+\infty}g(x)\sin x~\mathrm dx$. Если $\int_0^{+\infty}g$ сходится, то и произведение точно абсолютно сходится т.к. $|g(x)\sin x|\leqslant|g(x)|$. То есть что-то более трудное может получиться только если абсолютной сходимости $g$ нет.\\
            Рассмотрим $\lim\limits_{x\to+\infty}=l\in[0;+\infty]$. Докажем, что при $l=0$ интеграл сходится по признаку Дирихле. Ну, действительно, $\left|\int_0^A\sin x~\mathrm dx\right|=|1-\cos A|\leqslant 2$.\\
            Хорошо, мы знаем, что при $l=0$ интеграл сходится. Но не знаем, абсолютно ли. Когда $\int_0^{+\infty}g$ сходится, очевидно, да. А когда не сходится, нетривиально. В этом случае заметим, что $|\sin x|\geqslant\sin^2x=\frac{1-\cos 2x}2$. Тогда $\frac12\int_0^{+\infty}g(x)\mathrm dx$ расходится по условию, а $\frac12\int_0^{+\infty}g(x)\cos 2x\mathrm dx$ сходится по Дирихле, а значит $\int_0^{+\infty}g(x)\frac{1-\cos2x}2~\mathrm dx$ расходится, что не даёт $\int_0^{+\infty}g(x)|\sin x|~\mathrm dx$ сойтись.\\
            А теперь докажем, что при $l>0$ интеграл расходится. Давайте рассмотрим интеграл по кусочкам, где синус больше либо равен $\frac12$. Это кусочки $\left[\frac\pi6+2\pi k;\frac{5\pi}6+2\pi k\right]$. $\int\limits_{\frac\pi6+2\pi k}^{\frac{5\pi}6+2\pi k}g(x)\sin x~\mathrm dx\geqslant\frac12\int\limits_{\frac\pi6+2\pi k}^{\frac{5\pi}6+2\pi k}g=\frac12\frac{2\pi}3\inf\limits_{\left[\frac\pi6+2\pi k;\frac{5\pi}6+2\pi k\right]}g\underset{n\to\infty}\longrightarrow\frac\pi3l>0$.
        \end{Example}
        \begin{Comment}
            Как мы помним, $\int_{\to a}^{\to b}=\int_{\to a}^cf+\int_c^{\to b}f$, где $c\in(a;b)$.
        \end{Comment}
        \begin{Example}
            $\int_0^{+\infty}\sin x^2~\mathrm dx\overset{t=x^2}=\int_0^{+\infty}\frac{\sin t}{2\sqrt t}~\mathrm dt$. Условно сходится. Но тут причина сходимости и не в том, что она хорошо к нулю стремится, и не в том, что много где функция равна нулю, а в том, что положительные и отрицательные части этой функции съедают друг друга намного быстрее, чем успевают накопиться.\\
            $\int_1^{+\infty}\frac{\sin x}{x^\alpha}$ абсолютно сходится при $\alpha>1$, условно сходится при $\alpha\in(0;1]$ и расходится при $\alpha\leqslant0$.
        \end{Example}
        \dfn Пусть $f\colon(a;b)\to\mathbb R$. $c\in(a;b)$ --- \undercolor{red}{особая точка}, если $f$ не интегрируема ни на каком замыкании окрестности $c$.
        \dfn Пусть $f\colon(a;b)\to\mathbb R$. $a$ --- \undercolor{red}{особая точка}, если $a=-\infty$ или $a>-\infty$, но \textit{смотри определение сверху}.
        \dfn \undercolor{red}{Интеграл функций с конечным множеством особых точек} $a=c_0<c_1<c_2<\cdots<c_m=b$ на $(a;b)$ определим $\int_a^bf$ как $\sum\limits_{k=0}^{m-1}\int_{c_k}^{c_{k+1}}f$, если эта сумма имеет смысл.
        \dfn Пусть $f\colon[a;b]\to\mathbb R$, $c\in(a;b)$ --- единственная особая точка. Тогда $\int_a^bf=\int_a^cf+\int_c^bf=\lim\limits_{\substack{\varepsilon_1\to0+\\\varepsilon_2\to0+}}\int_a^{c-\varepsilon_1}f+\int_{c+\varepsilon_2}^bf$. Эта штука называется \undercolor{red}{главным значением} интеграла и обозначается $\mathrm{v.p.}\int_a^bf$.
        \thm Если интеграл сходится, то в смысле главного значения он и подавно сходится, но не наоборот.
        \begin{Proof}
            Так, $\int_{-1}^1\frac1x~\mathrm dx$ расходится, но $\mathrm{v.p.}\int_{-1}^1\frac1x~\mathrm dx=0$.
        \end{Proof}
        \dfn Аналогично, $\mathrm{v.p.}\int_{-\infty}^{+\infty}f$ --- это никогда не догадаетесь кто.
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Приложения интеграла}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Приложения к вычислению разных физических величин.\\
            Суть всегда одна. Искомая величина либо и является пределом сумм, либо ограничена снизу и сверху суммами, которые оказываются интегральными, либо эти суммы похожи на интегральные с низкой погрешностью --- детали разные, суть всегда одна.\\
            Начнём с того, что такое площадь. И тут будет частичное определение.
        \end{Comment}
        \dfn Отображение $U\colon\mathbb R^m\to\mathbb R^m$ --- \undercolor{red}{движение}, если оно сохраняет (евклидово) расстояние между точками (т.е. $\forall x,y\in\mathbb R~|U(x)-U(y)|=|x-y|$).
        \begin{Comment}
            Отсюда следуют разные свойства этого отображения (линейность и биективность), например, но мы их доказывать не будем.
        \end{Comment}
        \dfn \undercolor{red}{Площадью} называется неотрицательный функционал $S\colon\{P\}\to[0;+\infty)$, заданный на некотором классе $\{P\}$ подмножеств $\mathbb R^2$, называемых квадрируемыми фигурами, и обладающий следующими свойствами (аксиомами площади):
        \begin{enumerate}
            \item Аддитивность: если $P_1$ и $P_2$ --- квадрируемые фигуры и $P_1\cap P_2=\varnothing$, то $P_1\cup P_2$ квадрируемо и $S(P_1\cup P_2)=S(P_1)+S(P_2)$.
            \item Нормированность на прямоугольниках: площадь прямоугольника со сторонами $a$ и $b$ равна $ab$.
            \item Инвариантность относительно движений: если $P$ --- квадрируемая фигура, а $U$ --- движение $\mathbb R^2$, то $U(P)$ квадрируема и $S(U(P))=S(P)$.
        \end{enumerate}
        \begin{Comment}
            Это определение, конечно, не закончено, потому что мы не определили квадрируемые фигуры. Этого мы делать и не будем, но скажем нечто про это, когда будет теория меры.\\
            Также мы не будем говорить ничего о существовании и единственности площади. Потому наши утверждения будут иметь следующий характер: если площадь существует, то она, там, равна интегралу или если она существует, то какое-то другое её свойство выполняется.
        \end{Comment}
        \begin{Comment}
            Ещё во втором свойстве не сказано, какой прямоугольник --- открытый, замкнутый или кто вообще. Так вот, произвольный. То есть такая фигура, которая содержится в замкнутом и содержит открытый с теми же сторонами.
        \end{Comment}
        \begin{Comment}
            Третье свойство в школьной геометрии формулируется как <<площади равных фигур равны>>.
        \end{Comment}
        \thm \undercolor{darkgreen}{Монотонность площади}. Если $P_1\supset P_2$ --- квадрируемые фигуры, то $S(P_1)\leqslant S(P_2)$.
        \begin{Proof}
            $P_2=P_1\cup(P_2\setminus P_1)$. Мы просто верим в то, что $P_2\setminus P_1$ --- квадрируема, а если так, то $S(P_2)=S(P_1)+S(P_2\setminus P_1)\geqslant S(P_1)$.
        \end{Proof}
        \thm Если фигура $P$ содержится в некотором отрезке, то её площадь --- ноль.
        \begin{Proof}
            Пусть отрезок имеет длину $a$. Тогда фигуру $P$ можно заключить в произвольно малый прямоугольник $a\times\varepsilon$. А значит $\forall\varepsilon>0~S(P)\leqslant a\varepsilon$. А значит $S(P)$ не может быть положительным числом.
        \end{Proof}
        \thm \undercolor{darkgreen}{Усиленная аддитивность}. Если $S(P_1\cup P_2)=0$ и $P_1,P_2$ --- квадрируемы, то $S(P_1\cup P_2)=S(P_1)+S(P_2)$.
        \begin{Proof}
            $P_1\cup P_2=(P_1\cup P_2)\cup(P_1\setminus P_2)\cup(P_2\setminus P_1)$. Площадь первого --- 0, $S(P_1\setminus P_2)=S(P_1)-S(P_1\cap P_2)=S(P_1)$, $S(P_2\setminus P_1)=S(P_2)$.
        \end{Proof}
        \dfn Пусть $f\colon[a;b]\to\mathbb R_+$. Тогда \undercolor{red}{подграфик} $f$ --- множество $Q_f=\{(x;y)\mid x\in[a;b]\land y\in[0;f(x)]\}$.
        \thm Пусть $f$ не только задана на $[a;b]$ и неотрицательна, но ещё и интегрируема. Тогда площадь подграфика равна $\int_a^bf$.
        \begin{Proof}
            Возьмём $\tau=\{x_k\}_{k=0}^n$ --- дробление $[a;b]$. Пусть $M_k=\sup\limits_{[x_k;x_{k+1}]}f$, $m_k=\inf\limits_{[x_k;x_{k+1}]}f$. Пусть $S_\tau=\sum\limits_{k=0}^{n-1}M_k\Delta x_k$, $s_\tau=\sum\limits_{k=0}^{n-1}m_k\Delta x_k$. Аналитически это суммы Дарбу, геометрически --- сумма площадей прямоугольников строго большая и строго меньшая соотвественно данного подграфика. То есть $s_\tau\leqslant S(Q_f)\leqslant S_\tau$. Но также $s_\tau\leqslant\int_a^bf\leqslant S_\tau$. Но между всеми верхними и нижними суммами может содержаться только одно число, а значит $Q_f=\int_a^bf$
        \end{Proof}
        \begin{Comment}
            То же самое рассуждение применимо, если мы из подграфика выкидываем ось $x$ и/или график $f$ (т.е. в определении подграфика пишем $y\in(0;f(x))$).
        \end{Comment}
        \begin{Comment}
            Этот результат обобщается на основе свойств площади и здравого смысла. А именно, если функция меняет знак, то придётся на положительных участках брать $\int_a^bf$, а на отрицательных --- $-\int_a^bf$. Но это первое обобщение. А второе обобщение --- пусть $f\leqslant g$, и $f,g\in R[a;b]$. Тогда чтобы посчитать площадь между графиками $f$ и $g$, надо взять $\int_a^bg-f$.
        \end{Comment}
        \begin{Example}
            Площадь эллипса. Эллипс --- множество $\{(x;y)\mid\frac{x^2}{a^2}+\frac{y^2}{b^2}\leqslant1\}$, где $a,b>0$.\\
            Из соображений симметрии площадь эллипса --- четыре подграфика $b\sqrt{1-\frac{x^2}{a^2}}$ на $[0;a]$. $S=4\int_0^ab\sqrt{1-\frac{x^2}{a^2}}~\mathrm dx=\frac{4b}a\int_0^a\sqrt{a^2-s^2}~\mathrm dx\overset{x=a\cos t}=4ab\int_0^{\pi/2}\sin^2t~\mathrm dt=\pi ab$.
        \end{Example}
        \dfn Пусть $0<\beta-\alpha\leqslant2\pi$, $f\colon[\alpha;\beta]\to\mathbb R_+$. \undercolor{red}{Криволинейный сектор} --- это множество $\widetilde{Q_f}=\{(r\cos\varphi;r\sin\varphi)\mid\substack{\varphi\in[\alpha;\beta]\\r\in[0;f(\varphi)]}\}$.
        \thm Площадь криволинейного сектора. Если $f\in[\alpha;\beta]$, то $\widetilde{Q_r}=\frac12\int_\alpha^\beta f^2$.
        \begin{Proof}
            Возьмём $\tau=\{t_k\}_{k=0}^n$ --- дробление $[\alpha;\beta]$. Пусть $M_k=\sup\limits_{[t_k;t_{k+1}]\frac12f^2}$, $m_k=\inf\limits_{[t_k;t_{k+1}]\frac12f^2}$, $S_\tau=\sum\limits_{k=0}^{n-1}M_k\Delta t_k$, $s_\tau=\sum\limits_{k=0}^{n-1}m_k\Delta t_k$. Геометрически $M_k$ и $m_k$ --- площади б\'{о}льших и меньших круговых секторов. Аналитически --- суммы Дарбу для $\frac12f^2$.
        \end{Proof}
        \begin{Example}
            $r=a\varphi$, где $r,\varphi\geqslant0$, $a>0$ (спираль Архимеда). Хочется найти площадь от 0 до $2\pi$. Получится $\frac12\int_0^{2\pi}a^2\varphi^2~\mathrm d\varphi=\frac{4a^2\pi^3}3$.
        \end{Example}
        \dfn \undercolor{red}{Объёмом} называется неотрицательный функционал $V\colon\{T\}\to\mathbb R_+$, заданный на некотором классе $\{T\}$ тел (подмножеств $\mathrm R^3$), называемых кубируемыми, и обладающий следующими трем свойствами (аксиомами объёма):
        \begin{enumerate}
            \item Аддитивность.
            \item Нормированность на прямоугольных параллелепипедах.
            \item Инвариантность относительно движений.
        \end{enumerate}
        \begin{Comment}
            Комментарии те же самые, что для площади.
        \end{Comment}
        \thm \undercolor{darkgreen}{Монотонность объёма}.
        \thm Если тело содержится в некотором прямоугольнике, то его объём равен 0.
        \thm \undercolor{darkgreen}{Усиленная аддитивность}.
        \begin{Comment}
            См. аналогичные утверждения для площади.
        \end{Comment}
        \dfn Пусть $T\subset\mathbb R^3$, $x\in\mathbb R$. \undercolor{red}{Сечением} $T$ абсциссой $x$ называется множество $T(x)=\{(y;z)\mid(x;y;z)\in T\}$.
        \begin{Comment}
            \begin{center}
                \begin{tikzpicture}[scale=2]
                    \begin{axis}[
                        width = .3\textwidth,
                        height = .3\textwidth,
                        grid = major,
                        view={15}{30},
                        xmin = -5, xmax = 5,
                        ymin = -5, ymax = 5,
                        zmin = -5, zmax = 5,
                        xlabel = {$x$},
                        ylabel = {$y$},
                        zlabel = {$z$},
                        ticks = none,
                        colormap/cool,
                        ]
                        \pgfmathparse{sqrt(1-(-.5)^2/3^2)}
                        \let\s\pgfmathresult
                        \addplot3[domain=0:360,samples y=0,pattern=north east lines,pattern color=white]({-.5},{4*\s*sin(x)},{5*\s*cos(x)});
                        \addplot3[opacity=0,fill opacity=0.2,surf,domain=0:180,y domain=0:360]({3*sin(x)*cos(y)},{4*sin(x)*sin(y)},{5*cos(x)});
                    \end{axis}
                \end{tikzpicture}
            \end{center}
            При этом считается, что все сечения спроектированы на плоскость $yz$.
        \end{Comment}
        \begin{Comment}
            По умолчанию считаем, что $T(x)$ --- это сечение по абсциссе, а для сечения по остальным пишем $T_1(x)$, $T_2(y)$, $T_3(z)$.
        \end{Comment}
        \dfn Пусть $P\subset\mathbb R^2$, $h\geqslant0$, $Q=P\times[0;h]$. Тогда $Q$, а также любой образ $Q$ при движении называется \undercolor{red}{прямым цилиндром} с основанием $P$ и высотой $h$.
        \thm Если основание цилиндра квадрируемо, то сам цилиндр кубируем, а его площадь равна $S(P)\cdot h$.
        \begin{Proof}
            Приближаем основание снаружи и изнутри многоугольниками (которые сколь угодно сверху и снизу приближаются к границе $P$), а значит наш цилиндр приближается прямоугольными призмами, бла-бла-бла, махаем руками много и долго. Доказывать это честно мы не будем.
        \end{Proof}
        \thm Пусть тело $T$ удовлетворяет условиям
        \begin{enumerate}
            \item $\forall x\notin[a;b]~T(x)=\varnothing$. (То есть всё тело лежит между плоскостями $x=a$ и $x=b$.)
            \item $\forall x\in[a;b]~T(x)$ квадрируема и имеет площадь $S(x)$.
            \item $S\in R[a;b]$.
            \item $\forall\Delta\subset[a;b]~\exists\xi^*_\Delta,\xi^{**}_\Delta\in\Delta~\forall x\in\Delta~T(\xi^*_\Delta)\subset T(x)\subset T(\xi^{**}_\Delta)$.
        \end{enumerate}
        Тогда $T$ кубируемо и $V(T)=\int_a^bS$.
        \begin{Comment}
            Представьте себе колоду карт, в которой вы подвигали карты друг относительно друга. Но ваши карты бесконечно тонкие. Функция $S$ константа, поскольку все карты одинаковы, но они могут быть сдвинуты относительно друг друга так сильно, что объём телу приписать не получится. Поэтому тут есть третье условие.
        \end{Comment}
        \begin{Proof}
            Кубируемость мы доказывать не будем, а формулу --- будем.\\
            Возьмём дробление $\tau=\{x_k\}_{k=0}^n$ --- дробление $[a;b]$, $M_k=\sup\limits_{[x_k;x_{k+1}]}S$, $m_k=\inf\limits_{[x_k;x_{k+1}]}S$ (на самом деле $M_k$ и $m_k$ --- максимум и минимум соответственно, ввиду условия 3). Построим максимальный и минимальный (на $[x_k;x_{k+1}]$) цилиндр тела.
            \begin{center}
                \begin{tikzpicture}[scale=2]
                    \begin{axis}[
                        width = .3\textwidth,
                        height = .3\textwidth,
                        grid = major,
                        view={15}{30},
                        z buffer=sort,
                        xmin = -1, xmax = 1,
                        ymin = -1, ymax = 1,
                        zmin = -1, zmax = 1,
                        xlabel = {$x$},
                        ylabel = {$y$},
                        zlabel = {$z$},
                        ticks = none,
                        colormap/cool,
                        ]
                        
                        \addplot3[opacity=0,fill opacity=0.5,surf,domain=0:.75,y domain=0:360]({-.5+4*x/3},{(.5+x^2)*cos(y)},{(.5+x^2)*sin(y)});
                        
                        \addplot3[opacity=0.5,fill opacity=0,surf,faceted color=black,domain=0:.75,y domain=0:360,samples=6,samples y=20]({-.5+4*x/3},{.5*cos(y)},{.5*sin(y)});
                    \end{axis}
                \end{tikzpicture}
                \begin{tikzpicture}[scale=2]
                    \begin{axis}[
                        width = .3\textwidth,
                        height = .3\textwidth,
                        grid = major,
                        view={15}{30},
                        z buffer=sort,
                        xmin = -1, xmax = 1,
                        ymin = -1, ymax = 1,
                        zmin = -1, zmax = 1,
                        xlabel = {$x$},
                        ylabel = {$y$},
                        zlabel = {$z$},
                        ticks = none,
                        colormap/cool,
                        ]
                        
                        \addplot3[opacity=0,fill opacity=0.5,surf,domain=0:.75,y domain=0:360]({-.5+4*x/3},{(.5+x^2)*cos(y)},{(.5+x^2)*sin(y)});
                        
                        \addplot3[opacity=0.5,fill opacity=0,surf,faceted color=black,domain=0:.75,y domain=0:360,samples=6,samples y=20]({-.5+4*x/3},{(.5+.75^2)*cos(y)},{(.5+.75^2)*sin(y)});
                    \end{axis}
                \end{tikzpicture}
            \end{center}
            $T_k$ --- слой тела на $[x_k;x_{k+1}]$, $q_k$ --- цилиндр с меньшим основанием, $Q_k$ --- с б\'{о}льшим. Тогда $q_k\subset T_k\subset Q_k$. Площади оснований цилиндров --- $m_k$ и $M_k$, высоты --- $\Delta x_k$. Тогда $\sum\limits_{k=0}^{n-1}m_k\Delta x_k\leqslant V(T)\leqslant\sum\limits_{k=0}^{n-1}M_k\Delta x_k$. Здравствуйте, суммы Дарбу.
        \end{Proof}
        \begin{Example}
            Объём эллипсоида $D=\{(x;y;z)\mid\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}\leqslant1\}$, $a,b,c>0$ --- полуоси. Если $|x|=a$, то сечение состоит из точки, а если $|x|<a$, то получается эллипс $\frac{y^2}{b^2\left(1-\frac{x^2}{a^2}\right)}+\frac{z^2}{c^2\left(1-\frac{x^2}{a^2}\right)}\leqslant1$. Это --- эллипс с полуосями $b\sqrt{1-\frac{x^2}{a^2}}$ и $c\sqrt{1-\frac{x^2}{a^2}}$. Тогда $S=\pi bc\left(1-\frac{x^2}{a^2}\right)$. $\int_{-a}^a\pi bc\left(1-\frac{x^2}{a^2}\right)~\mathrm dx=2\pi bc\int_0^a1-\frac{x^2}{a^2}~\mathrm dx=\frac43\pi abc$.
        \end{Example}
        \dfn Пусть $f\colon[a;b]\to\mathbb R_+$. Рассмотрим её подграфик и будем вращать его вокруг оси абсцисс. Получится множество $T_f=\{(x;y;z)\mid x\in[a;b]\land y^2+z^2\leqslant f^2(x)\}$ --- \undercolor{red}{тело вращения} подграфика $f$ вокруг оси $x$.
        \thm Если $f$ интегрируема и неотрицательна, то $V(T_f)=\pi\int_a^bf^2$.
        \begin{Proof}
            Действительно, при фиксированном $x$ $T(x)$ --- круг с радиусом $f(x)$.
        \end{Proof}
        \begin{Example}
            Возьмём тор и посчитаем его объём. Для этого будем крутить круг вокруг оси $x$. Как это выглядит на двухмерном графике? Как окружность с центром $(0;R)$ и радиусом $r$. А значит задаётся это как $x^2+(y-R)^2=r^2$. Тогда $y=R\pm\sqrt{r^2-x^2}$, $x\leqslant r$. Пусть $f_1(x)=R-\sqrt{r^2-x^2}$ --- нижняя полуокружность, $f_2(x)=R+\sqrt{r^2-x^2}$ --- верхняя полуокружность. Тогда $V=\pi\int_{-r}^rf_2^2(x)-f_1^2(x)~\mathrm dx=4\pi R\int_{-r}^r\sqrt{x^2-r^2}~\mathrm dx=2\pi^2Rr^2$.
        \end{Example}
        \begin{Comment}
            Если брать площади неограниченных фигур, то формулы останутся верными, но в них будут участвовать несобственные интегралы.
        \end{Comment}
        \begin{Comment}
            Теперь мы хотим вычислять длины. Тут у нас не будет дыр в определении, мы чётко сформулируем, что такое длина.
        \end{Comment}
        \dfn Напоминание: \undercolor{red}{Путём} (в $\mathbb R^m$) называется непрерывное отображение отрезка в $\mathbb R^m$.
        \dfn \undercolor{red}{Путь замкнутый}, если его начало совпадает с концом.
        \dfn Путь \undercolor{red}{жорданов} (или \undercolor{red}{несамопересекающийся}), если $\gamma(t_1)=\gamma(t_2)$ возможно лишь при $t_1=t_2$ или $t_1=a\land t_2=b$ или $t_1=b\land t_2=a$.
        \dfn Пусть $r\in\mathbb N\cup\{\infty\}$. Тогда путь $\gamma$ \undercolor{red}{$r$-гладкий}, если $\forall i\in[1:m]~\gamma_i\in C^{(r)}[a;b]$. Просто \undercolor{red}{гладкий} --- 1-гладкий.
        \dfn Пусть \undercolor{red}{кусочно-гладкий}, если отрезок $[a;b]$ можно разбить на $n$ таких частей, что сужение пути на каждую часть гладко.
        \dfn Если $\gamma$ --- путь, то $\gamma^-$ --- такой путь что $\gamma^-(t)=\gamma(a+b-t)$ --- путь, \undercolor{red}{противоположный} $\gamma$.
        \dfn $\gamma^*=\gamma([a;b])$ --- \undercolor{red}{носитель пути} $\gamma$.
        \begin{Comment}
            Первое движение души --- назвать кривой носитель пути. Но это слишком широкое определение --- в него попадают какие-нибудь кривые Пеано (путь, носитель которого --- квадрат, куб или $m$-мерный куб). А второй вопрос связан с вычислением длин. Мы можем одну и ту же кривую задавать разными уравнениями, и хочется спросить, одно это и тоже или нет. Например, верхняя полуокружность --- это и $\gamma_1(t)=(t;\sqrt{1-t^2})\mid t\in[0;1]$, и $\gamma_2(t)=(\cos t;\sin t)\mid[0;\pi]$, и $\gamma_3(t)=(-\cos t;\sin t)\mid[0;\pi]$, и $\gamma_2(t)=(\cos t;|\sin t|)\mid[0;2\pi]$. Первое и третье, вроде как, одинаково совсем, второе отличается от третьей направлением обхода, а в четвёртой вообще обход осуществляется дважды.
        \end{Comment}
        \dfn \undercolor{red}{Пути} $\gamma\colon[a;b]\to\mathbb R^m$ и $\tilde\gamma\colon[\alpha;\beta]\to\mathbb R^m$ \undercolor{red}{эквивалентны}, если $\exists u\in[a;b]\to[\alpha;\beta]$ --- строго возрастающая сюръекция, для которой верно $\gamma=\tilde\gamma\circ u$. Функция $u$ называется \undercolor{red}{допустимым преобразованием параметра}.
        \thm Допустимое преобразование параметра непрерывно.
        \begin{Proof}
            Действует на отрезке, возрастает и имеет отрезок своим множеством значений.
        \end{Proof}
        \thm Отношение выше --- отношение эквивалентности.
        \begin{Proof}
            Рефлексивно --- $\id$, симметрично $u^{-1}$, транзитивно --- $u_1\circ u_2$.
        \end{Proof}
        \dfn Класс эквивалентных путей называется \undercolor{red}{кривой}, а каждый его представитель --- \undercolor{red}{параметризация кривой}.
        \thm Начала, концы и носители эквивалентных путей совпадают (поэтому можно говорить о начале, конце и носителе кривой).
        \dfn \undercolor{red}{Кривая $r$-гладкая}, \undercolor{red}{гладкая}, \undercolor{red}{кусочно-гладкая}, если у неё есть соответствующая параметризация.
        \begin{Comment}
            Если функция $u$ не гладкая, то гладкость испортится, поэтому у гладкой кривой может быть не гладкий представитель. Поэтому в качестве отношения эквивалентности можно требовать ещё и то, чтобы $u$ и $u^{-1}$ лежали в классе $C^{(r)}[a;b]$. Это называется $C^{(r)}$-эквивалентными путями.
        \end{Comment}
        \begin{Comment}
            Чего мы хотим от длины пути? Мы хотим аддитивность, мы хотим, чтобы она была не больше длины отрезка (из чего уже следует, что длина пути не больше длины ломаной), а ещё хочется, чтобы длину пути можно было сколь угодно близко приближалась вписанными ломаными. И вот эти три требования запишем в определение длины.
        \end{Comment}
        \dfn Пусть $\gamma$ --- путь в $\mathbb R^m$, заданный на $[a;b]$. \undercolor{red}{Длиной пути} $\gamma$ называется величины $s_\gamma=\sup_{\tau}l_\tau$, где $\tau=\{t_k\}_{k=0}^n$ --- дробление $[a;b]$, а $l_\tau=\sum\limits_{k=0}^{n-1}|\gamma(t_k)-\gamma(t_{k+1})|$.
        \dfn Если $s_\gamma<+\infty$, то $\gamma$ --- \undercolor{red}{спрямляемый путь}.
        \thm Существуют неспрямляемые пути.
        \begin{Proof}
            Пример будет позже.
        \end{Proof}
        \thm Длины эквивалентных путей равны.
        \begin{Proof}
            Пусть есть $\gamma\colon[a;b]\to\mathbb R^m$, $\tilde\gamma\colon[\alpha;\beta]\to\mathbb R^m$, $u\colon[a;b]\to[\alpha;\beta]$ --- возрастающая сюръекция, $\gamma=\tilde\gamma\circ u$.\\
            Пусть $\tau=\{t_k\}_{k=0}^{n}$ --- дробление $[a;b]$, $\theta_k=u(t_k)$ --- дробление $[\alpha;\beta]$. Тогда $l_\tau=\sum\limits_{k=0}^{n-1}|\gamma(t_{k+1})-\gamma(t_k)|=\sum\limits_{k=0}^{n-1}|\tilde\gamma(\theta_{k+1})-\tilde\gamma(\theta_k)|\leqslant s_{\tilde\gamma}$. Отсюда $s_\gamma\leqslant s_{\tilde\gamma}$. Но то же самое суждение можно проделать, поменяв $\gamma$ и $\tilde\gamma$ ролями.
        \end{Proof}
        \thm Аналогично, длины противоположных путей равны (только $\theta_k$ будут расположены по убыванию, но и пофиг).
        \dfn \undercolor{red}{Длина кривой} --- длина любой/какой-нибудь её параметризации.
        \thm Длины пути аддитивна.
        \begin{Proof}
            Что надо доказать? Если $\gamma[a;b]\to\mathbb R^m$ --- путь, $a<c<b$, $\gamma^1=\gamma\big|_{[a;c]}$, $\gamma^2=\gamma\big|_{[c;b]}$, то $s_\gamma=s_{\gamma^1}+s_{\gamma^2}$.\\
            Придётся немного повозиться, доказав два нестрогих неравенства. Пусть $\tau_1$, $\tau_2$ --- дробления $[a;c]$ и $[c;b]$. Тогда $\tau=\tau_1\cup\tau_2$ --- дробление $[a;b]$. Очевидно, $l_{\tau_1}+l_{\tau_2}=l_\tau\leqslant s_\gamma$. Возьмём супремум по $\tau_1$ и потом по $\tau_2$, получим что хотели.\\
            Теперь рассмотрим $\tau$ --- дробление $[a;b]$. Если $c\in\tau$, то всё очевидно, как выше, но в другую сторону. В противном случае возьмём $\tau'=\tau\cup\{c\}$. Чем отличаются эти разбиения? Если $c\in[t_\nu;t_{\nu+1}]$, то в $l_\tau$ есть слагаемое $|\gamma(t_{\nu+1})-\gamma(t_\nu)|$, а в $l_{\tau'}$ --- $|\gamma(t_{\nu+1})-\gamma(c)|+|\gamma(c)-\gamma(t_\nu)|$. Но $|\gamma(t_{\nu+1})-\gamma(c)|+|\gamma(c)-\gamma(t_\nu)|\geqslant|\gamma(t_{\nu+1})-\gamma(t_\nu)|$ в силу неравенства треугольника, а значит $l_\tau\leqslant l_{\tau'}\leqslant s_{\tau_1}+s_{\tau_2}$. Перейдём к супремуму.
        \end{Proof}
        \begin{Comment}
            В определении длины и леммах о длине непрерывность не использовалась
        \end{Comment}
        \thm \undercolor{darkgreen}{Длина гладкого пути}. Если $\gamma$ --- 1-гладкий путь на $[a;b]$, то он спрямляем и $s_\gamma=\int_a^b|\gamma'|$, где $\gamma'=(\gamma_1';\ldots;\gamma_m')$, а значит $|\gamma'|=\sqrt{\sum\limits_{k=1}^n{\gamma'_i}^2}$.
        \begin{Comment}
            Распространяется на кусочно-гладкий случай по аддитивности.
        \end{Comment}
        \begin{Proof}
            Двумерный случай: $\gamma=(\varphi;\psi)$. $|\gamma|=\sqrt{\varphi^2+\psi^2}$.
            Доказательство потом.
        \end{Proof}
        \thm \undercolor{darkgreen}{Длина графика}. Пусть $f\in C^{(1)}[a;b]$. Тогда $\Gamma_f$ спрямляем и $s_{\Gamma_f}=\int_a^b\sqrt{1+{f'}^2}$, где $\Gamma_f=(t;f(t))\mid t\in[a;b]$.
        \thm \undercolor{darkgreen}{Длина графика в полярных координатах}. Пусть $f\in C^{(1)}[\alpha;\beta]$, $f\geqslant0$. Тогда $s_\gamma=\int_\alpha^\beta\sqrt{f^2+{f'}^2}$.
        \begin{Proof}
            $x=r\cos\phi=f(\phi)\cos\phi=\varphi(\phi)$, $y=r\sin\phi=f(\phi)\sin\phi=\psi(\phi)$. Тогда ${\varphi'}^2(\phi)+{\psi'}^2(\phi)=(f'(\phi)\cos\phi-f(\phi)\sin(\phi))^2+(f'(\phi)\sin\phi-f(\phi)\cos(\phi))^2=f^2(\phi)+{f'}^2(\phi)$.
        \end{Proof}
        \begin{Example}
            Выразим через интеграл длину дуги эллипса. Для определённости $0<a<b$. Тогда $x=a\cos t$, $y=b\sin t$, $t\in[0;\beta]$. Тогда $\frac{\mathrm dx}{\mathrm dt}=-a\sin t$, $\frac{\mathrm dy}{\mathrm dt}=b\cos t$. Тогда $s(\beta)=\int_0^\beta\sqrt{a^2\sin^2t+b^2\cos^2t}~\mathrm dt=\int_0^\beta\sqrt{b^2-(b^2-a^2)\sin^2t}~\mathrm dt=b\int_0^\beta\sqrt{1-\varepsilon^2\sin^2t}~\mathrm dt$, где $\varepsilon=\frac{\sqrt{b^2-a^2}}b$ --- эксцентриситет. А это, извините, эллиптический интеграл второго рода, он не берётся
        \end{Example}
        \thm Докажем наконец \undercolor{darkgreen}{длину гладкого пути}. Если $\gamma$ --- 1-гладкий путь на $[a;b]$, то он спрямляем и $s_\gamma=\int_a^b|\gamma'|$, где $\gamma'=(\gamma_1';\ldots;\gamma_m')$, а значит $|\gamma'|=\sqrt{\sum\limits_{k=1}^n{\gamma'_i}^2}$.
        \begin{Proof}
            Сначала получим некую оценку длины сверху и снизу (тем доказав её конечность, а значит и спрямляемость). И делать мы это будем на подотрезках.\\
            Рассмотрим $\Delta=[\alpha;\beta]\subset[a;b]$. Рассмотрим $\eta=\{u_k\}_{k=0}^n$ --- дробление $\Delta$. Тогда $l_\eta=\sum\limits_{k=0}^{n-1}|\gamma(u_k)-\gamma(u_{k+1})|=\sum\limits_{k=0}^{n-1}\sqrt{\sum\limits_{i=1}^m(\gamma(u_k)-\gamma(u_{k+1})^2)}$. Сюда хочется применить формулу Лагранжа. $\exists c_{ik}\in(u_k;u_{k+1})~\sum\limits_{k=0}^{n-1}\sqrt{\sum\limits_{i=1}^m{\gamma'_i}^2(c_{ik})}\Delta u_k$. Тогда $M_\Delta^i=\max\limits_\Delta|\gamma'_i|$, $M_\Delta^i=\min\limits_\Delta|\gamma'_i|$. Первыми ми оцениваем всерху, вторыми --- снизу. Введём ещё обозначение $M_\Delta=\sqrt{\sum\limits_{i=1}^m{M_\Delta^i}^2}$, $m_\Delta=\sqrt{\sum\limits_{i=1}^m{m_\Delta^i}^2}$. Теперь мы можем написать, что$m_\Delta(\beta-\alpha)\leqslant l_\eta\leqslant M_\Delta(\beta-\alpha)$. Это для любой ломаной. Если мы перейдём к супремуму и инфимуму по всем ломаным, то получим оценку длины пути, а не ломаной. То есть $m_\Delta(\beta-\alpha)\leqslant s_{\gamma|_{\Delta}}\leqslant M_\Delta(\beta-\alpha)$. В частности в качестве $\Delta$ можно взять весь $[a;b]$ и правое неравенство даст спрямляемость $\gamma$.\\
            Теперь давайте запустим эту же технику для всего большого отрезка. Возьмём $\tau=\{t_k\}_{k=0}^n$ --- дробление $[a;b]$. Тогда по лемме об аддитивности пути $s_\gamma=\sum\limits_{k=0}^{n-1}s_{\gamma\big|_{[t_k;t_{k+1}]}}$. На каждом кусочке применим первую часть доказательства. Пусть $M_{[t_k;t_{k+1}]}=M_k$, $m_{[t_k;t_{k+1}]}=m_k$. Тогда $\sum\limits_{k=0}^{n-1}m_k\Delta t_k\leqslant s_\gamma\leqslant\sum\limits_{k=0}^{n-1}M_k\Delta t_k$. А для интеграла можно получить ту же самую оценку, он же тоже аддитивен по промежутку. $\int_a^b|\gamma'|=\sum\limits_{k=0}^{n-1}\int_{t_k}^{t_{k+1}}|\gamma'|$, а $\sum\limits_{k=0}^{n-1}m_k\Delta t_k\leqslant\sum\limits_{k=0}^{n-1}\int_{t_k}^{t_{k+1}}|\gamma'|\leqslant\sum\limits_{k=0}^{n-1}M_k\Delta t_k$. Но на этом закончить не удастся, потому что суммы слева и справа не обязаны быть верхними и нижними суммами Дарбу, потому что максимум суммы может не совпадать с суммой максимумов. Но получится так, что они отличаются от интегральных сумм на бесконечно малую. И тогда мы уже сможем заключить, что между верхними и нижними суммами находится одно число.\\
            Рассмотрим разность $\sum\limits_{k=0}^{n-1}M_k\Delta t_k-\sum\limits_{k=0}^{n-1}m_k\Delta t_k=\sum\limits_{k=0}^{n-1}(M_k-m_k)\Delta t_k$. Докажем, что эту сумму можно сделать бесконечно малой. Для этого придётся оценить разности $M_k-m_k$. Оценим её на отрезке $\Delta$. $M_k-m_k=\frac{M_k^2-m_k^2}{M_k+m_k}=\sum\limits_i^m\frac{M_\Delta^i+m_\Delta^i}{M_\Delta+m_\Delta}(M_\Delta^i-m\Delta^i)\leqslant\sum\limits_{i=1}^m(M_\Delta^i-m\Delta^i)$. Все производные $\gamma_i$ --- непрерывные функции, модули их тоже, а по теореме Кантора $|\gamma'_i|$ равномерно непрерывны на $[a;b]$. То есть для $\varepsilon>0$ мы можем подобрать $\delta>0$ что $\exists\overline t,\overline{\overline t}\in[a;b]:|\overline t-\overline{\overline t}|<\delta~||\gamma'_i(\overline t)|-|\gamma'_i(\overline{\overline t})||<\frac{\varepsilon}{m(b-a)}$. И теперь если взять ранг дробления $\lambda_\tau$ меньше $\delta$, то все соседние точки дробления будут отличаться на $\delta$, а значит $\sum\limits_{k=0}^{n-1}(M_k-m_k)\Delta t_k\leqslant\sum\limits_{k=0}^{n-1}\frac\varepsilon{m(b-a)}\cdot m\Delta t_k=\varepsilon$. Итого. Мы доказали, что верхние и нижние оценки в неравенствах могут отличаться бесконечно мало, между ними может быть одно число. А у нас там два --- интеграл и длина, значит это одно и то же.
        \end{Proof}
    \end{itemize}
    \subparagraph{\undercolorblack{orange}{Вычисление статических моментов и центра тяжести}.}
    \begin{itemize}
        \dfn Если $(x_0;y_0)$ --- материальная точка с массой $m$ на плоскости. Её \undercolor{red}{статические моменты} относительно $Ox$ и $Oy$ --- это $M_x=my_0$ и $M_y=mx_0$.
        \dfn Если $(x_k;y_k)$ --- $N$ материальная точка с массива соответственно $m_k$, то $M_x=\sum\limits_{k=1}^Nm_ky_k$, $M_y=\sum\limits_{k=1}^Nm_kx_k$.
        \dfn Пусть $\gamma\in C^{(1)}$ --- путь на $[a;b]$, $\gamma=(\varphi;\psi)$, масса $\gamma$ равна $m$, кривая однородна (масса пропорциональна длине), причём единица измерения выбрана так, что коэффициент пропорциональности --- 1 (то есть масса участка равна длине). Введём длину как длину с переменным правым концом: $s(t)=s_{\gamma\big|_{[a;t]}}$ для $t\in[a;b]$. Возьмём $\{t_k\}_{k=0}^n$ оснащённое точками $\{\theta_k\}$. Тогда \undercolor{red}{моменты кривой} относительно $Ox$ и $Oy$ --- $M_x=\lim\limits_{\lambda\to0}\sum\limits_{k=0}^{n-1}\psi(\theta_k)\Delta s_k$, $M_y=\lim\limits_{\lambda\to0}\sum\limits_{k=0}^{n-1}\varphi(\theta_k)\Delta s_k$, где $s_k=s_{\gamma\big|_{[t_k;t_{k+1}]}}=\int_a^b|\gamma'|$.
        \thm $M_x=\int_a^b\psi\sqrt{{\varphi'}^2+{\psi'}^2}$
        \begin{Proof}
            $\sum\limits_{k=0}^{n-1}\psi(\theta_k)\Delta s_k$. По теореме о среднем $\exists \widetilde\theta_k\in[t_k;t_{k+1}]$, что это равно $\sum\limits_{k=0}^{n-1}\psi(\theta_k)|\gamma'(\widetilde\theta_k)|\Delta t_k=\sum\limits_{k=0}^{n-1}\psi(\widetilde\theta_k)|\gamma'(\widetilde\theta_k)|\Delta t_k-\sum\limits_{k=0}^{n-1}(\psi(\theta_k)-\psi(\widetilde\theta_k))|\gamma'(\widetilde\theta_k)|\Delta t_k$. Первое --- эо интегральная сумма, она стремится к $\int_a^b\psi\sqrt{{\varphi'}^2+{\psi'}^2}$. Второе --- возьмём $\varepsilon$, подберём дробление, в котором разности $\psi(\theta_k)-\psi(\widetilde\theta_k)$ будут меньше $\varepsilon$, делённого на длину, а значит сумма буем меньше $\varepsilon$, что значит что она стремится к нулю.
        \end{Proof}
        \begin{Comment}
            Отсюда легко найти центр тяжести $(x_0;y_0)$ такой что $s_\gamma y_0=M_x$, $s_\gamma x_0=M_y$. Они будут равны $x_0=\frac1{s_\gamma}\int_a^b\varphi\sqrt{{\varphi'}^2+{\psi'}^2}$ и $y_0=\frac1{s_\gamma}\int_a^b\psi\sqrt{{\varphi'}^2+{\psi'}^2}$.
        \end{Comment}
        \begin{Example}
            Посчитаем центр тяжести полуокружности. $x_0=0$, что тривиально (мы же физики). Надо посчитать $y_0$. Для этого скажем, что $(x;y)=(\cos t;\sin t)$. Тогда $y_0=\frac1\pi\int_0^\pi\sin t~\mathrm dt=\frac2\pi$.
        \end{Example}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Функции ограниченной вариации}.}
    \begin{itemize}
        \dfn Пусть $f\colon[a;b]\to\mathbb R$. \undercolor{red}{Вариацией функции} $f$ называется $\Var abf=\sup\limits_\tau\sum\limits_{k=0}^{n-1}|f(x_{k+1})-f(x_k)|$.
        \begin{Comment}
            Почему это не длина пути? Потому что никто не говорит, что $f$ непрерывно, а значит необязательно путь.
        \end{Comment}
        \dfn \undercolor{red}{Множество функций ограниченной вариации} на $[a;b]$ обозначается $V[a;b]=\{f\colon[a;b]\to\mathbb R\mid\Var abf<+\infty\}$.
        \thm \undercolor{darkgreen}{Свойства вариации}.
        \begin{enumerate}
            \item $\Var abf=\Var acf+\Var cbf$
            \item Если $f$ кусочно-непрерывная, то $f\in V[a;b]$ и $\Var abf=\int_a^b|f|$.
            \item $[\alpha;\beta]\subset[a;b]$, $\Var abf\leqslant\Var\alpha\beta f$.
            \begin{Proof}
                \[
                \Var abf=\Var a\alpha f+\Var\alpha\beta f+\Var\beta bf\geqslant\Var\alpha\beta f
                \]
            \end{Proof}
            \item $\gamma=(\gamma_1;\ldots;\gamma_m)$ --- путь в $\mathbb R^m$, $\gamma\colon[a;b]\to\mathbb R^m$. Тогда $\gamma$ спрямляем тогда и только тогда, когда $\forall i\in[i:m]~\gamma_i\in V[a;b]$.
            \begin{Proof}
                \[
                |\gamma_i(t_{k+1})-\gamma_i(t_k)|\leqslant|\gamma_i(t_{k+1})-\gamma_i(t_k)|=\sqrt{\sum\limits_{i=1}^m(\gamma_i(t_{k+1})-\gamma_i(t_k))^2}\leqslant\sum\limits_{i=1}^m|\gamma_i(t_{k+1})-\gamma_i(t_k)|
                \]
                Правая часть не превосходит длины пути $\gamma$. Значит и супремум не превосходит, а значит и вариация --- тоже.\\
                Пусть все ограничены вариацией. Правая часть не больше суммы вариаций, значит длины всех ломаных ограничены, как и длина пути.
            \end{Proof}
            \item Если $f$ монотонна на $[a;b]$, то $f\in V[a;b]$ и $\Var abf=|f(b)-f(a)|$.
            \begin{Proof}
                $\sum\limits_{k=0}^{n-1}|f(x_{k+1})-f(x_k)|$Поскольку все слагаемые одного знака, $\left|\sum\limits_{k=0}^{n-1}f(x_{k+1})-f(x_k)\right|$. А это телескопическая сумма: $\left|\sum\limits_{k=0}^{n-1}f(x_{k+1})-f(x_k)\right|=|f(x_n)-f(x_0)|=|f(b)-f(a)|$.
            \end{Proof}
            \item Если $f\in V[a;b]$, то $f$ ограничена.
            \begin{Proof}
                Возьмём $x\in[a;b]$ и оценим чем-нибудь $|f(x)|$.\\
                $|f(x)|\leqslant|f(a)|+|f(x)-f(a)|+|f(b)-f(x)|$. Это дробление из трёх точек, а значит второе и третье слагаемые в сумме не больше вариации. А первое --- константа.
            \end{Proof}
        \end{enumerate}
        \dfn Свойство 3 позволяет определить \undercolor{red}{вариацию на промежутке произвольного типа}. $f\colon\ab\to\mathbb R$, $\Var ab=\sup\limits_{[\alpha;\beta]\subset\ab}\Var\alpha\beta f$.
        \thm Из четвёртого очевидно следует, что $f\in V[a;b]\Leftrightarrow\Gamma_f$ спрямляем.
        \thm \undercolor{darkgreen}{Арифметические действия над функциями с ограниченной вариацией}. Если $f,g\in V[a;b]$, то $f+g$, $f-g$, $fg$, $\alpha f$ и $|f|$ также $\in V[a;b]$. Если к тому же $\inf\limits_{[a;b]}g>0$, то $\frac fg\in V[a;b]$.
        \begin{Proof}
            $\Delta_k(f)=f(x_{k+1})-f(x_k)$. Тогда\\
            $|\Delta_k(f+g)|\leqslant|\Delta_k(f)|+|\Delta_k(g)|$, чему мы приписываем сумму $\sum\limits_{k=0}^{n-1}$ и получаем $\sum\limits_{k=0}^{n-1}|\Delta_k(f+g)|\leqslant\sum\limits_{k=0}^{n-1}|\Delta_k(f)|+\sum\limits_{k=0}^{n-1}|\Delta_k(g)|\leqslant \Var abf+\Var abg$. Взяв супремум, получим что хотели.\\
            Если $|f|<K$, $|g|<L$, то $|\Delta_k(fg)|\leqslant L|\Delta_k(f)|+K|\Delta_k(g)|$. Это можно найти в действиях с интегрируемыми функциями. Дальше как с суммой.\\
            $\Var ab(\alpha f)=|\alpha|\Var ab f$.\\
            $f-g$ --- смотри $\alpha f$ и $f+g$.\\
            $|\Delta_k(|f|)|\leqslant |\Delta_k(f)|$.\\
            Для частного достаточно доказать про $\frac1g$. $|\Delta_k(\frac1g)|=\left|\frac1{g(x_{k+1})}-\frac1{g(x_k)}\right|=\left|\frac{g(x_k)-g(x_{k+1})}{|g(x_k)||g(x_{k+1})|}\right|\leqslant\frac{|\Delta_k(g)|}{m^2}$
        \end{Proof}
        \thm Функция $f$ имеет на отрезке $[a;b]$ ограниченную вариацию тогда и только тогда, когда существуют $g,h$ --- возрастающие на $[a;b]$ такие что $f=g-h$.
        \begin{Proof}
            Следствие влево очевидно по свойству 5 и арифметическим действиям.\\
            Для следствия вправо предъявим эти функции. Пусть $g(x)=\Var axf$, $h(x)=f(x)-g(x)$. То что $g$ возрастает, очевидно из монотонности вариации. Проверим возрастаемость $h$. Рассмотрим $x_1,x_2\in[a;b]$, $x_1<x_2$. Тогда $h(x_2)-h(x_1)=g(x_2)-g(x_1)-(f(x_2)-f(x_1))=\Var{x_1}{x_2}f-(f(x_2)-f(x_1))\geqslant\Var{x_1}{x_2}-|f(x_2)-f(x_1)|\geqslant0$.
        \end{Proof}
        \thm Ещё \undercolor{darkgreen}{свойства функции ограниченной вариацией}.
        \begin{enumerate}
            \addtocounter{enumi}{6}
            \item Функция с ограниченной вариацией не может иметь разрывов 2 рода.
            \item $V[a;b]\subset R[a;b]$.
            \item $f\mapsto\Var abf$ --- полунорма на $V[a;b]$.
            \begin{Proof}
                Из доказательства теоремы 1 $\Var ab(f+g)\leqslant\Var abf+\Var abg$ и $\Var ab\alpha f=|\alpha|\Var abf$.
            \end{Proof}
        \end{enumerate}
        \begin{Comment}
            Чтобы из полунормы получить норму придётся отождествить все функции с нулевой нормой, и тогда получится норма на классах эквивалентности. Это называется факторизацией.\\
            А можно сделать и по-другому. У нас проблемы с функциями-константами, поэтому можно зафиксировать значение в некоторой точке $a$.
        \end{Comment}
        \thm Ни один из классов $V[a;b]$ и $C[a;b]$ не содержится в другом.
        \begin{Proof}
            $V[a;b]\not\subset C[a;b]$ мы уже доказали, ведь любая монотонная функция имеет ограниченную вариацию, но уж точно не любая непрерывна.\\
            А в обратную сторону вот: $f(0)=0$, $f(x)=x\cos\frac\pi x$ для $x\in(0;1]$.
            \begin{center}
                \begin{tikzpicture}
                    \begin{axis}[
                        axis equal image,
                        trig format plots = rad,
                        grid = none,
                        xmin = -0.1,
                        xmax = 1.1,
                        ymin = -1.1,
                        ymax = 1.1,
                        axis x line = middle,
                        axis y line = middle,
                        axis line style = {->},
                        ]
                        \addplot[domain=0.01:1, samples=100, color=red] {x * cos(pi / x)};
                        \addplot[dashed, domain=0.01:1, samples=2, color=red] {x};
                        \addplot[dashed, domain=0.01:1, samples=2, color=red] {-x};
                    \end{axis}
                \end{tikzpicture}
            \end{center}
            Она, тривиально, непрерывна. Докажем, что её вариация не ограничена. Пусть $n\in\mathbb N$. И пусть $x_k=\frac1k$. Тогда $f(x_k)=\frac{(-1)^k}k$. Возьмём $\sum\limits_{k=1}^{n-1}|f(x_k)-f(x_{k+1})|+|f(x_n)-f(0)|$. Тогда в сумме слагаемые противоположных знаков, а значит модуль можно раскрыть так:
            \[
            \sum\limits_{k=1}^{n-1}\left(\frac1k+\frac1{k+1}\right)+\frac1n=1+2\sum\limits_{k=1}^{n-1}\frac1k
            \]
            При $n\to\infty$ эта штука стремится к бесконечности. Тема про ряды будет следующей, но тем не менее это мы докажем сейчас. Пусть $H_n=\sum\limits_{k=1}^n\frac1k$. И докажем, что $H_n$ ничем не ограничено сверху. Как докажем? Рассмотрим $H_{2^m}=\underbracket{1}+\underbracket{\frac12}+\underbracket{\frac13+\frac14}+\underbracket{\frac15+\frac16+\frac17+\frac18}+\cdots$ и сгруппируем так, как написано, то есть группируем $\frac1{2^{m-1}+1}+\cdots+\frac1{2^m}$. Заметим, что тут $2^{m-1}$ слагаемое, каждое не меньше $\frac1{2^m}$. А значит сумма каждой группы равна $\frac12$. То есть $H_{2^m}\geqslant1+\frac m2$. Справедливо, это не ограничено сверху. А это значит, что $f\notin V[0;1]$. А $\Gamma_f$ --- неспрямляемый путь в $\mathbb R^2$.
        \end{Proof}
    \end{itemize}
    \section{Числовые ряды.}
    \paragraph{\undercolorblack{orange}{Простейшие свойства рядов}.}
    \begin{itemize}
        \dfn Пусть $\{a_k\}_{k=1}^\infty$ --- вещественная или комплексная последовательность. Символ $\sum\limits_{k=1}^\infty a_k$ называется \undercolor{red}{числовым рядом}. Числа $S_n=\sum\limits_{k=1}^na_k$ называются \undercolor{red}{частичными} или \undercolor{red}{частными суммами ряда}. Если существует конечный предел $\lim\limits_{n\to\infty}S_n=S$, то говорят, что \undercolor{red}{ряд} $\sum\limits_{k=1}^\infty a_k$ \undercolor{red}{сходится}, а $S$ называют \undercolor{red}{суммой ряда}. В противном случае говорят, что \undercolor{red}{ряд расходится}. Если же $S=\pm\infty$ или $S=\infty$, то его значение всё равно называется суммой ряда.
        \begin{Comment}
            Если $\{a_k\}_{k=m}^\infty$, то также можно рассматривать ряд. При этом взгляды на нумерацию $S_k$ разнятся. Мы будем считать, что $S_k$ также начинают нумерацию с $m$.
        \end{Comment}
        \begin{Comment}
            На самом деле можно провести обратный переход, от последовательностей к рядам. Если нам дана последовательность $S_k$, то можно записать $a_k=S_k-S_{k-1}$ ($S_0=0$), вот и вместо последовательности мы получили ряд.
        \end{Comment}
        \begin{Example}
            $\sum\limits_{k=1}^\infty0=0$
        \end{Example}
        \begin{Example}
            $\sum\limits_{k=1}^\infty1=+\infty$
        \end{Example}
        \begin{Example}
            $\sum\limits_{k=1}^\infty(-1)^k$ не существует, потому что $S_n=\begin{cases}
                0 & n\divby 2\\
                1 & n\not\divby 2
            \end{cases}$.
        \end{Example}
        \begin{Comment}
            Ряды появились гораздо раньше пределов. Поэтому когда определение появилось, захотелось его расширить. Например, указанный ряд был равен одной второй. И выкидывать этот интересный ряд на свалку не хотелось, так что хотелось обобщение. Мы своими глазами увидим необходимость этого расширения, когда будем работать с рядами Фурье.
        \end{Comment}
        \begin{Example}
            $\sum\limits_{k=1}^nz^k$. При $z=-1$ мы получаем предыдущий пример, при $z=1$ --- пред-предыдущий. При $z\neq1$ $S_n=\frac{1-z^{n+1}}{1-z}$. Чему равен предел? Если $|z|<1$, то $\sum\limits_{k=1}^\infty z^k=\frac1{1-z}$. Если $|z|>1$, то $\sum\limits_{k=1}^\infty z^k=\infty$ (в вещественном случае при $z>1$ можно уточнить, что это $+\infty$). А когда $|z|=1$, при $z=1$ сумма ряда $+\infty$, мы изучили, а при $z\neq1$ предела никакого нет в принципе.
        \end{Example}
        \begin{Example}
            \begin{itemize}
                \item $\sum\limits_{k=0}^\infty\frac{x^k}{k!}=e^x$
                \item $\sum\limits_{k=0}^\infty\frac{(-1)^k}{(2k)!}x^{2k}=\cos x$
                \item $\sum\limits_{k=0}^\infty\frac{(-1)^k}{(2k+1)!}x^{2k+1}=\sin x$
            \end{itemize}
        \end{Example}
        \begin{Example}
            $\sum\limits_{k=0}^\infty\frac1{k(k+1)}=1$. ($S_n=1-\frac1{n+1}$.)
        \end{Example}
        \begin{Example}
            $\sum\limits_{k=1}^\infty\frac1k$ расходится к $+\infty$.
        \end{Example}
        \thm Простейшие свойства рядов.
        \begin{enumerate}
            \item Если $m\in\mathbb N$ и $\sum\limits_{k=1}^\infty a_k$ сходится, то $\sum\limits_{k=m+1}^\infty a_k$ и $\sum\limits_{k=m+1}^\infty a_k=\sum\limits_{k=m+1}^\infty a_k+\sum\limits_{k=1}^m a_k$. Верно и обратное (если существует ряд начиная с $m$, то существует и с единицы).
            \begin{Proof}
                Возьмём $n>m$ и запишем равенство для частичных сумм: $\sum\limits_{k=1}^na_k=\sum\limits_{k=1}^ma_k+\sum\limits_{k=m+1}^na_k$. После чего перейдём к пределу
            \end{Proof}
        \end{enumerate}
        \dfn $\sum\limits_{k=m+1}^\infty a_k$ называется \undercolor{red}{остатком ряда} $\sum\limits_{k=1}^\infty a_k$ с $m$-того члена.
        \begin{enumerate}
            \addtocounter{enumi}{1}
            \item Остаток сходящегося ряда бесконечно мал при $m\to\infty$.
            \begin{Proof}
                $\sum\limits_{k=m+1}^\infty a_k=\sum\limits_{k=1}^\infty a_k-\sum\limits_{k=1}^m a_k\underset{m\to\infty}\rightarrow S-S=0$
            \end{Proof}
            \item Линейность суммирования.
            Если $\sum\limits_{k=1}^\infty a_k$ и $\sum\limits_{k=1}^\infty b_k$ сходятся, а $\alpha,\beta\in\mathbb R$, то $\sum\limits_{k=1}^\infty(\alpha a_k+\beta b_k)$ сходится к $\alpha\sum\limits_{k=1}^\infty a_k+\beta\sum\limits_{k=1}^\infty b_k$.
            \begin{Comment}
                Если $a_k$ сходится, а $b_k$ расходится, то $a_k+b_k$ расходится. Доказывается от противного.
            \end{Comment}
            \item Если $z_k=x_k+\im y_k$, то $\sum\limits_{k=1}^\infty z_k$ сходится тогда и только тогда, когда $\sum\limits_{k=1}^\infty x_k$ и $\sum\limits_{k=1}^\infty y_k$ сходятся.
            \item Если $a_k\leqslant b_k$ и $\sum\limits_{k=1}^\infty a_k$ и $\sum\limits_{k=1}^\infty b_k$ имеют предел в $\overline{\mathbb R}$, то первый меньше второго.
            \begin{Comment}
                Верны неравенства Йенсена, Гёльдера, Минковского.
            \end{Comment}
        \end{enumerate}
        \thm \undercolor{darkgreen}{Необходимое условие сходимости ряда}. Если $\sum\limits_{k=1}^\infty a_k$ сходится, то $\lim\limits_{n\to\infty}a_n=0$.
        \begin{Proof}
            $a_n=S_{n}-S_{n-1}\rightarrow S-S=0$.
        \end{Proof}
        \begin{Comment}
            Довольно редко удаётся найти сумму ряда. Намного чаще --- узнать сходимость. В частности если последовательность не стремится к нулю, то её ряд уж точно расходится.
        \end{Comment}
        \begin{Comment}
            Обратное к этой теореме не верно. Найдите ранее сами контрпример.
        \end{Comment}
        \begin{Comment}
            Интегралы (по бесконечному промежутку) и ряды являются близкими родственниками. У них и определения схожие, и свойства, и комплексные свойства тоже были бы, если бы мы интегрировали комплексные функции. Но вот свойство выше даёт нам явственное различие.
        \end{Comment}
        \thm \undercolor{darkgreen}{Критерий Больцано-Коши для рядов}. $\sum\limits_{k=0}^\infty$ сходится тогда и только тогда, когда $\exists\varepsilon>0~\exists N>0~\forall n>N~\forall p\in\mathbb N~\left|\sum\limits_{k=n+1}^{n+p} a_k\right|<\varepsilon$.
        \begin{Proof}
            Ну, блин, у нас в определении сходимости ряда $\forall n,m>N~|S_m-S_n|<\varepsilon$. Ну так не умаляя общности пусть $m>n$, тогда $p=m-n\in\mathbb N$.
        \end{Proof}
        \begin{Comment}
            Несложно заметить, что предыдущая теорема частный случай критерия Больцано-Коши.
        \end{Comment}
        \begin{Comment}
            Критерий Больцано-Коши обычно используется для определения расходимости ряда. Его переформулируют как $\sum\limits_{k=n}^m\underset{n,m\to\infty}\rightarrow0$ и пытаются подобрать $m(n)$, чтобы не стремилось.
        \end{Comment}
        \begin{Example}
            Для гармонического ряда можно взять $n=2m$, тогда данная сумма будет больше $\frac12$.
        \end{Example}
        \begin{Comment}
            Хорошо, чем-то ряды похожи на интегралы, а чем-то другим, очевидно, на конечные суммы. И какие-то свойства (линейность и монотонность) мы распространили на ряды. Но ещё в суммах можно группировать и переставлять члены местами. А можно ли в бесконечных? Ну, группировать нельзя, потому что в ряду $\sum\limits_{k=1}^\infty(-1)^k$ можно сгруппировать по парам и получить 0, а можно первый член оставить без пары, а остальные сгруппировать, получив 1. Можно сказать, что так всё плохо, потому что ряд расходится. И это правда, группировка в сходящемся ряду ничего не испортит. В отличие от перестановки.
        \end{Comment}
        \dfn Пусть $\sum\limits_{k=1}^\infty a_k$ --- некоторый ряд. Пусть $\{n_j\}_{j=0}^\infty$ строго возрастающая последовательность, $n_j\in\mathbb Z_+$, $n_0=0$. Пусть $A_j=\sum\limits_{k=n_j+1}^{n_{j+1}}a_k$. Тогда говорят, что $\sum\limits_{j=0}^\infty A_j$ \undercolor{red}{получен} из $\sum\limits_{k=1}^\infty a_k$ \undercolor{red}{группировкой членов}.
        \thm
        \begin{enumerate}
            \item Если ряд $\sum\limits_{k=1}^\infty$ имеет сумму (возможно, бесконечную), то $\sum\limits_{j=0}^\infty A_j$ имеет ут же сумму.
            \item Если члены в каждой <<группе>> одного знака (хотя бы в нестрогом смысле), и после группировки ряд имеет сумму, то и исходный ряд имеет такую же сумму.
            \item Если $a_n\rightarrow 0$, а количество членов в каждой группе ограничено одним и тем же числом $L$ (т.е. $\forall j\in\mathbb Z_+~n_{j+1}-n_j\leqslant L$), то также из того, что ряд после группировки имеет сумму, следует что и исходный ряд имеет такую же сумму.
        \end{enumerate}
        \begin{Proof}
            Докажем только первое утверждение, оставшиеся есть в учебнике.\\
            Что такое частичная сумма $\sum\limits_{j=0}^\infty A_j$? Ну, $T_m=\sum\limits_{j=0}^m A_j=\sum\limits_{k=1}^{n_{m+1}}=S_{n_{m+1}}$. Ну, а если последовательность куда-то стремится, то подпоследовательность стремится туда же.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Положительные ряды}.}
    \begin{itemize}
        \dfn Ряд называется \undercolor{red}{положительным}, когда все его члены неотрицательны.
        \thm Положительный ряд $\sum\limits_{k=1}^\infty a_k$ сходится $\Leftrightarrow\{S_n\}$ ограничена сверху.
        \begin{Proof}
            Ну, $\{S_n\}\uparrow$, а значит $\{S_n\}$ имеет предел тогда и только тогда, когда ограничена сверху.
        \end{Proof}
        \begin{Comment}
            Возрастающая последовательность или сходится, или расходится к $+\infty$. В обоих случаях её предел равен супремуму. А значит $\sum\limits_{k=1}^\infty=\sum\limits_{n\in\mathbb N}S_n\in[0;+\infty]$.
        \end{Comment}
        \begin{Comment}
            Для сходимости достаточно ограниченности сверху какой-нибудь подпоследовательности ($\{S_{n_j}\}$).
        \end{Comment}
        \thm \undercolor{darkgreen}{Признак сравнения сходимости положительных рядов}. Пусть $\{a_k\}$, $\{b_k\}$ --- неотрицательные последовательности. Пусть $a_k=O(b_k)$ при $k\to\infty$. Тогда
        \begin{enumerate}
            \item Если ряд $\sum\limits_{k=1}^\infty b_k$ сходится, то $\sum\limits_{k=1}^\infty a_k$ тоже сходится.
            \item Если $\sum\limits_{k=1}^\infty a_k$ расходится, то ряд $\sum\limits_{k=1}^\infty b_k$ также расходится.
        \end{enumerate}
        \begin{Proof}
            Ясно, что из первого следует второе от противного, а значит надо доказать только первое.\\
            Запишем, что значит $a_k=O(b_k)$. Это значит что $\exists K>0,M\in\mathbb N~\forall k>M~a_k\leqslant Kb_n$. Тогда $\sum\limits_{k=M+1}^\infty a_k\leqslant K\sum\limits_{k=M+1}^\infty b_k$. Если $\sum\limits_{k=1}^\infty b_k$ сходится, то его остаток $\sum\limits_{k=M+1}^\infty b_k$ также сходится, а значит и остаток $\sum\limits_{k=M+1}^\infty a_k$ сходится, а значит и оригинальный ряд $\sum\limits_{k=1}^\infty a_k$ --- тоже.
        \end{Proof}
        \thm \undercolor{darkgreen}{Признак сравнения в предельной форме}. Пусть $\{a_k\}$ --- неотрицательная последовательность, $\{b_k\}$ --- строго положительная последовательность. Пусть $\exists\lim\limits_{n\to\infty}\frac{a_k}{b_k}=l\in[0;+\infty]$. Тогда
        \begin{enumerate}
            \item Если $l<+\infty$ и ряд $\sum\limits_{k=1}^\infty b_k$ сходится, то ряд $\sum\limits_{k=1}^\infty a_k$ также сходится.
            \item Если $l>0$ и ряд $\sum\limits_{k=1}^\infty a_k$ сходится, то ряд $\sum\limits_{k=1}^\infty b_k$ также сходится.
            \item Если $0<l<+\infty$, то ряды $\sum\limits_{k=1}^\infty a_k$ и $\sum\limits_{k=1}^\infty b_k$ сходятся или расходятся одновременно.
        \end{enumerate}
        \begin{Proof}
            Поймём, что эти утверждения следуют из признака сравнения.
            \begin{enumerate}
                \item Напишем определение предела: $\exists M~\forall k>M~\frac{a_k}{b_k}<l+1$. Это можно переписать как $a_l\leqslant(l+1)b_k$, в качестве константы $K$ можно взять $l+1$.
                \item Меняем $a_k$ и $b_k$ ролями.
                \item Следствие предыдущих двух.
            \end{enumerate}
        \end{Proof}
        \thm Положительные ряды с эквивалентными общими членами сходятся или расходятся одновременно.
        \begin{Proof}
            Соответствует значению $l=1$.
        \end{Proof}
        \begin{Example}
            Мы знаем, что гармонический ряд расходится. Из этого очевидно следует, что если $\alpha<1$, то $\sum\limits_{k=1}^\infty \frac1{k^\alpha}$ расходится. Почему? Ну, потому что $\frac1{k^\alpha}\geqslant\frac1k$, а $\sum\limits_{k=1}^\infty \frac1k$ расходится. Расходится к $+\infty$, а куда же ещё.
        \end{Example}
        \pagebreak
        \begin{Example}
            Можем доказать, что обратные квадраты сходятся. $\frac1{k^2}\equiv\frac1{k(k+1)}$, а $\sum\limits_{k=1}^\infty \frac1{k(k+1)}$ сходится (к единице).
        \end{Example}
        \begin{Example}
            А раз ряд обратных квадратов сходится, то и $\forall\alpha>2~\sum\limits_{k=1}^\infty \frac1{k^\alpha}$ сходится, потому что $\frac1{k^\alpha}\leqslant\frac1{k^2}$.
        \end{Example}
        \begin{Comment}
            Оставшийся случай $\alpha\in(1;2)$ нами ещё не исследуем, а когда исследуем, то там (как для интегралов) будет сходимость.
        \end{Comment}
        \begin{Comment}
            К чему сходятся эти ряды --- сложный вопрос. Эйлер сначала узнал, чему равна сумма ряда обратных квадратов ($\frac{\pi^2}6$), потом посчитал для всех чётных положительных $\alpha$. Мы этим тоже займёмся, но это будет очень не скоро, потому что понадобится дополнительная техника.
        \end{Comment}
        \begin{Comment}
            Следующие два признака (Коши и Д'Адамбера) --- замаскированные признаки сравнения с геометрической прогрессией. Доказательство это прояснит.
        \end{Comment}
        \thm \undercolor{darkgreen}{Радикальный признак Коши}. Пусть все члены ряда $\sum\limits_{k=1}^\infty a_k$ неотрицательны. Пусть $K=\overline{\lim\limits_{n\to\infty}}\sqrt[n]{a_n}$. Тогда
        \begin{enumerate}
            \item Если $K>1$, то ряд $\sum\limits_{k=1}^\infty a_k$ расходится.
            \item Если $K<1$, то ряд $\sum\limits_{k=1}^\infty a_k$ сходится.
        \end{enumerate}
        \begin{Comment}
            В формулировке тут верхний предел. Опыт показывает, что студенты в этот момент забывают, что такое верхний предел. $\overline{\lim\limits_{n\to\infty}}=\lim\limits_{n\to\infty}\sup\limits_{k\geqslant n}x_k$ или равносильное определение: наибольший из частичных пределов.
        \end{Comment}
        \begin{Comment}
            Чаще всего теорема применяется, когда есть обычный предел. Но вообще верхний предел есть у любой вещественной последовательности, а обычный --- не у любой. Это первая полезность верхнего предела тут. А потом в степенных рядах будет признак Коши --- Адамара, где будет очень полезно, что тут верхний предел.
        \end{Comment}
        \begin{Comment}
            При $K=1$ возможны оба варианта. Примеры мы уже знаем --- для гармонического ряда и ряда обратных квадратов $K=1$.
        \end{Comment}
        \begin{Proof}
            \begin{enumerate}
                \item $K>1$. В таком случае $\forall N~\exists n>N~\sqrt[n]{a_n}>1$. Другими словами --- бесконечно много членов последовательности $\sqrt[n]{a_n}$ (а значит и самих $a_n$) больше единицы. Если бы это было не так, то частичный предел больше единицы не получился бы. А отсюда $a_n\nrightarrow0$, и ряд расходится.
                \item $K<1$. Обозначим $\varepsilon=\frac{1-K}2>0$. $q=\frac{1+K}2\in(0;1)$. Тогда $\exists N~\forall k>N~\sqrt[k]{a_k}<K+\varepsilon$. Почему так? Если бы было не так, бесконечно много членов последовательности были бы больше $K+\varepsilon$, а значит был бы частичный предел равный $K+\varepsilon$. А такого нет, ведь $K$ --- наибольший частичный предел. Итого $\sqrt[k]{a_k}<q\Leftrightarrow a_k<q^k$. Но $\sum\limits_{k=N+1}^\infty q^k$ сходится (посмотрите, откуда $q$), а значит и $\sum\limits_{k=N+1}^\infty a_k$ сходится. Как и оригинальный ряд $\sum\limits_{k=1}^\infty a_k$.    
            \end{enumerate}
        \end{Proof}
        \thm \undercolor{darkgreen}{Признак Д'Аламбера}. Пусть все члены ряда $\sum\limits_{k=1}^\infty a_k$ положительный и $\exists\lim\limits_{n\to\infty}\frac{a_{n+1}}{a_n}=D$. Тогда
        \begin{enumerate}
            \item Если $D>1$, то ряд $\sum\limits_{k=1}^\infty a_k$ расходится.
            \item Если $D<1$, то ряд $\sum\limits_{k=1}^\infty a_k$ сходится.
        \end{enumerate}
        \begin{Comment}
            Как можно заметить из тех же двух рядов (гармонического и обратных квадратов), для $D=1$ признак ответа не даёт.
        \end{Comment}
        \begin{Proof}
            \begin{enumerate}
                \item Поскольку $D>1$, $\exists N~\forall n>N~\frac{a_{n+1}}{a_n}>1$. То есть $\exists N~\forall n>N~a_{n+1}>a_n$. А значит $\exists N~\forall n>N~a_{n+1}>a_{N+1}$. Тогда $a_n\nrightarrow0$.
                \item $D<1$. Запишем определение предела на $\varepsilon$-языке, а за $\varepsilon$ выберем $\frac{1-D}2>0$. А $q=\frac{1+D}2\in(0;1)$. Итак, $\exists N~\forall k>N!\frac{a_{k+1}}{a_k}<D+\varepsilon=q$. Итак, $a_{k+1}<qa_k$. А значит $\forall k>N+1~a_k<qa_{k-1}<q^2a_{k-2}<\cdots<q^{k-N-1}a_{N+1}=\frac{a_{N+1}}{q^{N+1}}q^k$. Ряд $\sum\limits_{k=N+1}^\infty q^k$ сходится, если домножить его на константу $\frac{a_{N+1}}{q^{N+1}}$, ничего не поменяется, а значит и $\sum\limits_{k=N+1}^\infty a_k$ сходится.
            \end{enumerate}
        \end{Proof}
        \thm Признак Коши сильнее признака Д'Аламбера (он обслуживает больше рядов). То есть если $\exists\lim\limits_{n\to\infty}\frac{a_{n+1}}{a_n}=D$, то $\exists\lim\limits_{n\to\infty}\sqrt[n]{a_n}=D$.
        \begin{Comment}
            То есть если мы к чему-то можем применить признак Д'Аламбера, то признак Коши тоже можем. Это не всегда бывает удобно, но всё же.
        \end{Comment}
        \begin{Proof}
            Доказательства не будет.
        \end{Proof}
        \begin{Comment}
            Нам рассказывали вот такое утверждение: если $a_n>0$ и $\exists\lim\limits_{n\to\infty}\frac{a_{n+1}}{a_n}<1$, то $a_n\rightarrow0$. По сути своей оно является признаков Д'Аламбера для последовательностей. А признак для рядов по факту является усилением этого утверждения.
        \end{Comment}
        \begin{Example}
            $\sum\limits_{k=1}^\infty \frac{a^k}{k!}$, например, можно легко исследовать на сходимость по признаку Д'Аламбера.
        \end{Example}
        \begin{Comment}
            Нам уже говорили, что интегралы и ряды --- близкие родственники. И некоторые свойства были общими. А некоторые --- не общими, как тот же признак Д'Аламбера. А вот сейчас будет нечто другое по формулировке.
        \end{Comment}
        \thm Пусть $f\colon[1;+\infty)\to\mathbb R$ и \textbf{монотонна}. Тогда ряд $\sum\limits_{k=1}^\infty f(k)$ и $\int\limits_1^{\to+\infty}f$ сходятся или расходятся одновременно.
        \begin{Proof}
            Функция монотонна, а значит не умаляя общности считать, что она неотрицательна. Почему так можно считать? Начиная с некоторого места она не меняет знак, а если этот знак не тот, который нужно --- рассмотрим $-f$. А аргумент про <<начиная с некоторого места>> работает, потому что и про ряды, и про интегралы мы знаем, что они сходятся и расходятся одноимённо со своими остатками.\\
            Ещё можно считать, что $f$ убывает. В противном случае (и если функция не тождественный ноль), то и ряд, и интеграл расходятся.\\
            Будем рассматривать функцию между соседними целыми точками. Если $k\in\mathbb N$, $x\in[k;k+1]$, то $f(k+1)\leqslant f(x)\leqslant f(k)$. Теперь мы это неравенство проинтегрируем на $[k;k+1]$: $f(k+1)\leqslant\int_k^{k+1} f(x)\leqslant f(k)$. Теперь возьмём натуральное число $N$ и просуммируем это от 1 до $N$: $\sum\limits_{k=2}^{N+1}f(k)\leqslant\int_1^{k+1}f(x)\leqslant\sum\limits_{k=1}^Nf(k)$.\\
            Дальше легко. Если ряд сходится, то $\forall n>N~\int_1^{n+1}f\leqslant\sum\limits_{k=1}^Nf(k)=S$, а тогда по лемме для интегралов неотрицательной функции $\int_1^{+\infty}\leqslant S$.\\
            Обратно: если интеграл сходится к $I$, то $\sum\limits_{k=2}^{N-1}f(k)\leqslant I$, а значит и $\sum\limits_{k=2}^\infty f(k)\leqslant I$
        \end{Proof}
        \begin{Example}
            $\sum\limits_{k=1}^\infty \frac1{k^\alpha}$ сходится тогда и только тогда, когда $\alpha>1$. Ну, мы имели то же самое для рядов.
        \end{Example}
        \begin{Example}
            $\sum\limits_{k=1}^\infty \frac1{k^\alpha\ln^\beta k}$ сходится тогда и только тогда, когда $\left[\begin{aligned}
                \alpha>1\\
                \alpha=1\land\beta>1
            \end{aligned}\right.$. Надо только проверить монотонность, она, к сожалению, выполняется не везде, а начиная с определённого момента, что в принципе не важно.
        \end{Example}
        \begin{Comment}
            Вообще, эту теорему можно применять и обратно. Но на самом деле, контринтуитивно, интеграл проще суммы. Явно сумму ряда (или даже конечную сумму) найти удаётся довольно редко. А для интеграла есть формула Ньютона --- Лейбница, которая может помочь довольно часто. Для интеграла есть замена переменной и интегрирование по частям. А вот что такое замена переменной для рядов... Дискретный вариант интегрирования по частям, что интересно, есть (и называется преобразованием Абеля), но он всё же сложнее и менее универсален, чем интегрирование по частям.
        \end{Comment}
        \begin{Comment}
            Обозначим через $A_n$ такую величину: $\sum\limits_{k=1}^nf(k)-\int_1^{n+1}f$. Что-то мы тут про него доказали. Во-первых, $A_n\uparrow$, потому что $A_{n+1}-A_n=f(n+1)-\int_{n+1}^{n+2}f\overset{f\downarrow}\geqslant0$.\\
            Также $0\leqslant A_n\leqslant f(1)$. Отсюда $\exists\lim\limits_{n\to\infty}A_n=c$. Отсюда $\sum\limits_{k=1}^Nf(k)=\int_1^{n+1}f+c+\varepsilon_n$, где $\varepsilon_n\to0$.\\
            Есть тут одна неприятность --- мы почти никогда не умеем находить $c$. Тем не менее она бывает полезна.\\
            При этом если есть сходимость, то $c$ --- это просто разность суммы $\sum\limits_{k=1}^\infty f(k)$ и интеграла $\int_1^{+\infty}f$. А если интеграл и сумма оба расходятся, то мы получаем во-первых, их эквивалентность, а во-вторых то что их разность сходится. При этом $\int_1^{n+1}f$ мы довольно часто умеем считать, а значит имеем асимптотику роста $\sum\limits_{k=1}^N f(k)$
        \end{Comment}
        \begin{Example}
            $H_n=\sum\limits_{k=1}^n \frac1k$. Тогда $H_n=\ln(n+1)+C_{\text{Э}}+\varepsilon_n$. $C_{\text{Э}}$ --- постоянная Эйлера, причём существует она только одна в силу существования только одного асимптотического разложения. Также преобразуем $\ln(n+1)+C_{\text{Э}}+\varepsilon_n=\ln n+C_{\text{Э}}+\delta_n$, где $\delta_n\to0$. Численное значение $C_{\text{Э}}$ примерно равно $0.577\ldots$. Пока что $H_n\sim\ln n$.\\
            Теперь докажем, что $0<\delta_n<\frac1{2n}$. Для начала скажем, что $\ln(n+1)=\sum\limits_{k=1}^n(\ln(k+1)-\ln k)=\sum\limits_{k=1}^n\ln\left(1+\frac1k\right)$. А значит отняв от гармонической суммы этот логарифм, получим $C_{\text{Э}}=\sum\limits_{k=1}^\infty\left(\frac1k-\ln\left(1+\frac1k\right)\right)$. Итак,
            \[
            \begin{split}
                \delta_n=H_n-\ln n-C_{\text{Э}}=\sum\limits_{k=1}^N\frac1k-\sum\limits_{k=1}^{n-1}\ln\left(1+\frac1k\right)-\sum\limits_{k=1}^\infty\left(\frac1k-\ln\left(1+\frac1k\right)\right)=\\
                =\frac1n+\sum\limits_{k=n}^\infty\left(\frac1k-\ln\left(1+\frac1k\right)\right)
            \end{split}
            \]
            Заметим, что $\ln(1+x)<x$ при $x>-1$, $x\neq0$. Тогда
            $$\ln\left(1+\frac1k\right)=-\ln\frac{k}{k+1}=-\ln\left(1-\frac1{k+1}\right)\overset{x=-\frac1{k+1}}>-\left(-\frac1{k+1}\right)=\frac1{k+1}$$
            Это даёт нам оценку погрешности снизу: $$\delta_n>\frac1n+\sum\limits_{k=n}^\infty\left(\frac1{k+1}-\frac1k\right)=\frac1n-\frac1n=0$$
            Теперь докажем, что $\ln\left(1+\frac1k\right)<\frac12\left(\frac1k+\frac1{k+1}\right)$. Это мы докажем при помощи производной. Разве что обозначим $u=1+\frac1k\geqslant1$ (тогда $k=\frac1{u-1}$). То есть мы хотим доказать, что $\ln u<\frac12\left(u-1+\frac{u-1}u\right)=\frac12\left(u-\frac1u\right)$. При $u=1$ имеем равенство. Берём производную: $\frac1u<\frac12\left(1+\frac1{u^2}\right)$. Ну, это верно, а значит исходное равенство выполняется. Итого
            $$\delta_n<\frac1n+\frac12\sum\limits_{k=n}^\infty\left(\frac1{k+1}-\frac1k\right)=\frac1{2n}$$
        \end{Example}
        \begin{Example}
            Теперь давайте возьмём $\alpha\in(0;1)$ и $f(x)=\frac1{x^\alpha}$. Тогда $\int_1^{n+1}\frac{\mathrm dx}{x^\alpha}=\frac{x^{1-\alpha}}{1-\alpha}\Big|_1^{n+1}=\frac{(n+1)^{1-\alpha}-1}{1-\alpha}$. Для суммы явной формулы нет, но $\sum\limits_{k=1}^n\frac1{k^\alpha}=\frac{(n+1)^{1-\alpha}-1}{1-\alpha}+c_\alpha+\varepsilon_n=\frac{(n+1)^{1-\alpha}}{1-\alpha}+d_\alpha+\varepsilon_n$. Итого
            $$
            \sum\limits_{k=1}^\infty\frac1{k^\alpha}\sim\frac{n^\alpha}{1-\alpha}
            $$
        \end{Example}
        \begin{Comment}
            $\int_{n+1}^{+\infty}f\leqslant\sum\limits_{k=n+1}^\infty f(k)\leqslant\int_n^{+\infty}f$. Тут мы получаем информацию о скорости стремления остатка к нулю.
        \end{Comment}
        \begin{Example}
            $\alpha>1$, $f(x)=\frac1{x^\alpha}$. Тогда
            $$
            \frac1{(\alpha-1)(n+1)^{\alpha-1}}\leqslant\sum\limits_{k=n+1}^\infty\frac1{k^\alpha}\leqslant\frac1{(\alpha-1)n^{\alpha-1}}
            $$
            Левая и правая часть эквивалентны друг другу, а значит и средняя эквивалентна им же. И вот мы уже знаем, что $\sum\limits_{k=n+1}^\infty\frac1{k^\alpha}\sim\frac1{(\alpha-1)n^{\alpha-1}}$
        \end{Example}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Ряды с произвольными членами}.}
    \begin{itemize}
        \dfn Если ряд $\sum\limits_{k=1}^\infty |a_k|$ сходится, то говорят, что $\sum\limits_{k=1}^\infty a_k$ \undercolor{red}{абсолютно сходится}.
        \thm Множество абсолютно сходящихся рядов линейно. (То если линейная комбинация абсолютно сходящихся рядов абсолютно сходится.)
        \begin{Proof}
            Очевидно из того, что $|\alpha a_k+\beta b_k|\leqslant|\alpha||a_k|+|\beta||b_k|$.
        \end{Proof}
        \thm Если $a_k=x_k+\im y_k$, то абсолютная сходимость $\sum\limits_{k=1}^\infty a_k$ равносильна абсолютной сходимости $\sum\limits_{k=1}^\infty x_k$ и $\sum\limits_{k=1}^\infty y_k$.
        \begin{Proof}
            В одну сторону --- из предыдущего, в другую --- $|x_k|\leqslant|a_k|$ и $|y_k|\leqslant|a_k|$.
        \end{Proof}
        \thm Если $\exists\sum\limits_{k=1}^\infty a_k\in\overline{\mathbb R}$, то $\left|\sum\limits_{k=1}^\infty a_k\right|\leqslant\sum\limits_{k=1}^\infty|a_k|$.
        \begin{Proof}
            Пишем для частичных сумм, переходим к пределу.
        \end{Proof}
        \thm Если ряд абсолютно сходится, он сходится.
        \begin{Proof}
            Сначала пусть $a_k\in\mathbb R$. Тогда $a_k=(a_k)_++(a_k)_-$. Понятно, что $0\leqslant(a_k)_\pm\leqslant|a_k|$, а значит $\sum\limits_{k=1}^\infty (a_k)_\pm$ сходятся (мы его правда применяем к положительным рядам). А значит и $\sum\limits_{k=1}^\infty a_k$ сходится как разность сходящихся.\\
            Теперь пусть $a_k\in\mathbb C$. Делаем то же самое, что вместо $(a_k)_\pm$ рассматриваем $\Re a_k$ и $\Im a_k$. Как мы знаем, они оба абсолютно сходятся, а значит (они вещественные) просто сходятся, а значит и $a_k$ сходится.
        \end{Proof}
        \begin{Comment}
            Обратное неверно, пример чуть позже.
        \end{Comment}
        \dfn Если ряд сходится, но не абсолютно сходится, то он \undercolor{red}{условно сходится} или \undercolor{red}{неабсолютно сходится}.
        \thm Сумма абсолютно и условно сходящихся рядов условно сходится.
        \begin{Proof}
            Тривиально.
        \end{Proof}
        \thm \undercolor{darkgreen}{Радикальный признак Коши абсолютной сходимости рядов}.
        Пусть $K=\overline{\lim\limits_{n\to\infty}}\sqrt[n]{|a_n|}$. Тогда
        \begin{itemize}
            \item Если $K>1$, то $\sum\limits_{k=1}^\infty a_k$ расходится.
            \item Если $K<1$, то $\sum\limits_{k=1}^\infty a_k$ абсолютно сходится
        \end{itemize}
        \thm \undercolor{red}{Признак Д'Аламбера абсолютной сходимости рядов}. Пусть $\forall k~a_k\neq0$ и $\exists\lim\limits_{n\to\infty}\left|\frac{a_{k+1}}{a_k}\right|=D$. Тогда
        \begin{enumerate}
            \item Если $D>1$, то $\sum\limits_{k=1}^\infty a_k$ расходится.
            \item Если $D<1$, то $\sum\limits_{k=1}^\infty a_k$ абсолютно сходится.
        \end{enumerate}
        \begin{Proof}
            Применяем соответствующий признак к ряду модулей. Если $D/K<1$ --- ясно. А вот $D/K>1$ не так понятно. Но на самом деле мы в доказательствах признаков мы доказали, что $a_n\nrightarrow0$. А утверждения $a_n\nrightarrow0$ и $|a_n|\nrightarrow0$ равносильны.
        \end{Proof}
        \thm \undercolor{darkgreen}{Признаки Дирихле и Абеля сходимости рядов}. Пусть $a_k\in\mathbb R$ либо $\mathbb C$, а $b_k\in\mathbb R$. И пусть $\{b_k\}$ монотонна. Тогда
        \begin{itemize}
            \item[\undercolor{darkgreen}{Дирихле}] Если $\left\{\sum\limits_{k=1}^N a_k\right\}_{N=1}^\infty$ ограничены, а $b_n\to\mathbb0$, то $a_kb_k$ сходится.
            \item[\undercolor{darkgreen}{Абель}] Если $a_k$ сходится, а $\{b_n\}$ ограничена, то $a_kb_k$ сходится.
        \end{itemize}
        \begin{Proof}
            Доказательство будет потом. Сильно потом. Потому что мы докажем эти признаки для равномерной сходимости функциональных рядов, то есть в более общем виде.
        \end{Proof}
        \dfn Ряд $\sum\limits_{k=1}^\infty (-1)^{k-1}b_k$, где $\forall k~b_k\geqslant0$ или $\forall k~b_k\leqslant0$ называется \undercolor{red}{знакочередующимся}.
        \thm \undercolor{red}{Признак Лейбница}. Если $\{b_n\}$ монотонна и стремится к нулю, то знакочередующийся ряд $\sum\limits_{k=1}^\infty (-1)^{k-1}b_k$ сходится.
        \begin{Proof}
            Это, конечно, частный случай признака Дирихле, но мы всё равно его докажем в лоб двусторонними оценками. И даже извлечём отсюда некоторую информацию.\\
            Рассмотрим частичные суммы этого ряда. И сначала рассмотрим частичную сумму чётного числа слагаемых. $S_{2m}=b_1-b_2+b_3-b_4+\cdots+b_{2m-1}-b_{2m}$. Не умаляя общности, $\{b_n\}\downarrow$ и все её члены неотрицательны.\\
            Очевидно, $S_{2m}\geqslant0$. Также очевидно, что $\{S_{2m}\}\uparrow$. Хочется ограничить последовательность сверху. Для этого сгруппируем члены так: $S_{2m}=b_1\underbrace{-b_2+b_3}\underbrace{-b_4+b_5}-\cdots\underbrace{-b_{2m-2}+b_{2m-1}}-b_{2m}$. И тут уже тривиально, что $S_{2m}\leqslant b_1$. Значит $S_{2m}$ имеет предел.\\
            Теперь рассмотрим $S_{2m+1}$. Тут ничего делать не придётся, ведь $S_{2m+1}=S_{2m}+b_{2m+1}$, а $b_{2m+1}\to0$. А значит $S_{2m+1}$ и $S_{2m}$ сходятся к одному числу. В таком случае и $S_{n}$ сходится.
        \end{Proof}
        \begin{Comment}
            Ряды из условия признака часто называют лейбницевыми.
        \end{Comment}
        \begin{Comment}
            Так вот мы выяснили, что сумма лейбницевого ряда совпадает по знаку (нестрого) с первым членом и не превосходит его по абсолютной величине.
        \end{Comment}
        \begin{Comment}
            Остаток лейбницевого ряда также является лейбницевым. Что даёт нам право оценить остаток лейбницевого ряда --- он совпадает по знаку и не превосходит по модулю первого члена остатка.
        \end{Comment}
        \begin{Example}
            Вот тут у нас уже легко получаются примеры условно сходящихся рядов. Например, для $\alpha\in(0;1]$ $\sum\limits_{k=1}^\infty\frac{(-1)^{k-1}}{k^\alpha}$ условно сходится.
        \end{Example}
        \begin{Example}
            Сумму ряда $\sum\limits_{k=1}^\infty\frac{(-1)^{k-1}}k$ можно явно найти. Рассмотрим сумму чётного числа слагаемых $S_{2m}=1-\frac12+\frac13-\frac14+\cdots+\frac1{2m-1}-\frac1{2m}$. Давайте добавим и отнимем все нечётные слагаемые с плюсом. Мы получим
            $$
            H_{2m}-2\left(\frac12+\frac14+\frac16+\cdots+\frac1{2m}\right)=H_{2m}-H_m=\ln(2m)+C_{\text{Э}}+\delta_{2m}-\ln m-C_{\text{Э}}-\delta_m\rightarrow\ln 2
            $$.
        \end{Example}
        \begin{Comment}
            Заметим, что $\ln(1+x)=\sum\limits_{k=1}^\infty\frac{(-1)^{k-1}}kx^k+o(x^n)$. Если подставить сюда $x=1$ и устремить $n$ к $\infty$, то получится очень правдоподобно, но мы всё же не знаем, устремится ли $o(x^n)$ к нулю или нет. На самом деле да, но $x=1$ --- граница области сходимости, поэтому надо аккуратно. Точно и математично мы сейчас этим заниматься не будем.
        \end{Comment}
        \begin{Example}
            А давайте возьмём ту же самую последовательность, но переставим члены местами:
            $$
            1-\frac12-\frac14+\frac13-\frac16-\frac18+\frac15-\frac1{10}-\cdots
            $$
            Пусть $S_n$ --- суммы непереставленного ряда, а $T_n$ --- текущего. Так вот рассмотрим $T_{2m}$. Тогда отрицательных слагаемых будет вдвое больше, чем положительных. $T_{3m}=\sum\limits_{k=1}^m\left(\frac1{2k-1}-\frac1{4k-2}-\frac1{4k}\right)=\frac12\sum\limits_{k=1}^m\left(\frac1{2k-1}-\frac1{2k}\right)$. Это же наш старый знакомый, $\frac12S_{2m}$, а значит $T_{3m}$ стремится к $\frac12\ln 2$.\\
            Остаётся $T_{3m+1}=T_{3m}+\frac1{2m+1}\rightarrow\frac12\ln 2+0=\frac12\ln 2$ и $T_{3m+2}=T_{3m+1}-\frac1{4m+2}\rightarrow\frac12\ln 2+0=\frac12\ln 2$.\\
            Мы видим, что перестановка изменила сумму ряда. Занятно, не правда ли. Группировать в сходящемся ряде можно, а вот переставлять элементы, как видно, нельзя. Произошло это потому, что ряд сходится условно, в абсолютно сходящемся ряде такого не бывает. И мы это докажем. Но сначала определим, что такое перестановка.
        \end{Example}
        \dfn \undercolor{red}{Перестановкой натурального ряда} называется биекция $\varphi\colon\mathbb N\to\mathbb N$. Говорят, что ряд $\sum\limits_{k=1}^\infty a_{\varphi(k)}$ \undercolor{red}{получен перестановкой} из ряда $\sum\limits_{k=1}^\infty a_k$.
        \thm \undercolor{darkgreen}{Перестановка членов абсолютно сходящегося ряда}. Пусть $\sum\limits_{k=1}^\infty a_k$ абсолютно сходится, а $\varphi$ --- некая перестановка натурального ряда. Тогда $\sum\limits_{k=1}^\infty a_{\varphi(k)}$ абсолютно сходится к той же сумме (то есть $\sum\limits_{k=1}^\infty a_{\varphi(k)}=\sum\limits_{k=1}^\infty a_k$, а не $\sum\limits_{k=1}^\infty|a_{\varphi(k)}|=\sum\limits_{k=1}^\infty|a_k|$, хотя это тоже).
        \begin{Proof}
            Докажем сначала для положительных рядов. В таком случае оба ряда имеют сумму. Пусть $S_n$ --- частичные суммы исходного ряда, $T_n$ --- частичные суммы переставленного ряда. Пусть $m=\max\{\varphi(1);\ldots;\varphi(n)\}$. Тогда $T_n\leqslant S_m\leqslant S=\sum\limits_{k=1}^\infty a_k$. А значит частичные суммы $T_n$ также ограничены сверху числом $S$, и $\sum\limits_{k=1}^\infty a_{\varphi(k)}=T\leqslant S$. А ещё можно рассмотреть $\varphi^{-1}$, и получить $S\leqslant T$, откуда $T=S$.\\
            Теперь докажем теорему для $a_k\in\mathbb R$. Тогда вернёмся к нашим друзьям $(a_k)_+$ и $(a_k)_-$. Раз $\sum\limits_{k=1}^\infty a_k$ сходится абсолютно, $\sum\limits_{k=1}^\infty (a_k)_\pm$ сходятся. Тогда $\sum\limits_{k=1}^\infty (a_{\varphi(k)})_\pm$ сходятся к тем же суммам. А в таком случае $\sum\limits_{k=1}^\infty a_{\varphi(k)}$ сходится к тому же числу, что и $\sum\limits_{k=1}^\infty a_k$, причём даже абсолютно.\\
            А теперь $a_k\in\mathbb C$. Абсолютно аналогично предыдущему случаю с рассмотрением $\Re a_k$ и $\Im a_k$.
        \end{Proof}
        \thm Перестановка расходящегося положительного ряда приводит к расходящемуся ряду.
        \thm Если $\sum\limits_{k=1}^\infty a_k$ условно сходится, то \textbf{оба} $\sum\limits_{k=1}^\infty (a_k)_+$ и $\sum\limits_{k=1}^\infty (a_k)_-$ расходятся.
        \thm \undercolor{darkgreen}{Теорема Римана}. Если $a_k\in\mathbb R$ и $\sum\limits_{k=1}^\infty a_k$ сходится условно, то $\forall S\in\overline{\mathbb R}$ существует перестановка, после которой ряд имеет сумму $S$. Также существует перестановка, после которой ряд суммы не имеет.
        \begin{Comment}
            Доказательство на экзамене не требуется, потому что тут аккуратно рассказано не будет. Тут будет только идея. Аккуратное доказательство можно найти в книжке.
        \end{Comment}
        \begin{Proof}
            Давайте рассмотрим отдельно положительные, отдельно отрицательные члены ряда. Нули неважно где, пусть среди положительных членов. Теперь давайте наберём неотрицательных членов столько, чтобы сумма была больше $S$. Так можно сделать потому что $\sum\limits_{k=1}^\infty (a_k)_+$ расходится. Дальше начнём брать отрицательные члены, и опять же будем брать их столько, чтобы сумма была меньше $S$. И так продолжим. Разумеется, мы получим какую-то перестановку. Почему полученный ряд будет сходится к $S$? Ну, давайте посмотрим, настолько от $S$ будут отличаться частичные суммы? Пусть на первом шаге мы взяли $a_{n_1}$. Тогда поскольку до этого сумма была меньше $S$, $0<S_{n_k}-S<a_{n_1}$ и аналогично с отрицательными (только там модуль). При этом $a_{n_k}\to0$, поскольку $\sum\limits_{n=1}^\infty a_n$ всё же сходится. А значит и $S_{n_k}\to S$. Остаётся доказать, что вся последовательность $S_n\to S$, а не только её подпоследовательность $S_{n_k}$. Ну, $|S_n-S|\leqslant\{|a_{n_k};a_{n_{k+1}}|\}$.\\
            С $\pm\infty$ можно делать похоже --- берём положительные до единицы, один отрицательный --- потом положительные до двух, один отрицательный. И так далее.\\
            Чтобы ряд расходится, можно делать то же самое, что и для $S\in\mathbb R$, но для положительных и отрицательных групп брать разные $S$.
        \end{Proof}
        \thm Если $a_k\in\mathbb C$, $\sum\limits_{k=1}^\infty a_k$ сходится условно, то существует его перестановка, после которой ряд будет расходится.
        \begin{Comment}
            Понятно, что аналогичное для $\mathbb R$ утверждение неверно, если все члены вещественные, очень сложно заставить его сходится к $\im$.
        \end{Comment}
        \begin{Comment}
            Что делать с произведением рядов? Как мы в вещественном случае это делали? Вот так:
            $$
            \left(\sum\limits_{i=1}^n a_i\right)\left(\sum\limits_{j=1}^m b_j\right)=\sum\limits_{i=1}^n\sum\limits_{j=1}^ma_ib_j
            $$
            То есть по сути мы имели своеобразную матрицу $m\times n$, в которой попарные произведения.\\
            А тут у нас появится в обе стороны бесконечная матрица, которую мы хотим сложить. А как мы уже знаем, от порядка их сложения может зависеть результат. И пока у нас нет причин предпочесть один способ нумерации другому, дадим такое определение:
        \end{Comment}
        \dfn Пусть $\sum\limits_{k=1}^\infty a_k$ и $\sum\limits_{j=1}^\infty b_j$ --- числовые ряды, $\gamma=(\phi;\psi)\colon\mathbb N\to\mathbb N^2$ --- биекция (т.е. нумерация бесконечной матрицы). Тогда ряд $\sum\limits_{l=1}^\infty a_{\phi(l)}b_{\psi(l)}$ называется \undercolor{red}{произведением рядов} $\sum\limits_{k=1}^\infty a_k$ и $\sum\limits_{j=1}^\infty b_j$
        \thm \undercolor{darkgreen}{Теорема О. Коши}. Пусть имеются два ряда: $\sum\limits_{k=1}^\infty a_k$ и $\sum\limits_{j=1}^\infty b_j$, сходящихся (абсолютно) к $A$ и $B$ соответственно. Тогда любое их произведение абсолютно сходится к $AB$.
        \begin{Proof}
            Сначала проверим абсолютную сходимость. Пусть $\sum\limits_{k=1}^\infty |a_k|=A^*$, $\sum\limits_{j=1}^\infty |b_j|=B^*$. Докажем, что $\sum\limits_{l=1}^\infty|a_{\phi(l)}b_{\psi(l)}|$ сходится. Для этого достаточно доказать ограниченность частичных сумм. Ну, очевидно.
            $$
            \sum\limits_{l=1}^n|a_{\phi(l)}b_{\psi(l)}|\leqslant\left(\sum\limits_{k=1}^N|a_k|\right)\left(\sum\limits_{j=1}^M|b_j|\right)
            $$
            Где $N=\max\{\phi(1);\phi(2);\ldots;\phi(n)\}$, $M=\max\{\psi(1);\psi(2);\ldots;\psi(n)\}$. А значит $\sum\limits_{l=1}^\infty|a_{\phi(l)}b_{\psi(l)}|$ ограничены сверху числом $A^*B^*$.\\
            Теперь докажем, что произведения сходятся к нужному числу. Ну, раз ряд абсолютно сходится, мы можем переставить его элементы как хотим, а значит выбрать одно удобное нам $\gamma$, и доказать для него. <<Удобное>> в данном случае --- <<по квадратам>>. $S_{n^2}=\left(\sum\limits_{k=1}^na_k\right)\left(\sum\limits_{j=1}^nb_j\right)$ То есть тут не только конкретное $\gamma$, но её и группировка, которая, как мы знаем, также законна. А при такой нумерации сумма считается элементарно, и действительно равна $AB$.
        \end{Proof}
        \thm Для последнего шага абсолютная сходимость не нужна. Для любых сходящихся рядов произведение <<по квадратам>> даст нам $AB$.
        \begin{Proof}
            Единственное, что тут нужно понять, почему ничего не испортится, если мы возьмём другое $S_n$ (не для $n=m^2$). Ну, берём $m=\lceil\sqrt{n}\rceil$. Тогда $S_n=S_{m^2}+\Delta$. А в $\Delta$ вошло несколько подряд идущих слагаемых внешнего квадрата по вертикали и по горизонтали. То есть $|\Delta|\leqslant|b_{m+1}|\left|S^a_{N_2}-S^a_{N_1}\right|+|a_{m+1}|\left|S^b_{M_2}-S^b_{M_1}\right|$. Первое --- вертикаль (одно $b_{m+1}$ на какую-то сумму $a$), второе --- горизонталь. И важно тут, что разности $S^a_{N_2}-S^a_{N_1}$ ограничены, а $a_{m+1}$ и $b_{m+1}$ стремятся к нулю, значит $\Delta$ бесконечно мало.
        \end{Proof}
        \dfn Ряд $\sum\limits_{k=1}^\infty c_k$, где $s_k=\sum\limits_{j=1}^ka_jb_{k+1-j}$ называется \undercolor{red}{произведением рядов} $\sum\limits_{k=1}^\infty a_k$ и $\sum\limits_{j=1}^\infty b_j$ \undercolor{red}{по Коши}.
        \begin{Comment}
            То есть $c_k$ --- сумма элементов на $k$-той диагонали.
        \end{Comment}
        \begin{Comment}
            Этот способ важнее, чем <<по квадратам>>. Почему? Потому что это связано с произведением многочленов и степенных рядов. Когда мы умножаем $(a_0+a_1x+a_2x^2+\cdots)(b_0+b_1x+b_2x^2+\cdots)$, коэффициент при $x^k$ будет равен $c_k$. Сами степенные ряды мы будем изучать отдельно и потом.
        \end{Comment}
        \thm Тривиальное следствие предыдущей теоремы: если $\sum\limits_{k=1}^\infty a_k$ и $\sum\limits_{j=1}^\infty b_j$ абсолютно сходятся к $A$ и $B$ соответственно, то их произведение по Коши сходится к $AB$.
        \begin{Example}
            Пример ситуации, когда ряды условно сходятся, а их произведение по Коши расходится:
            $a_k=b_k=\frac{(-1)^{k-1}}{\sqrt k}$. Тогда $c_k=\sum\limits_{j=1}^k \frac{(-1)^{j-1}}{\sqrt j}\frac{(-1)^{k-j}}{\sqrt{k+1-j}}=(-1)^{k-1}\sum\limits_{j=1}^k\frac1{(\sqrt j)(\sqrt{k+1-j})}$. Оценим $|c_k|$ снизу. $|c_k|=\sum\limits_{j=1}^k\frac1{\sqrt{j(k+1-j)}}$. Заметим, что $j(k+1-j)\leqslant\left(\frac{k+1}2\right)^2$, а значит $|c_k|\geqslant k\frac2{k+1}\nrightarrow0$.
        \end{Example}
        \begin{Comment}
            Без доказательства запишем следующие два утверждения:
            \begin{enumerate}
                \item Для сходимости произведения по Коши достаточно потребовать абсолютной сходимости только одного ряда (и сходимости другого).
                \item Если ряды $\sum\limits_{k=1}^\infty a_k$ и $\sum\limits_{j=1}^\infty b_j$ сходятся и их произведение по Коши сходится, то сходится куда надо (к $AB$)
            \end{enumerate}
        \end{Comment}
        \begin{Example}
            Произведение по Коши двух расходящихся рядов может быть сходящимся. Тут оба ряда будут нумероваться с нуля.
            $$a_k=\begin{cases}
                1 & k=0\\
                2^{n-1} & k\in\mathbb N
            \end{cases}\qquad\qquad b_j=\begin{cases}
                1 & j=0\\
                -1 & j\in\mathbb N
            \end{cases}$$
            Тогда $c_k=\sum\limits_{j=0}^k a_jb_{k-j}=1(-1)+\sum\limits_{j=1}^{k-1}2^{j-1}(-1)+2^{k-1}=-(1+1+2+4+\cdots+2^{k-2})+2^{k-1}=0$. Это, очевидно, сходится.
        \end{Example}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Суммируемые семейства}.}
    \begin{itemize}
        \dfn \undercolor{red}{Числовое семейство} --- это функция $a\in T\to\mathbb R$ либо $a\in T\to\mathbb C$, $T$ --- произвольное множество. Только вместо $a(t)$ пишется $a_t$.
        \begin{Comment}
            Хочется просуммировать все значения семейства $a$. У нас в качестве семейства были последовательности. То есть в качестве $T$ было $\mathbb N$ либо $\mathbb N_0$. А тут мы можем захотеть складывать целочисленную решётку. Первая проблема --- у нас нет естественной нумерации. Тут возникают подозрения, что должна быть абсолютная сходимость или некое её подобие. Но это можно только если $T\cong\mathbb N$, а если в качестве $T$ выступает $\mathbb R$ или ещё что страшнее, то вообще грустно.
        \end{Comment}
        \begin{Comment}
            Сумму семейства $a$ обозначаем знаком $\sum\limits_{t\in T}a_t$ либо $\sum\limits_T a$. Сумма пустого семейства считаем равным нулю, а ещё мы уже знаем, чему равна сумма конечного семейства. Также в этом параграфе под словом <<конечное>> имеется ввиду <<конечное или пустое>>.
        \end{Comment}
        \dfn Число $S$ называется \undercolor{red}{суммой семейства} $a$, если $\forall\varepsilon>0~\exists\text{ конечное }A\subset T~\forall\text{ конечное }B:A\subset B\subset T~\left|S-\sum\limits_{t\in B}a_t\right|<\varepsilon$. Суммы $S_B=\sum\limits_Ba$ называются при этом \undercolor{red}{частичными суммами} семейства. Семейство, имеющее некоторое число своей суммой, называют \undercolor{red}{суммируемым}. \undercolor{red}{Класс суммируемых семейств} называют $L(T)$ (в честь Лебега).
        \dfn Говорят, что \undercolor{red}{сумма семейства} $a\colon T\to\mathbb R$ \undercolor{red}{равна $+\infty$}, если $\forall E>0~\exists\text{ конечное }A\subset T~\forall\text{ конечное }B:A\subset B\subset T~\sum\limits_{t\in B}a_t>E$.
        \dfn Говорят, что \undercolor{red}{сумма семейства} $a\colon T\to\mathbb R$ \undercolor{red}{равна $-\infty$}, если $\forall E>0~\exists\text{ конечное }A\subset T~\forall\text{ конечное }B:A\subset B\subset T~\sum\limits_{t\in B}a_t<-E$.
        \dfn Говорят, что \undercolor{red}{сумма семейства} $a\colon T\to\mathbb R$ либо $\mathbb C$ \undercolor{red}{равна $\infty$}, если $\forall E>0~\exists\text{ конечное }A\subset T~\forall\text{ конечное }B:A\subset B\subset T~\sum\limits_{t\in B}|a_t|>E$.
        \begin{Comment}
            Вопрос, чем отличается $\sum\limits_{k\in\mathbb N}a_k$ и $\sum\limits_{k=1}^\infty a_k$? Разница в частичных суммах, во втором случае берём несколько слагаемых подряд, а в первом --- не обязательно подряд. Отсюда тривиально следует, что если есть сумма как семейства, то и обычная сумма ряда тоже есть, а теперь вопрос.
        \end{Comment}
        \thm Сумма семейства если существует, то единственна.
        \begin{Proof}
            Пусть есть две: $S$ и $\tilde S$. Рассмотрим две непересекающиеся окрестности $V_S$ и $V_{\tilde S}$. Каждому из них подберём $A$. Это будет $A$ и $\tilde A$. Тогда рассмотрим $T\supset B\supset A\cup\tilde A$, и тогда $S_B\in V_S$ и $S_B\in V_{\tilde S}$, а значит $S_B\in V_S\cap V_{\tilde S}=\varnothing$.
        \end{Proof}
        \thm \undercolor{red}{Свойства суммируемых семейств}.
        \begin{enumerate}[1.]
            \item Частичные суммы ограничены.
            \begin{Proof}
                Применим определение к $\varepsilon=1$. $\exists\text{ конечное }A\subset T~\forall\text{ конечное }B:A\subset B\subset T~\left|S-S_B\right|<1\Longrightarrow|S_B|<|S|+1$. Посмотрим на остальные частичные суммы, которые не $S_B$. Рассмотрим произвольное конечное $D\subset T$. Тогда $D=(D\cup A)\setminus(D\setminus A)$. Тогда, по вине конечности множеств $S_D=S_{D\cup A}-S_{D\setminus A}$. А значит $|S_D|=|S_{D\cup A}-S_{D\setminus A}|\leqslant|S_{D\cup A}|+|S_{D\setminus A}|<|S|+1+\sum\limits_Aa$. А это число конечно.
            \end{Proof}
            \item[1'.] Аналогично, если сумма семейства равна $+\infty$, то его частичные суммы ограничены снизу, а если $-\infty$ --- то сверху.
            \item Если $a$ и $b$ --- суммируемые семейства на множестве $T$, то $\alpha a+b$ также суммируемо, и его сумма равна $\alpha\sum\limits_Ta+\sum\limits_Tb$.
            \begin{Proof}
                Докажем, например, для аддитивности. Пусть $\sum\limits_{t\in T} a_t=S^a$, $\sum\limits_{t\in T} b_t=S^b$. Тогда рассмотрим $\varepsilon>0$. Для него существуют $A^a$ и $A^b$ из определения для числа $\frac\varepsilon2$. Теперь подберём конечное множество для суммы --- это $A=A^a\cup A^b$. Тогда для любого $B\text{ конечное }:A\subset B\subset T~|\sum\limits_{t\in T}(a+b)-(S^a+S^b)|\leqslant|\sum\limits_{t\in T}a_t-S^a|+|\sum\limits_{t\in T}b_t-S^b|<\varepsilon$.\\
                Аналогично для однородности.
            \end{Proof}
            \item[2'.] Если $\sum\limits_T a=+\infty$, $\sum\limits_T b\in(-\infty;+\infty]$, то $\sum\limits_T(a+b)=+\infty$. Аналогично наоборот.
            \item Если $a=x+\im y$ --- комплексное семейство, то его суммируемость равносильна одновременной суммируемости $x$ и $y$.
            \begin{Proof}
                В одну сторону --- линейность, в другую вот:\\
                Пусть $\sum\limits_T a=S=X+\im Y$. Тогда $\left|X-\sum\limits_T x\right|\leqslant\left|A-\sum\limits_T a\right|$ и $\left|Y-\sum\limits_T y\right|\leqslant\left|A-\sum\limits_T a\right|$. А значит если мы добьёмся того, что правая часть $<\varepsilon$, то левая будет тоже.
            \end{Proof}
            \item Пусть $a\colon T\to\mathbb R$ или $\mathbb C$, $\varphi\colon\tilde T\to T$ --- биекция. Пусть $\tilde a=a\circ\varphi$. Тогда $\sum\limits_{t\in T} a_t=\sum\limits_{\tilde t\in\tilde T}\tilde a_{\tilde t}$.
            \begin{Comment}
                Понимать равенство надо так: обе части существуют или нет одновременно, если существуют, то равны.
            \end{Comment}
            \begin{Proof}
                Неважно, в какую сторону доказывать. Пусть $\sum\limits_Ta=S$. Рассмотрим $V_S$. Мы знаем, что \textit{какие-то кванторы} $\sum\limits_Ba\in V_S$. Давайте рассмотрим $\tilde A=\varphi^{-1}(A)\subset\tilde T$. Очевидно, оно конечно. Также очевидно, что если мы возьмём $\tilde B$, то $A\subset\varphi(\tilde B)=B\subset T$. Тогда (поскольку в конечных суммах заменять индекс мы умеем), $\sum\limits_{\tilde B}\tilde a=\sum\limits_B a\in V_S$.
            \end{Proof}
        \end{enumerate}
        \thm Пусть $a\colon T\to[0;+\infty)$. Тогда $\exists\sum\limits_{t\in T}a_t\in[0;+\infty]$ и $\sum\limits_{t\in T}a_t=\sup\limits_{T\supset B\text{ конечно }}S_B$.
        \begin{Proof}
            Возьмём $\sup\limits_{T\supset B\text{ конечно }}S_B=S$ и докажем, что это сумма.\\
            Если $S\neq+\infty$, то вот: рассмотрим $\varepsilon>0$. По определению супремума, $\exists A\supset T$ конечное, что $S_A>S-\varepsilon$. Тогда это $A$ подойдёт нам. Рассмотрим $B$ из определения, тогда $S-\varepsilon<S_A\leqslant S_B\leqslant S<S+\varepsilon$.\\
            Если $S=+\infty$, то аналогично, напишите сами.
        \end{Proof}
        \begin{Comment}
            Удобно разрешить $a$ принимать значение $+\infty$. Тогда всё просто --- если есть хотя бы одно бесконечное значение --- сумма равна $+\infty$. В условии такого допущения предыдущее утверждение остаётся верным.
        \end{Comment}
        \begin{Comment}
            Если $a$ и $b$ неотрицательные семейства и $a\leqslant b$, то $\sum\limits_Ta\leqslant\sum\limits_Tb$. Это очевидно, потому что все частичные суммы $a$ не больше аналогичных $b$, а значит супремум $a$ не больше супремума $b$.
        \end{Comment}
        \thm Суммируемость и абсолютная суммируемость. Пусть $a$ --- семейство. Тогда для суммируемости $a$ необходимо и достаточно суммируемости $|a|$.
        \begin{Proof}
            Достаточно рассматривать вещественные семейства, потому что для комплексных эта теорема является следствием из свойства 3 и замечания выше при помощи утверждения $|x|,|y|\leqslant|a|\leqslant|x|+|y|$.\\
            $0\leqslant a_\pm\leqslant |a|$ и $a_++a_-=|a|$, следовательно $|a|\in L(T)\Leftrightarrow a_\pm\in L(T)$. осталось доказать, что $a_\pm\in L(T)\Leftrightarrow a\in L(T)$. Вправо это очевидно. Проверим влево. Проверим для $a_+$, для $a_-$ аналогично. У неотрицательного семейства суммируемость равносильна ограниченности сверху частичных сумм. При этом мы уже знаем ограниченность с обеих сторон частичных сумм $a$. Тогда рассмотрим для каждого $B$ рассмотрим $B_+=\{t\in B\mid a_t\geqslant0\}$. Тогда частичная сумма $\sum\limits_{B}a_+$ равна некоторой частичной сумме $\sum\limits_{B_+}a$. Вторая штука ограничена, как мы уже говорили, в том числе ограничена сверху.
        \end{Proof}
        \begin{Comment}
            $$\sum\limits_T a=+\infty\Leftrightarrow\left\{\begin{aligned}
                a_+&\notin L(T)\\
                a_-&\in L(T)\\
            \end{aligned}\right\}\qquad\qquad
            \sum\limits_T a=-\infty\Leftrightarrow\left\{\begin{aligned}
                a_+&\in L(T)\\
                a_-&\notin L(T)\\
            \end{aligned}\right.$$
            Для доказательства этого достаточно случаи перебрать и несколько раз отослаться к характеристическому свойству сходимости положительного ряда.\\
            Когда $a_+$ и $a_-$ оба суммируемы, знаем. Если суммируемо только одно, то другое равно $\pm\infty$, а дальше аддитивность, если не суммируемо ничего, то ни сверху, ни снизу суммы не ограничены, а значит семейство не может иметь суммы в $\overline{\mathbb R}$.
        \end{Comment}
        \thm Если $a,b\colon T\to\mathbb R$, $a\leqslant b$ и оба семейства имеют сумму в $\overline{\mathbb R}$, тогда $\sum\limits_Ta\leqslant\sum\limits_Tb$.
        \begin{Proof}
            $$a<b\Leftrightarrow\left\{\begin{aligned}
                a_+&\leqslant b_+\\
                a_-&\geqslant b_-\\
            \end{aligned}\right.$$
            Сюда можно применить монотонность неотрицательных семейств, после чего вычтем из одного равенства другое. И вычесть можно, потому что не образуется разность $+\infty$ или $-\infty$. А у нас такой случай исключёны.
        \end{Proof}
        \thm Пусть $a\colon T\to\mathbb R$, $b\colon T\to[0;+\infty)$, $b\in L(T)$, $|a|\leqslant b$. Тогда $a\in L(T)$.
        \begin{Proof}
            По монотонности $|a|\in L(T)$. Дальше теорема об абсолютной суммируемости.
        \end{Proof}
        \thm Если $a$ имеет сумму, то $\left|\sum\limits_Ta\right|\leqslant\sum\limits_T|a|$.
        \begin{Proof}
            Если $\sum\limits_Ta=\infty$, то доказывать нечего.\\
            Если $\sum\limits_Ta=0$, то доказывать тоже нечего.\\
            Для вещественных семейств можно было бы рассмотреть $a_+$ и $a_-$, но для комплексных чисел тут возникает заминка, поэтому для них используется очень искусственный приём. Обозначим $z=\frac{\left|\sum\limits_Ta\right|}{\sum\limits_Ta}$. Это какое-то комплексное число, по модулю равное единице. Тогда $\left|\sum\limits_Ta\right|=z\sum\limits_Ta=\sum\limits_Tza$. Эта сумма вещественна, значит совпадает со своей вещественной частью. $\sum\limits_Tza=\Re\sum\limits_Tza=\sum\limits_T\Re za\leqslant\sum\limits_T|za|=\sum\limits_T|a|$.
        \end{Proof}
        \thm Сужение суммируемого семейства суммируемо.
        \begin{Proof}
            Рассмотрим вот такое неравенство: $\sum\limits_{T_0}|a|\leqslant\sum\limits_T|a|$. Второе суммируемо (по равносильности абсолютной и обычной суммируемости), а значит $<+\infty$, значит первое также меньше, то есть также суммируемо. Посте применяем теорему об абсолютной суммируемости ещё раз.
        \end{Proof}
        \thm Связь с рядами. Пусть $a\colon\mathbb N\to\mathbb R$ или $\mathbb C$. Тогда $a\in L(\mathbb N)\Leftrightarrow a$ абсолютно сходится. При этом $\sum\limits_{\mathbb N}a=\sum\limits_{k=1}^\infty a_k$.
        \begin{Proof}
            Сперва пусть $a\geqslant0$. Как мы обсуждали, запас частичных сумм семейства содержит в себе запас частичных сумм ряда. А значит $\sup\limits_{\text{конечное }B\subset T}S_B\geqslant\sup\limits_{n\in\mathbb N}S_n$. При этом для \textbf{конечного} $B$ мы можем обозначить $n=\max B$, и тогда $S_B\leqslant S_n$, то есть и $\sup\limits_{\text{конечное }B\subset T}S_B\leqslant\sup\limits_{n\in\mathbb N}S_n$.\\
            Теперь $a\colon\mathbb N\to\mathbb R$. Как мы знаем по предыдущему случаю $\sum\limits_{\mathbb N}(a)_\pm=\sum\limits_{k=1}^\infty(a_k)_\pm$. Сходимость левой части равносильна $a\in L(\mathbb N)$, сходимость правой --- абсолютной сходимости $\sum\limits_{k=1}^\infty a_k$.\\
            Когда $a\colon\mathbb N\to\mathbb C$, рассмотрим по отдельности $\Re a$ и $\Im a$, получим то же самое, что и обычно.
        \end{Proof}
        \thm \undercolor{darkgreen}{Ненулевые члены суммируемого семейства}. Пусть $a\in L(T)$. Тогда
        \begin{enumerate}
            \item $\forall\varepsilon>0~T_\varepsilon=\{t\in T\mid|a_t|\geqslant\varepsilon\}$ конечно.
            \item $T_0=\{t\in T\mid a_t\neq0\}$ не более чем счётно.
        \end{enumerate}
        \begin{Proof}
            Для начала, будем рассматривать только $a\geqslant0$, от этого утверждения теоремы почти не изменятся, а сходимость $a$ равносильна сходимости $|a|$.\\
            Тогда $+\infty>\sum\limits_Ta\geqslant\sum\limits_{T_\varepsilon}a\geqslant\sum\limits_{T_\varepsilon}\varepsilon=\varepsilon\operatorname{card}T_\varepsilon$.\\
            Второе следует из первого: $T_0=\{t\in T\mid a_t>0\}=\bigcup\limits_{n=1}^\infty T_{1/n}$. Второе --- счётное объединение конечных множество, оно не более чем счётно.
        \end{Proof}
        \thm \undercolor{darkgreen}{Суммирование группами}. Пусть $T=\bigsqcup\limits_{\lambda\in\Lambda}T_\lambda$. Пусть $a\colon T\to[0;+\infty]$ или $a\in L(T)$. Тогда
        $$
        \sum\limits_{t\in T}a_t=\sum\limits_{\lambda\in\Lambda}\left(\sum\limits_{t\in T_\lambda}a_t\right)
        $$
        \begin{Proof}
            Первый и основной случай --- неотрицательное семейство. Тогда не вызывает сомнений существование сумм слева и справа. Обозначим левую часть буквой $S$, правую --- буквой $Q$, а суммы $\sum\limits_{t\in T_\lambda}a_t$ --- $q_\lambda$. Докажем, что $S\leqslant Q$ и что $S\geqslant Q$.
            \begin{itemize}
                \item[$S\leqslant Q$]. Достаточно доказать, что все частичные суммы $S$ не больше $Q$. То есть мы выбираем некоторое конечное множество $t_1;\ldots;t_N\in T$. Тогда каждое $t_i$ попадает в некоторое (ровно одно) $q_\lambda$. То есть $\sum\limits_{i=1}^N a_{t_i}\leqslant\sum\limits_{j=1}^Mq_j$, где $M\leqslant N$. А это уж явно не больше $Q$ как частичная сумма $\sum\limits_{\lambda\in\Lambda}q_\lambda$.
                \item[$S\geqslant Q$]. Если $S=+\infty$, доказывать нечего. Отныне считаем, что $S<+\infty$. Тогда все $q_\lambda$ также конечны, потому что это сумма подсемейства $a$. Зафиксируем $\varepsilon>0$ и возьмём частичную сумму семейства $\{q_\lambda\}$. То есть возьмём различные $\lambda_1;\ldots;\lambda_M\in\Lambda$. Заметим, что $\forall j\in[1:M]~\exists t_{j;1};\ldots;t_{j;N_j}\in T_{\lambda_j}~\sum\limits_{k=1}^{N_j}a_{t_{j;k}}\geqslant q_{\lambda_j}-\frac\varepsilon M$. То есть мы приближаем сумму $q_{\lambda_j}$ частичными суммами $\sum\limits_{k=1}^{N_j}a_{t_{j;k}}$ с точностью $\frac\varepsilon M$. Тогда
                $$
                \sum\limits_{j=1}^Mq_{\lambda_j}<\sum\limits_{j=1}^M\sum\limits_{k=1}^{N_j}a_{t_{j;k}}+\varepsilon
                $$
                Также стоит заметить, что все $t_{j;k}$ различны. Если мы берём разные $k$, то по построению, а если разные $j$, то потому что $T_\lambda$ дизъюнктны. То есть двойная сумма справа является некоторой частичной суммой $a_t$. А значит она не больше $S$, то есть
                $$
                \sum\limits_{j=1}^Mq_{\lambda_j}<\sum\limits_{j=1}^M\sum\limits_{k=1}^{N_j}a_{t_{j;k}}+\varepsilon\leqslant S+\varepsilon
                $$
                А это именно то, что мы хотим.
            \end{itemize}
            Теперь перейдём к случаю, когда $a\in L(T)$. Ну, приёмы всё те же. Сначала рассмотрим вещественное семейство $a$ и рассмотрим $(a_t)_+$ и $(a_t)_-$. Мы из доказанного знаем, что
            \[
            \begin{split}
                \sum\limits_{t\in T}(a_t)_+&=\sum\limits_{\lambda\in\Lambda}\left(\sum\limits_{t\in T_\lambda}(a_t)_+\right)\\
                \sum\limits_{t\in T}(a_t)_-&=\sum\limits_{\lambda\in\Lambda}\left(\sum\limits_{t\in T_\lambda}(a_t)_-\right)
            \end{split}
            \]
            Вычтем второе из первого. Мы можем так сделать, потому что мы уже знаем, что семейство суммируемо. Чтобы перейти к комплексному случаю, никогда не догадаетесь, что нужно сделать.
        \end{Proof}
        \begin{Comment}
            Если мы хотим применить это правило, надо сначала убедиться в суммируемости семейства. Как проверить, что семейство суммируемо? Ну, это равносильно абсолютной суммируемости, мы можем поставить модуль, заключить что
            $$
            \sum\limits_{t\in T}|a_t|=\sum\limits_{\lambda\in\Lambda}\left(\sum\limits_{t\in T_\lambda}|a_t|\right)
            $$
            И посмотреть на правую часть. Если она меньше бесконечности, то аналогичная формула верна и без модулей.
        \end{Comment}
        \thm Пусть $T=X\times Y$. Пусть $a\colon T\colon[0;+\infty]$ или $a\in L(T)$. Тогда
        $$
        \sum\limits_{(x;y)\in X\times Y}a_{xy}=\sum\limits_{x\in X}\sum\limits_{y\in Y}a_{xy}=\sum\limits_{y\in Y}\sum\limits_{x\in X}a_{xy}
        $$
        \begin{Proof}
            Ну, лол, $X\times Y=\bigsqcup\limits_{y\in Y}X\times\{y\}$. В общем-то всё.
            $$
            \sum\limits_{(x;y)\in X\times Y}a_{xy}=\sum\limits_{y\in Y}\sum\limits_{(x;y)\in X\times\{y\}}a_{xy}=\sum\limits_{y\in Y}\sum\limits_{x\in X}a_{xy}
            $$
        \end{Proof}
        \begin{Comment}
            В учебниках по анализу семейств обычно нет, а есть теоремы о суммировании двойных и повторных рядов. Так вот, при $X=Y=\mathbb N$ мы имеем
            $$
            \sum\limits_{j,k=1}^\infty a_{kj}=\sum\limits_{k=1}^\infty\sum\limits_{j=1}^\infty a_{kj}=\sum\limits_{j=1}^\infty\sum\limits_{k=1}^\infty a_{kj}
            $$
            Это не то же самое, что и нумерация бесконечной матрицы. Понятно, почему можно. Правда, для рядов понимают суммы и разными другими способами точно также как и для обычных рядов. Так в $\sum\limits_{j,k=1}^\infty a_{kj}$ можно понимать частичную сумму как сумму <<по прямоугольникам>>. И определяют эту сумму как двойной предел. Тут пригодится теорема о двойном и повторных пределах. Та теорема нам говорила, что если у нас есть двойной предел и есть конечный внутренний предел, то есть повторный предел равен двойному. Сюда, понятно, она также применяется.\\
            Точно также можно брать <<круговые>> частичные суммы или вообще произвольную выпуклую окрестность нуля и раздувать её. И если нет абсолютной сходимости, полученные разные суммы могут не совпадать.
        \end{Comment}
        \thm Пусть $\{a\}_{x\in X}$, $\{b\}_{y\in Y}$ --- оба неотрицательные или оба суммируемые семейства. Тогда
        $$
        \left(\sum\limits_{x\in X}a_x\right)\left(\sum\limits_{y\in Y}b_y\right)=\sum\limits_{(x;y)\in X\times Y}a_x\cdot b_y
        $$
        \begin{Proof}
            Тут пока что никто не обещал, что $\{a_xb_y\}_{(x;y)\in X\times Y}$ суммируемо. И никто не говорил, что делать, если (в любой из частей) мы получили $0\times\infty$. Отсюда и далее вводим соглашение, что это равно $0$.\\
            Докажем сначала для неотрицательных. Тогда с существованием никаких вопросов нет. Тогда
            \[
            \sum\limits_{(x;y)\in X\times Y}a_x\cdot b_y=\sum\limits_{x\in X}\sum\limits_{y\in Y}a_x\cdot b_y\overset{(*)}=\sum\limits_{x\in X}a_x\left(\sum\limits_{y\in Y}b_y\right)\overset{\text{аналогично }(*)}=\left(\sum\limits_{y\in Y}b_y\right)\left(\sum\limits_{x\in X}a_x\right)
            \]
            Касательно $(*)$: если всё конечно, то это обычная однородность. Если $a_x=0$, то вне зависимости от суммы и $b_y$ имеем $0=0$. Если $a_x=+\infty$, то либо все $b_y=0$ (тогда обе суммы дают 0), либо нет (тогда $+\infty$).\\
            Теперь давайте для произвольных суммируемых семейств. Ну, мы уже знаем, что делать в таком случае. В таком случае мы знаем, что указанное равенство верно для модулей, а значит $\{a_xb_y\}_{(x;y)\in X\times Y}$ абсолютно суммируемо, а значит и просто суммируемо. Далее аналогично неотрицательному случаю.
        \end{Proof}
        \begin{Comment}
            Оба следствия теоремы о суммировании группами по индукции распространяются на произведение нескольких семейств.
        \end{Comment}
        \begin{Example}
            $$\sum\limits_{k=1}^\infty\sum\limits_{j=1}^\infty\frac1{(k+j)^\alpha}$$
            Из-за неотрицательных слагаемых это точно также можно написать как сумму по прямоугольникам, или сумму по семейству пар или ещё как угодно, нам будет удобно в виде повторной суммы.
            $$
            \sum\limits_{j=1}^\infty\frac1{(k+j)^\alpha}\overset{l=k+j}=\sum\limits_{l=k+1}^\infty\frac1{l^\alpha}\sim\frac{\mathrm{const}}{k^{\alpha-1}}
            $$
            $$
            \sum\limits_{k=1}^\infty\sum\limits_{j=1}^\infty\frac1{(k+j)^\alpha}\sim\sum\limits_{k=1}^\infty\frac{\mathrm{const}}{k^{\alpha-1}}
            $$
            Это сходится тогда и только тогда, когда $\alpha-1>1$, как мы знаем, то есть $\alpha>2$.
        \end{Example}
        \dfn Пусть $a\colon T\to\overline{\mathbb R}$. Полагают $\sum\limits_T a=\sum\limits_T a_+-\sum\limits_T a_-$, если правая часть имеет смысл (то есть нет разности двух $+\infty$).
        \begin{Comment}
            Теорема о суммировании группами и её следствиях остаются верными.
        \end{Comment}
    \end{itemize}
    \section{Дифференциальное исчисление в евклидовых пространствах.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Напоминаем то, что было очень давно.\\
            Функция называлась дифференцируемой в точке $x$, если $f(x+h)=f(x)+Ah+o(h)$. И ещё было про предел разностного отношения. И мы доказывали равносильность таких определений. Так вот, оказывается, что при переходе к функциям нескольких переменных (и дальше, если область значений также многомерна), два определения перестают быть равносильны. Основным и настоящим будет написанное выше, а предел разностного отношения приводит к понятию частной производной. Так вот. У нас число $h$ умножалось на число $A$. А тут у нас $h$ будет вектором, а $A$ будет матрицей. И в таком случае можно трактовать $A$ как линейный оператор.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Линейные операторы в евклидовых пространствах}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Для более детального рассказа сходите почитайте конспект по ЛинАлу. Но там не полностью всё, что нам надо.
        \end{Comment}
        \begin{Comment}
            Понятие линейного оператора применимо не только к евклидовым пространствам, да и даже не только к конечномерным пространствам. Причём в анализе чаще пространства именно что бесконечномерные.
        \end{Comment}
        \dfn Пусть $X,Y$ --- векторные пространства над полем $\scriptK$. Отображение $\scriptA\colon X\to Y$ называется \undercolor{red}{линейным отображением}, \undercolor{red}{линейным оператором} или \undercolor{red}{линейным преобразованием}, если оно, \textit{барабанная дробь}, линейно. То есть $\forall x_1;x_2\in X~\forall\lambda,\mu\in\scriptK~\scriptA(\lambda x_1+\mu x_2)=\lambda\scriptA x_1+\mu\scriptA x_2$.
        \begin{Comment}
            У нас в анализе $\scriptK=\mathbb R$ или $\scriptK=\mathbb C$. В нашем параграфе будет $\mathbb R$.
        \end{Comment}
        \thm $\scriptA\left(\sum\limits_{k=1}^N\lambda_kx_k\right)=\sum\limits_{k=1}^N\lambda_k\scriptA x_k$.
        \begin{Proof}
            Индукция.
        \end{Proof}
        \thm $\scriptA\theta_X=\theta_Y$.
        \begin{Proof}
            $\lambda=0$.
        \end{Proof}
        \thm Множество линейных операторов --- векторное пространство над $\scriptK$.
        \begin{Proof}
            Проверьте аксиомы аккуратно, они все тривиальны.
        \end{Proof}
        \dfn Если $X;Y;Z$ --- линейные пространство над $\scriptK$, то \undercolor{red}{произведением операторов} $\scriptA\colon X\to Y$ и $\scriptB\colon Y\to Z$ называется оператор $\scriptB\scriptA$, равный $\scriptB\circ\scriptA$.
        \begin{Comment}
            Внизу мы нумеруем координаты, а вверху --- сами вектора. То есть последовательность векторов --- это $v^1;\ldots;v^n$.
        \end{Comment}
        \dfn \undercolor{red}{Орт} --- это $e^i$, такой что где $e^i_k=\delta_{ik}=\begin{cases}
            1 & k=i\\
            0 & k\neq i
        \end{cases}$.
        \thm Координаты вектора $x$ являются координатами в базисе ортов.
        \begin{Comment}
            Мы знаем, как умножать матрицу на вектор (в нотации вектор-столбец).\\
            Ещё мы знаем, что есть взаимооднозначное соответствие между матрицей и оператором. Если $(A)$ --- это оператор матрицы $A$, то, как мы знаем, $(A+B)=(A)+(B)$, $(\lambda A)=\lambda A$, $(\Theta)=\mathbb 0_{m\times N}$, $(\mathcal E)=I_{n\times n}$, $(BA)=(B)(A)$.
        \end{Comment}
        \thm Пусть $\scriptA$ --- линейный оператор $\mathbb R^n\to\mathbb R^n$. Тогда следующие утверждения равносильны.
        \begin{itemize}
            \item $\scriptA$ обратим.
            \item $\scriptA$ сюръективен.
            \item $\det\scriptA\neq0$.
        \end{itemize}
        \begin{Proof}
            Первое очевидно равносильно третьему, а второе --- это $\rank\scriptA=n$.
        \end{Proof}
        \thm Обратный оператор также линеен и его матрица является обратной матрице $\scriptA$.
        \begin{Comment}
            Отсюда начинается что-то новое.
        \end{Comment}
        \dfn Пусть $X,Y$ --- нормированные пространства (оба вещественные или оба комплексные). \undercolor{red}{Нормой} \undercolor{red}{линейного оператора} $\scriptA\colon X\to Y$ называется величина $\|\scriptA\|=\sup\limits_{\|x\|_X\leqslant1}\|\scriptA x\|_Y$.
        \begin{Comment}
            То есть мы берём единичный шар в $X$, деформируем его при помощи $\scriptA$ и смотрим, насколько он раздулся.
        \end{Comment}
        \begin{Comment}
            Из этого определения непонятно, конечна ли норма. На самом деле, не всегда.
        \end{Comment}
        \dfn Если $\|\scriptA\|<+\infty$, то $\scriptA$ --- \undercolor{red}{ограниченный оператор}. Множество ограниченных операторов $X\to Y$ обозначается как $\scriptL(X\to Y)$ или $\scriptL(X;Y)$. В случае $X=Y$ также пишут $\scriptL(X)$.
        \begin{Comment}
            Это определение не совпадает с определением ограниченного отображения (ограниченное отображение --- множество значений содержится в некотором шаре). Если $\scriptA\neq\Theta$, то $\exists x~\scriptA x\neq\theta\Rightarrow\|\scriptA\lambda x\|=|\lambda|\|\scriptA x\|\overset{\lambda\to+\infty}\longrightarrow+\infty$.
        \end{Comment}
        \thm Норма оператора действительно является нормой.
        \begin{Proof}
            \begin{enumerate}
                \item $\|\scriptA\|=0\Leftrightarrow\scriptA=\Theta$ --- очевидно.
                \item $\|\lambda\scriptA\|=|\lambda|\|\scriptA\|$?
                $$
                \|\lambda\scriptA\|=\sup\limits_{\|x\|_X\leqslant1}\|\lambda\scriptA x\|_Y=\sup\limits_{\|x\|_X\leqslant1}|\lambda|\|\scriptA x\|_Y=|\lambda|\sup\limits_{\|x\|_X\leqslant1}\|\scriptA x\|_Y
                $$
                Последнее равенство использует соглашение $0\cdot\infty=0$.
                \item $\|\scriptA+\scriptB\|\leqslant\|\scriptA\|+\|\scriptB\|$?
                $$
                \text{Пусть }\|x\|_X\leqslant1\qquad
                \|(\scriptA+\scriptB)x\|_Y=\|\scriptA x+\scriptB x\|_Y\leqslant\|\scriptA x\|_Y+\|\scriptB x\|_Y
                $$
                Перейдём к супремуму, получим что надо.
            \end{enumerate}
        \end{Proof}
        \thm $\scriptL(X\to Y)$ с введённой нормой является нормированным пространством.
        \thm \undercolor{darkgreen}{Вычисление нормы оператора}. Пусть $\scriptA\colon X\to Y$ --- линейный оператор. Тогда
        \[
        \begin{split}
            \|\scriptA\|&=\overbrace{\sup\limits_{\|x\|_X\leqslant1}\|\scriptA x\|_Y}^{\romanic1}=\overbrace{\sup\limits_{\|x\|_X<1}\|\scriptA x\|_Y}^{\romanic2}=\overbrace{\sup\limits_{\|x\|_X=1}\|\scriptA x\|_Y}^{\romanic3}=\\
            &=\underbrace{\sup\limits_{x\neq\theta_X}\frac{\|\scriptA x\|_Y}{\|x\|_X}}_{\romanic4}=\underbrace{\inf\{C\in\mathbb R\mid\forall x\in X~\|\scriptA x\|_Y\leqslant C\|x\|_X\}}_{\romanic5}
        \end{split}
        \]
        \begin{Proof}
            Докажем $\romanic3\leqslant\romanic1\leqslant\romanic5\leqslant\romanic4\leqslant\romanic2\leqslant\romanic3$.
            \begin{itemize}
                \item[$\romanic3\leqslant\romanic1$] Очевидно.
                \item[$\romanic1\leqslant\romanic5$] Если $\romanic{5}=+\infty$, доказывать нечего. Иначе $\forall n\in\mathbb N~\forall x\in X~\|\scriptA x\|\leqslant(\romanic{5}+\frac1n)\|x\|$. Устремим $n\to\infty$. Получим $\forall x\in X~\|\scriptA x\|\leqslant\romanic{5}\|x\|$. А значит $\forall \|x\|\leqslant1$ $\|\scriptA x\|\leqslant\romanic{5}\|x\|\leqslant\romanic{5}$. Возьмём инфинум обеих частей.
                \item[$\romanic5\leqslant\romanic4$]. Для $x\neq\theta$ верно что $\|\scriptA x\|\leqslant\romanic{4}\|x\|$, и для $\theta$ тоже верно. Тогда если $\romanic{4}=+\infty$, то тривиально, а иначе это некоторое число (равное $C$), которое $\forall x~\|\scriptA x\|\leqslant C\|x\|$. А $\romanic{5}$ --- инфинум таких чисел.
                \item[$\romanic4\leqslant\romanic2$] Докажем, что $\forall x\neq\theta~\frac{\|\scriptA x\|}{\|x\|}\leqslant\romanic{2}$. Если $\romanic{2}=+\infty$, то доказывать нечего. Положим $\tilde x=\frac x{(1+\varepsilon)\|x\|}$. Очевидно, $\|\tilde x\|=\frac1{1+\varepsilon}\leqslant1$. Также $\|\scriptA x\|\leqslant\romanic{2}\|x\|$. Тогда $\|\scriptA x\|=\|\scriptA((1+\varepsilon)\|x\|\tilde x)\|=(1+\varepsilon)\|x\|\|\scriptA\tilde x\|\leqslant(1+\varepsilon)\|x\|\romanic{2}$. Осталось устремить $\varepsilon\to0+$.
                \item[$\romanic2\leqslant\romanic3$] Возьмём $\|x\|<1$ и докажем, что $\|\scriptA x\|\leqslant\romanic{3}$. Если $x=\theta$, неравенство очевидно. Положим $x^*=\frac x{\|x\|}$. Тогда $\|x^*\|=1$. А значит $\|\scriptA x\|=\|x\|\|\scriptA x^*\|\leqslant\|x\|\romanic{3}\leqslant\romanic{3}$.
            \end{itemize}
        \end{Proof}
        \thm $\|\scriptA x\|\leqslant\|\scriptA\|\|x\|$.
        \begin{Proof}
            Из четвёртого.
        \end{Proof}
        \thm Если для $C\geqslant0$ верно что $\forall x~\|\scriptA x\|\leqslant C\|x\|$, то $\|\scriptA\|\leqslant C$.
        \begin{Proof}
            Из пятого.
        \end{Proof}
        \thm Если для $c>0$ верно что $\exists x^*\neq\theta~\|\scriptA x^*\|\geqslant c\|x^*\|$, то $\|\scriptA\|>c$.
        \begin{Proof}
            Из четвёртого: если для какого-то вектора дробь не меньше $c$, то и для супремума также.
        \end{Proof}
        \thm Если $\|\scriptA\|<+\infty$, то в \romanic{5} можно взять $\min$, а не $\inf$.
        \thm $\|\scriptB\scriptA\|\leqslant\|\scriptB\|\cdot\|\scriptA\|$.
        \begin{Proof}
            Рассмотрим произвольное $x\in X$. Для него $\|\scriptB\scriptA x\|\leqslant\|\scriptB\|\cdot\|\scriptA x\|\leqslant\|\scriptB\|\cdot\|\scriptA\|\cdot\|x\|$. Тогда $\|\scriptB\|\cdot\|\scriptA\|$ подходит как $C$ в свойство на три выше.
        \end{Proof}
        \thm \undercolor{darkgreen}{Критерий непрерывности линейного оператора}. Пусть $\scriptA\colon X\to Y$ --- линейный оператор. Тогда следующие утверждения равносильны:
        \begin{enumerate}
            \item $\scriptA$ ограничен.
            \item $\scriptA$ непрерывен в нуле.
            \item $\scriptA$ непрерывен в каждой точке.
            \item $\scriptA$ равномерно непрерывен.
        \end{enumerate}
        \begin{Proof}
            Докажем $4\Rightarrow3\Rightarrow2\Rightarrow1\Rightarrow4$.
            \begin{itemize}
                \item[$4\Rightarrow3$] Тривиально.
                \item[$3\Rightarrow2$] Тривиально.
                \item[$2\Rightarrow1$] Возьмём $\varepsilon=1$, подберём $\delta>0$, что $\forall x\in X:\|x\|<\delta~\|\scriptA x\|<1$. Нужно оценить норму $\|\scriptA x\|$ по всем $x$ из шара. Если $x_\delta=\delta x$, то $\|x_\delta\|<\delta$. Тогда
                $$
                \|\scriptA x\|=\frac1\delta\|\scriptA x_\delta\|<\frac1\delta\Rightarrow\|\scriptA\|\leqslant\frac1\delta
                $$
                \item[$1\Rightarrow4$] Нужно оценивать норму разности $\|\scriptA x_1-\scriptA x_2\|$. Она равна $\|\scriptA(x_1-x_2)\|\leqslant\|\scriptA\|\|x_1-x_2\|$. Теперь проверяем определение равномерной непрерывности. Если $\scriptA=\Theta$, очевидно. Иначе $\forall\varepsilon>0~\delta=\frac\varepsilon{\|\scriptA\|}>0$. Тогда $\forall x_1;x_2\in x:|x_1-x_2|<\delta\Rightarrow\|\scriptA x_1-\scriptA x_2\|<\|\scriptA\|\frac\varepsilon{\|\scriptA\|}=\varepsilon$.
            \end{itemize}
        \end{Proof}
        \begin{Comment}
            К предыдущим четырём утверждениям есть два бонусных равносильных им --- <<образ любого ограниченного множество ограничен>> и <<образ единичного шара ограничен>>.
        \end{Comment}
        \begin{Comment}
            Любой линейный оператор $\mathbb R^n\to\mathbb R^m$ непрерывен (поскольку является умножением на матрицу, а умножение на матрицу --- арифметические действия над координатами). И тогда можно в определении ограниченности можно сразу заменить $\sup$ на $\max$ по теореме Вейерштрасса (единичный шар компактен).\\
            То есть $\scriptL(X\to Y)=L(X;Y)$.
        \end{Comment}
        \begin{Comment}
            Существуют линейные неограниченные операторы. Потому что в бесконечномерном случае оператор не сводится к матрице.
            \begin{Example}
                Рассмотрим $C[a;b]$. $\|x\|=\max\limits_{t\in[a;b]}|x(t)|$ (она называется равномерной нормой). Проверка того, что это норма --- очевидно. И рассмотрим его подпространство $C^{(1)}[a;b]$. А в качестве оператора возьмём $\frac{\mathrm d}{\mathrm dx}$. Он разрывен. Почему? Возьмём $a=0$, $b=\pi$, $x_n(t)=\sin nt$, $n\in\mathbb N$. Тривиально, $\|x_n\|=1$. Известно, что $x_n'(t)=n\cos nt$, то есть $\|x_n'\|=n$. Пожалуйста, оператор не ограничен.\\
                Правда, если за норму взять $\|x\|_{C^{(1)}[a;b]}=\|x\|_{C[a;b]}+\|x'\|_{C[a;b]}$, то будет ограничен.
            \end{Example}
        \end{Comment}
        \thm \undercolor{darkgreen}{Оценка нормы оператора в евклидовых пространствах}. Пусть $\scriptA\in\scriptL(\mathbb R^n\to\mathbb R^m)$, $A=a_{ik}$ --- его матрица. Тогда
        $$
        \|\scriptA\|\leqslant\sqrt{\sum\limits_{i=1}^m\sum\limits_{k=1}^na_{ik}^2}
        $$
        \begin{Proof}
            Рассмотрим $x\in\mathbb R^n$.
            \[
            \begin{split}
                \|\scriptA x\|^2&=\sum\limits_{i=1}^m(\scriptA x)_i^2=\sum\limits_{i=1}^m\left(\sum\limits_{k=1}^na_{ik}x_k\right)^2\overset{\text{К---Б---Ш}}\leqslant\\
                &\leqslant\sum\limits_{i=1}^m\left(\sum\limits_{k=1}^na_{ik}^2\right)\left(\sum\limits_{k=1}^nx_k^2\right)=\sum\limits_{i=1}^m\left(\sum\limits_{k=1}^na_{ik}^2\right)\|x\|^2
            \end{split}
            \]
            Ну, извлекаем отсюда квадрат и всё.
        \end{Proof}
        \begin{Comment}
            До тех пор пока мы не умножаем матрицу на вектор мы можем рассмотреть её как вектор из $\mathbb R^{nm}$. И тогда, как несложно заметить $\sqrt{\sum\limits_{i=1}^m\sum\limits_{k=1}^na_{ik}^2}$ будет нормой такого вектора. Она даже название имеет --- норма Гильберта --- Шмидта.
        \end{Comment}
        \begin{Comment}
            В теореме выше равенство, вообще говоря, не достигается. Более того, оценка может быть довольно грубой. Если взять $n=m$ и тождественный оператор, его норма равна единице, а оценка даёт нам $\sqrt n$. То есть в данном случае оценка становится тем грубее, чем больше $n$. Впрочем, эта оценка лучше чем ничего.\\
            А когда мы дойдём до именно что дифференциального счисления, то получим явный ответ, он будет связан с собственными числами.
        \end{Comment}
        \thm Если $n=1$ или $m=1$, то неравенство в <<оценке нормы оператора в евклидовых пространствах>> превращается в равенство.
        \begin{Proof}
            Если $n=1$, то $A=\matr{a_{11}\\\vdots\\a_{m1}}$. Утверждается, что его операторная норма совпадает с евклидовой нормой как вектора. Оценка сверху у нас есть, докажем оценку снизу. Возьмём $t^*=\matr{1}=1$. Тогда $\scriptA t^*=A$. То есть $\|t^*\|=1$, а $\|\scriptA t^*\|=\sqrt{\sum\limits_{i=1}^ma_{i1}^2}$. По одному из замечаний к теоремы о вычислении нормы оператора это значит, что $\|\scriptA\|\geqslant\sqrt{\sum\limits_{i=1}^ma_{i1}^2}$.\\
            Если $m=1$, то $A=\matr{a_{11} & \cdots & a_{1n}}$. Если $\scriptA=\Theta$, доказывать нечего. Иначе возьмём $x^*=A^T$. Тогда $\|\scriptA x^*\|=\sum\limits_{k=1}^na_{1k}^2$, $\|x^*\|=\sqrt{\sum\limits_{k=1}^na_{1k}^2}$. По тому же замечанию $\|\scriptA\|=\sqrt{\sum\limits_{k=1}^na_{1k}^2}$
        \end{Proof}
        \begin{Example}
            Проекторы на координатные оси.\\
            $x=\matr{x_1\\\vdots\\x_n}$, $k\in[1:n]$. Тогда $\pi_k\colon\mathbb R^n\to\mathbb R^1$, его матрица выглядит как все нули и единица на $k$-том места.\\
            \\
            Операторы вложения.\\
            $U_i\colon\mathbb R^1\to\mathbb R^m$ --- берём число и помещает его на $i$-ту. позицию вектора. Остальные заполняет нулями. Понятно, что его матрица выглядит также как и $\pi_k$, но вертикально.
        \end{Example}
        \begin{Comment}
            Что будет, если вместо евклидовой нормы задать другую. Останется ли оператор непрерывным? Ответ мы знаем, но всё же.
        \end{Comment}
        \dfn Пусть $X$ --- вещественное или комплексное векторное пространство, $p_1$ и $p_2$ --- две нормы в $X$. Нормы $p_1$ и $p_2$ называют \undercolor{red}{эквивалентными}, если $\exists c,C>0~\forall x\in X~cp_2(x)\leqslant p_1(x)\leqslant Cp_2(x)$.
        \thm Очевидно, что это отношение эквивалентности.
        \thm Если $p_1\sim p_2$ и $x_n\overset{p_1}\longrightarrow a$, то $x_n\overset{p_2}\longrightarrow a$.
        \begin{Proof}
            $0\leqslant p_2(x-a)\leqslant\frac1cp_1(x-a)\longrightarrow0$.
        \end{Proof}
        \thm Непрерывность отображения (в частности, линейного оператора) сохраняется, при замене норм на эквивалентные (вне зависимости от того, меняем ли мы норму в области определения или в области прибытия). Тем самым сохранится ограниченность.
        \thm В пространстве $\mathbb R^m$ все нормы эквивалентны.
        \begin{Proof}
            Достаточно доказать, что всякая норма $p$ эквивалента евклидовой $|\cdot|$.\\
            Сначала докажем, что $p$ непрерывна относительно евклидовой нормы. Возьмём $x,y\in\mathbb R^m$.
            \[
            \begin{split}
                |p(x)-p(y)|&\leqslant p(x-y)=p\left(\sum\limits_{k=1}^m(x_k-y_k)e^k\right)\leqslant\\
                &\leqslant\sum\limits_{k=1}^m|x_k-y_k|p(e^k)\overset{\text{К---Б---Ш}}\leqslant\sqrt{\sum\limits_{k=1}^mp^2(e^k)}|x-y|=\mathrm{const}|x-y|
            \end{split}
            \]
            Действительно, непрерывна. Тогда по теореме Вейерштрасса существуют минимум и максимум функции $p$ на единичной сфере. Они подойдут как соответственно $c$ и $C$. Понятно, что они положительны (на сфере норма положительна). И если $x\neq\mathbb0$, то $p(x)=p\left(|x|\frac x{|x|}\right)=|x|p\left(\frac x{|x|}\right)\leqslant C|x|$. Аналогично оценка снизу.
        \end{Proof}
        \begin{Comment}
            Мы доказали лемму для пространства $\mathbb R^m$, но вообще она верна в любом конечномерном пространстве, потому что конечномерное вещественное пространство сводится к $\mathbb R^m$, если перейти к координатам, а с комплексными пространствами можно просто сделать то же самое.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Дифференцируемость и частные производные}.}
    \begin{itemize}
        % TODO: https://en.wikibooks.org/wiki/LaTeX/Rotations
        \dfn Пусть $f\colon \underset{{\subset\mathbb R^n}}D\to\mathbb R^m$, $x\in\Int D$. Если
        $$\exists\scriptA\in\scriptL(\mathbb R^n\to\mathbb R^m)~f(x+h)=f(x)+\scriptA h+o(h);h\to\mathbb 0_n$$
        то отображение $f$ называется \undercolor{red}{дифференцируемым в точке} $x$. При этом оператор $\scriptA$ называется \undercolor{red}{производным оператором} (\undercolor{red}{производным отображением}, \undercolor{red}{производной}) отображения $f$ в точке $x$ и обозначается $f'(x)$.
        \pagebreak
        \begin{Comment}
            $x\in\Int D$ гарантирует нам, что при достаточно малых по норме $h$ $x+h\in D$. Это во многих случаях важно.
        \end{Comment}
        \begin{Comment}
            Напоминание: $\varphi(h)=o(h)\Leftrightarrow\frac{\varphi(h)}{|h|}\underset{h\to\mathbb0_n}\longrightarrow\mathbb0_m$ или $\varphi(h)=\alpha(h)|h|$, где $\alpha(h)\underset{h\to\mathbb0_n}\longrightarrow\mathbb0_m$.\\
           Запись $\varphi(h)=o(|h|)$ значит то же самое.
        \end{Comment}
        \begin{Comment}
            Переформулировка: $\exists\scriptA\in\scriptL(\mathbb R^n\to\mathbb R^m)~\exists\alpha\colon V_{\mathbb0_n}\to\mathbb R^m:\alpha(\mathbb 0_n)=\mathbb0_m$, $\alpha$ непрерывна в нуле $f(x+h)=f(x)+\scriptA h+\alpha(h)|h|$.
        \end{Comment}
        \dfn Слагаемое $f'(x)h$ называется \undercolor{red}{дифференциалом} $f$ в точке $x$ и обозначается $\mathrm df(x;h)$.
        \begin{Comment}
            Тогда $\mathrm df(x;\cdot)=f'(x)$.
        \end{Comment}
        \thm Производная оператора единственна.
        \begin{Proof}
            Если это так, то есть два оператора с разным значением на каких-то двух $h$. Понятно, что $h\neq\mathbb0$.\\
            Итак, пусть $B(x;r)\in D$. Тогда возьмём $t\in\mathbb R~0<|t|<\frac r{|h|}$. Тогда $x+th\in D$. Подставим $th$ в формулу, устремив $t$ у нулю:
            $$
            f(x+th)=f(x)+t\scriptA h+o(t)
            $$
            Тогда $\frac{f(x+th)-f(x)}t=\scriptA h+\frac{o(t)}t\longrightarrow\scriptA h$. А это значит, что $\scriptA h$ --- это чей-то предел, а предел, как мы знаем, единственный.
        \end{Proof}
        \dfn Матрица линейного оператора $f'$ называется \undercolor{red}{матрицей Якоби} отображения $f$ в точке $x$.
        \begin{Comment}
            Дальше мы хотим рассмотреть два частных случая: $n=1$ и $m=1$. Сначала $m=1$. Тогда, понятно, $A$ --- вектор-строка. Равна она пусть $\matr{a_1&\cdots&a_n}$. Что такое $\scriptA h$? Это $\dotprod Ah=\sum\limits_{k=1}^na_kh_k$.
        \end{Comment}
        \dfn Функция $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R$, $x\in\Int D$. Тогда $f$ дифференцируема в точке $x$, если $\exists a\in\mathbb R^m~f(x+h)=f(x)+\dotprod{a}{h}+o(h);h\to\mathbb0_n$. $a$ называется \undercolor{red}{градиентом} $f$ и обозначается $\grad f$ или $\nabla f$.
        \thm Дифференцируемость отображения равносильна дифференцируемости всех его координатных функций.
        \begin{Proof}
            Обозначим координатные функции так: $f=\matr{f_1&\cdots&f_m}$.\\
            Пусть $f$ дифференцируемо в точке $x$, то есть $f(x+h)=f(x)+\scriptA h+o(h)$. При фиксированных значениях всего это равенство --- равенство $m$-мерных векторов. Запишем же его покоординатно: $f_i(x+h)=f_i(x)+\scriptA_i h+(o(h))_i$. Ну так лол, не дифференцируемость ли это координатных функций?\\
            В обратную сторону --- то же самое.
        \end{Proof}
        \thm Из доказательства выше строки матрицы Якоби --- это градиенты координатных функций.
        \begin{Comment}
            Теперь частный случай $n=1$. Тогда переменная одна, а координатных функций $m$ штук. Ну так лол, тогда всё сводится к дифференцируемости функции одной переменной, а производная функции одной переменной --- предел разностного отношения.
        \end{Comment}
        \dfn Пусть $f\colon\underset{\subset\mathbb R}\ab\to\mathbb R$, $x\in\ab$. Тогда $f'(x)=\lim\limits_{h\to\mathbb0}\frac{f(x+h)-f(x)}h$.
        \begin{Comment}
            Далее при $n=1$ игнорируем $x\in\Int D$, а считаем, что с концом промежутка $x$ тоже может совпасть.
        \end{Comment}
        \thm Если $f$ дифференцируема в точке $x$, то $f$ непрерывна в точке $x$. Обратное неверно.
        \begin{Proof}
            Неверность обратного мы уже знаем, следствие прямо --- вот:
            $$
            f(x+h)=f(x)+\scriptA h+o(h)\underset{h\to\mathbb0_n}\longrightarrow f(x)
            $$
        \end{Proof}
        % TODO: rotate
        \dfn Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R^m$. Пусть $D_1$ --- множество точек $D$, в которых $f$ дифференцируема. Тогда \undercolor{red}{производное отображение} $f$ или \undercolor{red}{производная} $f$ --- отображение $\colon D_1\to\scriptL(\mathbb R^n\to\mathbb R^m)$, сопоставляя точке $x$ оператор $f'(x)$.
        \dfn Отображение \undercolor{red}{дифференцируемо на множестве} (обычно открытом), если дифференцируемо в каждой его точке.
        \begin{Example}
            Пусть $f(x)=C=\mathrm{const}$. Тогда $f(x+h)=f(x)+\Theta h+0$ --- матрица Якоби нулевая ($m\times n$).
        \end{Example}
        \begin{Example}
            Пусть $f(x)=\scriptA\in\scriptL(\mathbb R^n\to\mathbb R^m)$. Тогда $\scriptA(x+h)=\scriptA x+\scriptA h+0$. То есть линейный оператор в любой точке дифференцируем и его производная равна ему самому.\\
            Никакого противоречия с одномерным случаем нет --- линейный оператор $\mathbb R\to\mathbb R$ --- это функция $f(x)=ax$, и её производная равна $a$.
        \end{Example}
        % TODO: rotate
        \thm \undercolor{darkgreen}{Линейность дифференцирования}. Пусть $f,g\colon\underset{\subset\mathbb R^n}D\to\mathbb R^m$, $\lambda\in\mathbb R$. Пусть $x\in\Int D$. Если $f$ и $g$ дифференцируемы в точке $x$, то $f+g$ и $\lambda f$ также дифференцируемы и $(f+g)'=f'+g'$ и $(\lambda f)'=\lambda f'$.
        \begin{Proof}
            \[\begin{aligned}
                f(x+h)&=f(x)+f'(x)h+o(h)\\
                g(x+h)&=g(x)+g'(x)h+o(h)
            \end{aligned}\]
            Ну так сложим эти равенства. Получим
            $$
            (f+g)(x+h)=(f+g)(x)+(f'+g')(x)h+o(h)
            $$
            Это значит, что $f+g$ дифференцируемо и что $(f+g)'=f'+g'$. Аналогично с умножением на константу.
        \end{Proof}
        \thm По индукции линейность верна для нескольких слагаемых.
        \thm $\mathrm d(f+g)(x;h)=\mathrm df(x;h)+\mathrm dg(x;h)$ и $\mathrm d(\lambda f)(x;h)=\lambda \mathrm df(x;h)$.
        \thm \undercolor{darkgreen}{Производная композиции}. Пусть $f\colon\underset{\subset \mathbb R^n}D\to\mathbb R^m$, $f\colon\underset{\subset \mathbb R^m}E\to\mathbb R^l$, $f(D)\subset E$. Пусть $x\in\Int D$, $f(x)\in\Int E$, $f$ дифференцируемо в $x$, $g$ дифференцируемо в $f(x)$. Тогда $g\circ f$ дифференцируемо в $x$ и $(g\circ f)(x)=g'(f(x))\cdot f'(x)$.
        \begin{Proof}
            Пусть $y=f(x)$.
            \[\begin{aligned}
                f(x+h)&=f(x)+f'(x)h+\alpha(h)|h|\\
                g(y+k)&=g(y)+g'(y)h+\beta(k)|k|
            \end{aligned}\]
            Где $\alpha$ и $\beta$ в нулях равны нулю и непрерывны.
            Возьмём $k=k(h)=f'(x)h+\alpha(h)|h|$. Получим:
            \[
            \begin{split}
                g(f(x+k))=g(f(x))&+g'(f(x))f'(x)h+\\
                &+g'(y)\alpha(h)|h|+\beta(k)|k|
            \end{split}
            \]
            Обозначим нижнюю часть как $\gamma(h)=g'(y)\alpha(h)+\beta(k)\frac{|k|}{|h|}$. Если мы скажем, что $\gamma(h)$ также в нуле равна нулю и непрерывна, то докажем теорему. Ну, $k(h)$, понятно, в нуле равна нулю и непрерывна.\\
            Итак, $|\gamma(h)|\leqslant\|g'(y)\|\underbrace{|\alpha(h)|}_{\text{б.м.}}+\underbrace{|\beta(k)|}_{\text{б.м.}}(\|f'(x)\|+|\alpha(h)|)\underset{h\to\mathbb0}\longrightarrow0$
        \end{Proof}
        \thm $\mathrm d(g\circ f)=\mathrm df\cdot\mathrm dg$ (тут каждый из дифференциалов справа --- в своей точке).
        \thm Правило по индукции распространяется на несколько отображений.
        \begin{Example}
            Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R^m$, $x,h\in\mathbb R^m$, $\forall t\in\ab~x+th\in D$. Положим $F(t)=f(x+th)$, то есть $F\colon\ab\to\mathbb R^m$. Хочется её дифференцировать. Ну, если $f$ дифференцируемо в $x+th$, то $F'(t_0)=f'(x+t_0h)h$.
        \end{Example}
        \begin{Comment}
            В одномерном случае было дифференцирование произведения. Что будет тут? Можно скаляр умножать на вектор, а можно брать скалярное произведение двух векторных функций.
        \end{Comment}
        \thm \undercolor{darkgreen}{Произведение скалярной и векторной функций}. Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R^m$, $\lambda\colon D\subset\mathbb R$. Пусть $f$, $\lambda$ дифференцируемы в точке $x\in\Int D$. Тогда $f\lambda$ дифференцируемо в точке $x$ и
        $$
        (f\lambda)'(x)h=(\lambda'(x)h)f(x)+\lambda(x)f'(x)h
        $$
        \begin{Comment}
            Очень сложно, нихуя не понятно. Ну, $\lambda'(x)$ --- $n$-мерный вектор-строка, $h$ --- $n$-мерный вектор-столбец, $f(x)$ --- $m$-мерный вектор. Поэтому первое слагаемое --- $m$-мерный вектор. Второе, если расписать, тоже $m$-мерный вектор, и левая часть также $m$-мерный вектор.
        \end{Comment}
        \begin{Proof}
            \[
            \begin{split}
                \lambda(x+h)f(x+h)-\lambda(x)f(x)&=(\lambda(x+h)-\lambda(x))f(x+h)+\lambda(x)(f(x+h)-f(x))=\\
                &=(\lambda'(x)h+o(h))f(x+h)+\lambda(x)(f'(x)h+(h))=\\
                &=\lambda'(x)hf(x+h)+\lambda(x)f'(x)h+\underbrace{o(h)f(x+h)+o(h)\lambda(x)}_{o(h)}
            \end{split}
            \]
        \end{Proof}
        \thm \undercolor{darkgreen}{Производная скалярного произведения}. Пусть $f;g\colon\underset{\subset\mathbb R^n}D\to\mathbb R^m$ --- дифференцируемы в $x\in\Int D$. Тогда $\dotprod{f}{g}$ дифференцируемо в $x$ и 
        $$\dotprod{f}{g}'(x)h=\dotprod{f'(x)h}{g(x)}+\dotprod{f(x)}{g'(x)h}$$
        \begin{Proof}
            $\dotprod{f}{g}=\sum\limits_{i=1}^mf_ig_i$. Тогда
            \[
            \begin{split}
                \dotprod{f}{g}'(x)h&\overset{\text{линейность}}{=}\sum\limits_{i=1}^m(f_ig_i)'(x)h\overset{f_i=\mathrm{const}}=\\
                &=\sum\limits_{i=1}^mf_i'(x)hg_i(x)+f_i(x)g_i'(x)h=\dotprod{f'(x)h}{g(x)}+\dotprod{f(x)}{g'(x)h}
            \end{split}
            \]
        \end{Proof}
        \thm В случае $n=1$ получаем $\dotprod fg'(x)=\dotprod{f'}g(x)+\dotprod f{g'}(x)$.
        \begin{Comment}
            Частное нам дифференцировать тупа лень, а вот с отображением обратной функции всё довольно сложно, и на эту тему будет отдельный параграф.
        \end{Comment}
        \thm \undercolor{darkgreen}{Теорема Лагранжа для вектор-функции одной переменной}. Пусть $f\colon[a;b]\to\mathbb R^m$ --- непрерывна на $[a;b]$, дифференцируема на $(a;b)$. Тогда $\exists c\in(a;b)~|f(b)-f(a)|\leqslant|f'(c)|(b-a)$.
        \begin{Comment}
            Что такое $f'(c)$? Это линейный оператор из $\mathbb R^1$ в $\mathbb R^m$. А это значит, что вместо $|f'(c)|$ можно писать $\|f'(c)\|$, потому что норма оператора (в случае $\mathbb R\to\mathbb R^n$) совпадает с нормой его как вектора.
        \end{Comment}
        \begin{Proof}
            Рассмотрим $\varphi(t)=\dotprod{f(t)}{f(b)-f(a)}$. Это обычная числовая функция, которая также дифференцируема на $(a;b)$ и непрерывна на $[a;b]$. Тогда по одномерной формуле Лагранжа $\exists c\in(a;b)~\varphi(b)-\varphi(a)=\varphi'(c)(b-a)$. Тогда
            \[
            |f(b)-f(a)|^2=\dotprod{f'(c)}{f(b)-f(a)}(b-a)\overset{\text{К---Б---Ш}}\leqslant|f'(c)||f(b)-f(a)|(b-a)
            \]
            В случае $f(b)=f(a)$ ясно, иначе можно сократить неравенство.
        \end{Proof}
        \thm Равенство может не достигаться нигде.
        \begin{Example}
            $f(t)=\matr{\cos t\\\sin t}$, $[a;b]=[-\pi;\pi]$. Очевидно, $f(b)-f(a)=\matr{0\\0}$. А $f'(t)=\matr{-\sin t\\\cos t}$, то есть $|f'(t)|=1$. Увы и ах, вне зависимости от $c$ не получится равенство $0=1\cdot 2\pi$.
        \end{Example}
        \thm Теорема Лагранжа для отображений. Пусть $D\subset\mathbb R^n$ открыто, $f\colon D\to\mathbb R^m$ дифференцируема. Пусть $a;b\in\mathbb R^n$, $\overline{a,b}\subset D$. Тогда
        $$
        \exists\theta\in(0;1)~f(b)-f(a)\leqslant\|f'(a+\theta(b-a))\||b-a|
        $$
        \begin{Proof}
            Пусть $F(t)=f(a+t(b-a))$, $t\in[0;1]$. Она, как мы проверяли в одном из примеров, имеет производную, а значит по утверждению выше
            $$\exists\theta\in(0;1)~F(1)-F(0)\leqslant\|F'(\theta)\|(1-0)$$
            Это равносильно
            $$f(b)-f(a)\leqslant\|f'(a+\theta(b-a))(b-a)\|\leqslant\|f'(a+\theta(b-a))\||b-a|$$
            Что и требовалось доказать.
        \end{Proof}
        \thm \undercolor{darkgreen}{Оценка приращения отображения}. Если в условиях утверждения выше $\forall t\in[0;1]~\|f'(a+t(b-a))\|\leqslant M$, то $|f(b)-f(a)|\leqslant M|b-a|$.
        \begin{Comment}
            Если множество $D$ выпукло, то по определению выпуклости $\forall a,b\in D~\overline{a,b}\subset D$. В таком случае формулу можно применять к любой паре точек множества $D$.
        \end{Comment}
        \begin{Comment}
            $$|f(x+h)-f(x)|\leqslant\|f'(x+\theta h)\||h|$$
        \end{Comment}
        \begin{Comment}
            Если $D$ открыто и выпукло в $\mathbb R^n$, $f$ дифференцируема на $D$ и $\exists M>0~\forall u\in D~\|f'(u)\|\leqslant M$. Тогда $\forall x,x+h\in D~|f(x+h)-f(x)|\leqslant M|h|$, то есть $f$ принадлежит классу Липшица c постоянной $M$.
        \end{Comment}
        \begin{Comment}
            Что ж, теперь посмотрим, к чему приводит обобщение определения производной через предел разностного отношения.
        \end{Comment}
        \dfn Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R$, $x\in\Int D$, $h\in\mathbb R^n$. Тогда
        $$
        \lim\limits_{t\to0}\frac{f(x+th)-f(x)}t
        $$
        называется \undercolor{red}{производной функции} $f$ в точке $x$ \undercolor{red}{по вектору} $h$. Обозначается $D_hf(x)$ или $\frac{\partial f(x)}{\partial h}$ либо $\frac{\partial f}{\partial h}(x)$. Тогда $f$ называется \undercolor{red}{дифференцируемой по вектору} $h$, если $D_hf(x)$ существует и конечно.
        \begin{Comment}
            Если $h=0$, то неинтересно, всегда $0$ получается.\\
            Как вообще можно на это определение посмотреть? Например, так: мы сужаем функцию на прямую, получаем одномерную функцию и берём её производную.
        \end{Comment}
        \dfn Вектор единичной длины также называют \undercolor{red}{направлением}.
        \begin{Comment}
            Возьмём уже знакомую нам функцию $F_h(t)=f(x+th)$. Тогда, несложно заметить, $D_hf(x)=F_h'(0)$. Более того, это равно $\dotprod{\grad f(x)}h$
        \end{Comment}
        \thm \undercolor{darkgreen}{Производная дифференцируемой функции по вектору}. Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R$ дифференцируема в точке $x\in\Int D$. Тогда
        $\forall h\in\mathbb R^n~f$ дифференцируема по вектору $h$ в точке $x$ и
        $$
        D_hf(x)=f'(x)h
        $$
        \begin{Proof}
            $$
            D_hf(x)=F_h'(0)=f'(x)h
            $$
        \end{Proof}
        \thm \undercolor{darkgreen}{Экстремальное свойство градиента}. Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R$ дифференцируема в точке $x\in\Int D$. Пусть $\grad f(x)\neq\mathbb0$. Тогда
        $$
        \forall h\in\mathbb R^n:|h|=1~-|\grad f(x)|\leqslant D_hf(x)\leqslant|\grad f(x)|
        $$
        Причём равенства верны только при $h=\mp\frac{\grad f(x)}{|\grad f(x)|}$.
        \begin{Comment}
            Это можно трактовать так: градиент показывает то направление, в котором функция растёт быстрее всего.
        \end{Comment}
        \begin{Proof}
            $$|D_hf(x)|=|\dotprod{\grad f(x)}h|\overset{\text{К---Б---Ш}}\leqslant|\grad f(x)|1$$
            А неравенство К---Б---Ш обращается в равенство только когда вектора коллинеарны.
        \end{Proof}
        \dfn Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R$, $x\in\Int D$, $k\in[1:n]$. Производная $D_{e_k}f$ называется \undercolor{red}{частной производной} функции $f$ по $k$-тому аргументу. Обозначения: $D_kf(x)$, $\frac{\partial f}{\partial x_k}(x)$, $\frac{\partial f(x)}{\partial x_k}$, $f'_{x_k}(x)$, иногда даже $f'_k(x)$.
        \thm \undercolor{darkgreen}{Частные производные дифференцируемой функции}. Если $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R$ дифференцируема в точке $x\in\Int D$, то $\forall k\in[1:n]~D_k(f)=(\grad f(x))_k$.
        \thm Пусть $\varphi_k(u)=f(x_1;\ldots;x_{k-1};u;x_{k+1};\ldots;x_n)$. Тогда $D_k(x)=\varphi'_k(x_k)$.
        \begin{Comment}
            То есть частные производные искать очень легко --- надо находить производные функции одной переменной.
        \end{Comment}
        \thm \undercolor{darkgreen}{Структура матрицы Якоби и градиента}. Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R^m$ дифференцируема в точке $x\in\Int D$, тогда матрица Якоби выглядит так:
        \[\matr{D_1f_1(x) & \cdots & D_nf_1(x)\\\vdots & \ddots & \vdots\\D_1f_m(x) & \cdots & D_nf_m(x)}\]
        В частности при $m=1$
        \[\grad f(x)=\matr{D_1f(x)&\cdots&D_nf(x)}\]
        \begin{Proof}
            То что строками матрицы Якоби являются градиенты координатный функций, мы доказали в комментариях к определению дифференцируемости, а как устроен градиент мы доказали только что.
        \end{Proof}
        \thm Для функции $f$ $\mathrm df(x;h)=\sum\limits_{k=1}^nD_kf(x)h_k$.
        \thm \undercolor{darkgreen}{Правило цепочки в координатах}. Пусть $f\colon\underset{\subset R^n}D\to\mathbb R^m$, $g\colon\underset{\subset R^m}E\to\mathbb R^l$, $f(D)\subset E$, $f$ дифференцируема в $x\in\Int D$, $g$ дифференцируема в $f(x)\in\Int E$. Тогда
        \[\begin{split}
            \forall k\in[1:n]~\forall j\in[1:l]~&\\
            &\frac{\partial(g\circ f)_j(x)}{\partial x_k}=\sum\limits_{i=1}^m\frac{\partial g_j(f(x))}{\partial y_i}\frac{\partial f_i(x)}{\partial x_k}\\
            &D_k(g\circ f)_j(k)=\sum\limits_{i=1}^nD_ig_j(f(x))D_kf_i(x)
        \end{split}\]
        \begin{Proof}
            Мы знаем, что $(g\circ f)'(x)=g'(f(x))f'(x)$. А тут мы просто два раза записали это в координатах, зная элементы матрицы Якоби и правило умножения матриц (строка на столбец).
        \end{Proof}
        \begin{Comment}
            Ещё это можно записать так: $f'_z(x;y;z)$. Только надо понимать, что $z$ в нижнем индексе --- номер переменной, по которой дифференцируем, а $z$ в аргументах --- точка, где берём производную.
        \end{Comment}
        \begin{Comment}
            Как и в одномерном случае, приращение (которое мы обозначали $h$) нередко обозначают за $\mathrm dx$. Но это также обычный $n$-мерный вектор. В таких обозначениях
            \[
            \mathrm dg(y;\mathrm dy)=\sum\limits_{i=1}^m\frac{\partial g(y)}{\partial y_i}\mathrm dy_i
            \]
            Но на самом деле данная формула верна также если трактовать $y$ как $f(x)$, а $\mathrm dy$ как $\mathrm df(x)$. Ну, действительно, ведь, $\mathrm dy_i=\sum\limits_{k=1}^n\frac{\partial y_i}{\partial x_k}\mathrm dx_k$, и отсюда всё выводится.\\
            Это свойство называют инвариантность формы дифференциала.
        \end{Comment}
        \begin{Example}
            $$f(x;y)=\begin{cases}
                0 & x=0\lor y=0\\
                1 & \mathrm{otherwise}
            \end{cases}$$
            Понятно, что её частные производные в нуле равны 0. Любые другие частные производные не существуют, потому что взяв эту функцию по любому другому направлению, найдём разрыв в нуле. Отсюда 2 вывода:
            \begin{itemize}
                \item наличие частных производных в точке не влечёт не только дифференцируемости, но и непрерывности.
                \item Наличие производных по одним направлениям не влечёт существование производных по другим.
            \end{itemize}
        \end{Example}
        \begin{Example}
            $$f(x;y)=\begin{cases}
                1 & x>0,y=x^2\\
                0 & \mathrm{otherwise}
            \end{cases}$$
            Тогда в начале координат по любому вектору производная существует и равна 0. Отсюда вывод: даже наличие производных по всем направлениям не влечёт дифференцируемости функции в точке. В данном случае она вообще разрывна.
        \end{Example}
        \begin{Comment}
            Эти два примера выше дают нам нечто неприятное --- во всех приложениях нужна дифференцируемость. Производную мы можем легко найти, но только если она есть, а доказывать дифференцируемость мы пока не умеем.\\
            Так давайте научимся.
        \end{Comment}
        \thm \undercolor{darkgreen}{Дифференцируемость функции с непрерывными частными производными}. Пусть $f\colon\underset{\subset\mathrm R^n}D\to\mathbb R^m$, $x\in\Int D$, $\forall k\in[1:n]~D_kf$ существует в окрестности $x$ и непрерывна в самой $x$. Тогда $f$ дифференцируема в $x$.
        \begin{Proof}
            Будем проверять определение дифференцируемости. $f(x+h)=f(x)+f'(x)h+o(h)$. Мы уже знаем, чему должно быть равно $f(x)$. Подставим это:
            $$
            f(x+h)=f(x)+\sum\limits_{k=1}^nD_kf(x)h_k+o(h)
            $$
            Это мы хотим доказать. Пусть $R(h)=f(x+h)-f(x)-\sum\limits_{k=1}^nD_kf(x)h_k$. Требуется проверить, что $R(h)=o(h)$. Мы считаем, что $h$ достаточно мало, чтобы попадать в указанную окрестность $x$.\\
            Давайте сделаем вот что: $v^k=\sum\limits_{j=1}^kh_je^j$. Тогда $v^0=\mathbb 0_k$, $v^n=h$. То есть мы хотим двигаться не вдоль $h$, а вдоль ортов. Хорошо, что тогда такое $f(x+h)-f(x)$? Это $\sum\limits_{k=1}^n(f(x+v^k)-f(x+v^{k-1}))$ (телескопическая сумма). В таком случае
            $$
            R(h)=\sum\limits_{k=1}^nf(x+v^k)-f(x+v^{k-1})-D_kf(x)h_k
            $$
            Пусть $\varphi_k(t)=f(x+v^{k-1}+th_ke^k)$, $t\in[0;1]$. То есть мы \textit{медленно} преобразуем $v^{k-1}$ в $v^k$. $\varphi_k$ дифференцируема на $[0;1]$ потому что $\varphi'_k(t)=D_kf(x+v^{k-1}+th_ke^k)h_k$. А значит у ней можно применить формулу Лагранжа: $\exists\theta_k\in(0;1)~\varphi_k(1)-\varphi_k(0)=\varphi_k'(\theta_k)\Leftrightarrow f(x+v^k)-f(x+v^{k-1})=D_kf(x+v^{k-1}+\theta_k h_ke^k)h_k$. Теперь это можно подставить в $R(x)$:
            \[
            \begin{split}
                R(h)&=\sum\limits_{k=1}^nD_kf(x+v^{k-1}+\theta_k h_ke^k)h_k-D_kf(x)h_k=\\
                &=\sum\limits_{k=1}^n\gamma_k(h)h_k\mid\gamma_k(h)=D_kf(x+v^{k-1}+\theta_k h_ke^k)-D_kf(x)
            \end{split}
            \]
            Поскольку $D_kf$ непрерывна в $x$, $\gamma_k(h)\underset{h\to\mathbb0_n}\longrightarrow0$. А отсюда по К---Б---Ш
            $$
            |R(h)|\leqslant\underbrace{\sqrt{\sum\limits_{k=1}^n\gamma_k^2(h)}}_{\to0}|h|
            $$
        \end{Proof}
        \begin{Example}
            Является ли условие теоремы выше необходимым? Нет, не является.
            $$
            f(x;y)=\begin{cases}
                x^2+y^2 & \text{ровно одно из }x\text{ и }y\text{ рационально}\\
                0 & \mathrm{otherwise}
            \end{cases}
            $$
            $f(x;y)=\underbracket{f(0;0)}_0+0+o(\sqrt{x^2+y^2})$. То есть $f$ дифференцируема в нуле. И её частные производные равны 0. Но в других точек частных производных $f$ не имеет, не говоря уже ничего про непрерывность. Почему не имеет? Ну, потому что. Вне зависимости от рациональности $x$ и $y$ есть точки с рациональной абсциссой и есть с иррациональной, а значит частная производная по $x$ не существует так как сужение функции на любую горизонтальную прямую разрывно.
        \end{Example}
        \begin{Example}
            $$
            f(x;y)=\begin{cases}
                (x^2+y^2)\sin\frac1{x^2+y^2} & (x;y)\neq(0;0)\\
                0 & (x;y)=(0;0)
            \end{cases}
            $$
            В нуле $f$ тоже дифференцируема и имеет нулевые частные производные. А во всех остальных точках можно производные находить по обычным правилам:
            \[
            \begin{split}
                f'_x(x;y)&=2x\sin\frac1{x^2+y^2}+(x^2+y^2)\cos\frac1{x^2+y^2}\cdot\frac{-2x}{(x^2+y^2)^2}=\\
                &=2x\sin\frac1{x^2+y^2}\frac{-2x}{x^2+y^2}\cos\frac1{x^2+y^2}
            \end{split}
            \]
            \[
            f'_y(x;y)=2y\sin\frac1{x^2+y^2}\frac{-2y}{x^2+y^2}\cos\frac1{x^2+y^2}
            \]
            То есть $f'_y$ и $f'_x$ непрерывны на $\mathbb R^2\setminus\{0\}$, то есть там есть производная $f$. А что с непрерывностью частных производных в нуле? Запишем для $x\neq0$, чему равно $f'_x(x;0)=2x\sin\frac1{x^2}-\frac2x\cos\frac1{x^2}$. То есть $f'_x\left(\frac1{\sqrt{2\pi n}};0\right)=-2\sqrt{2\pi n}\underset{n\to\infty}\longrightarrow-\infty$.
        \end{Example}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Частные производные высших порядков}. \undercolorblack{orange}{Формула Тейлора}.}
    \begin{itemize}
        \dfn Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R^m$, $x\in\Int D$, $k\in[1:n]$. Пусть $D_kf$ существует в окрестности точки $x$. Тогда определена функция $D_kf\colon V_x\to\mathbb R$. Если $j\in[1:n]$, то $D_j(D_kf)(x)$ называется \undercolor{red}{частной производной} $f$ \undercolor{red}{второго порядка} по $k$-той по $j$-той переменной в точке $x$. Обозначается $D_{kj}^2f(x)$. Другие обозначения: $\frac{\partial^2f(x)}{\partial x_k\partial x_j}$, $\frac{\partial^2f}{\partial x_k\partial x_j}(x)$, $f''_{x_kx_j}(x)$, $f''_{kj}(x)$.
        \dfn Пусть $r-1\in\mathbb N$. Пусть частные производные $r-1$-го порядка определены. Тогда частной производной $r$-того порядка называется
        $$
        D^r_{i_1\cdots i_r}f(x)=D_{i_r}(D^{r-1}_{i_1\cdots i_{r-1}}f)(x)
        $$
        Существование $D^r_{i_1\cdots i_r}f(x)$ влечёт существование $D^{r-1}_{i_1\cdots i_{r-1}}f$ в окрестности $x$.
        \begin{Comment}
            Вопрос: важно ли, в каком порядке мы дифференцируем, сначала по $x_k$, а потом по $x_j$ или наоборот? Ну, смотрите. для одночленов $f(x;y)=x^\alpha y^\beta$ получается одно и то же, значит для многочленов тоже, но в общем случае к сожалению нет:
        \end{Comment}
        \begin{Example}
            $$
            f(x;y)=\begin{cases}
                xy\frac{x^2-y^2}{x^2+y^2} & (x;y)\neq(0;0)\\
                0 & (x;y)=(0;0)
            \end{cases}
            $$
            Чтобы найти смешанные производные нужно найти производные первого порядка, причём не только в нуле, а в окрестности.\\
            При $(x;y)\neq(0;0)$
            \[
            \begin{split}
                f'_x(x;y)&=y\frac{x^2-y_2}{x^2+y^2}+xy\frac{2x(x^2+y^2)-2x(x^2-y^2)}{(x^2+y^2)^2}=\\
                &=y\frac{(x^2-y_2)(x^2+y^2)+4x^2y^2}{(x^2+y^2)^2}=y\frac{x^4-y^4+4x^2y^2}{(x^2+y^2)^2}
            \end{split}
            \]
            \[
            f'_y(x;y)=-x\frac{y^4-x^4+4x^2y^2}{(x^2+y^2)^2}
            \]
            При $(x;y)=(0;0)$ $f'_x(0;0)=f'_y(0;0)=0$.\\
            Теперь посмотрим на частные производные второго порядка, которые будем считать только в нуле. Тогда $f'_x(0;y)=-y$, $f'_y(x;0)=x$, причём как для $(x;y)\neq0$, так и для $(x;y)=0$. В таком случае $f''_{xy}=-1$, $f''_{yx}=1$, они не равны.
        \end{Example}
        \begin{Comment}
            Какой-то очень искусственный пример, не находите?
        \end{Comment}
        \thm \undercolor{darkgreen}{Независимость частных производных второго порядка от очерёдности дифференцирования}. Пусть $f\colon\underset{\subset\mathbb R^2}D\to\mathbb R$, $(x^0;y^0)\in\Int D$. Пусть смешанные производные $f''_{xy}$ и $f''_{yx}$ существует в окрестности $(x^0;y^0)$ существуют в $V_{(x^0;y^0)}$ и непрерывны в $(x^0;y^0)$. Тогда $f''_{xy}(x^0;y^0)=f''_{yx}(x^0;y^0)$.
        \begin{Proof}
            Пусть $h$ --- приращение по $x$, $k$ --- приращение по $y$. И пусть $(x^0+h;y^0+k)\in V_{(x^0;y^0)}$.\\
            Тогда пусть $\Delta=f(x^0+h;y^0+k)-f(x^0+h;y^0)-f(x^0;y^0+k)+f(x^0;y^0)$ (смешанная разность). Давайте двумя разными способами сгруппируем слагаемые и преобразуем.\\
            Пусть $\varphi(s)=f(s;y^0+k)-f(s;y^0)$. Тогда $\Delta=\varphi(x^0+h)-\varphi(x^0)$. Функция $\varphi$ дифференцируема на отрезке $[x^0;x^0+h]$ потому что есть производные второго порядка, а значит есть и первого. По формуле Лагранжа
            $$
            \exists\theta_1\in(0;1)~\Delta=\varphi'(x_0+\theta_1h)h=(f'_x(x^0+\theta_1h;y^0+k)-f'_x(x^0+\theta_1h;y^0))h
            $$
            Ну, введём $\tilde\varphi(t)=f'_x(x^0+\theta_1h;t)$ и по формуле Лагранжа
            $$
            \exists\theta_2\in(0;1)~\Delta=f''_{xy}(x^0+\theta_1h;y^0+\theta_2k)kh
            $$
            Теперь другим способом: $\psi(t)=f(x^0+h;t)-f(x^0;t)$, и тогда $\Delta=\psi(x^0+h;y^0+k)-\psi(x^0+h;y^0)$. Всё ещё $\psi$ дифференцируема на $[y^0;y^0+k]$, а значит
            $$
            \exists\theta_3\in(0;1)~\Delta=\psi'(y^0+\theta_3k)=(f'_y(x^0+h;y^0+\theta_3k)-f'_y(x^0;y^0+\theta_3k))k
            $$
            И снова по формуле Лагранжа
            $$
            \exists\theta_4\in(0;1)~\Delta=f''_{yx}(x^0+\theta_4h;y^0+\theta_3k)kh
            $$
            Давайте приравняем два полученных выражения для $\Delta$:
            $$
            f''_{xy}(x^0+\theta_1h;y^0+\theta_2k)=f''_{yx}(x^0+\theta_4h;y^0+\theta_3k)
            $$
            (на $h$ и $k$ можно делить, они не нулевые). Теперь можно устремить $(h;k)$ к $(0;0)$, и по непрерывности получить 
            $$
            f''_{xy}(x^0;y^0)=f''_{yx}(x^0;y^0)
            $$
        \end{Proof}
        \begin{Comment}
            Осталось распространить это свойство и на функции большего количества переменных, и на производные порядка выше.
        \end{Comment}
        \dfn Пусть $D$ открыто в $\mathbb R^n$, $f\colon D\to\mathbb R$, $r\in\mathbb N$. Тогда $f$ называется \undercolor{red}{$r$ раз непрерывно дифференцируемой} (или \undercolor{red}{$r$-гладкой}) на множестве $D$, если у неё есть все частные производные до $r$-того порядка включительно, и все они непрерывны на $D$.
        \dfn Отображение $f\colon D\to\mathbb R^m$ называется \undercolor{red}{$r$ раз непрерывно дифференцируемым} (или \undercolor{red}{$r$-гладкым}), если все его координатные функции таковы. Обозначение --- $C^{(r)}(D)$ для функций и $C^{(r)}(D\to\mathbb R^m)$.
        \dfn Договоримся, что $f^{(0)}=f$, $C^{(0)}=C$.
        \dfn \undercolor{red}{Бесконечно дифференцируемые функции/отображения} --- $C^{(\infty)}(D)=\bigcap\limits_{r=0}^\infty C^{(r)}(D)$.
        \begin{Comment}
            Техническое замечание: разумеется в определении достаточно потребовать существования и непрерывности частных производных не любого порядка до $r$, а порядка ровно $r$. Почему? Ну, если частные производные порядка $r$ существуют и непрерывны, то порядка $r-1$ существуют и имеют непрерывные частные производные, следовательно дифференцируемы, а значит и непрерывны. Дальше по обратной индукции.
        \end{Comment}
        \thm \undercolor{darkgreen}{Независимость частных производных высших порядков от очерёдности дифференцирования}. Пусть $D$ открыто в $\mathbb R^n$, $r-1\in\mathbb N$, $f\in C^{(r)}(D)$. Пусть $i_1;\ldots;i_r\in[1:r]$, $j_1;\ldots;j_r$ --- перестановка $i_1;\ldots;i_r$. Тогда $\forall x\in D~D^{r}_{i_1;\ldots;i_r}f(x)=D^{r}_{j_1;\ldots;j_r}f(x)$.
        \begin{Proof}
            Достаточно доказать теорему для элементарной перестановки (т.е. элементарной транспозиции), поскольку всякая перестановка является композицией конечного набора таковых.\\
            Итак, посуть мы поменяли местами два соседних индекса: $k$-й и $k+1$-й. Если $i_k=i_{k+1}$, доказывать нечего. Иначе надо доказать, что $D^{r}_{\cdots i_ki_{k+1}\cdots}f(x)=D^{r}_{\cdots i_{k+1}i_{k}\cdots}f(x)$.\\
            За первые $k-1$ раз ничего интересного не происходит. Продифференцируем $k-1$ раз и введём новую функцию двух переменных:
            $$
            \varphi(s;t)=D^{k-1}_{i_1\cdots i_{k-1}}f(x_1;\ldots;x_{i_k-1};s;x_{i_k+1};\cdots;x_{i_{k+1}-1};t;x_{i_{k+1}+1};\cdots;x_n)
            $$
            Ну, а теперь воспользуемся предыдущей теоремой о функции двух переменных. Мы знаем, что $\varphi''_{st}(x_{i_k};x_{i_{k+1}})=\varphi''_{ts}(x_{i_k};x_{i_{k+1}})$. То есть $D^{k+1}_{i_1;\ldots;i_k;i_{k+1}}f(x)=D^{k+1}_{i_1;\ldots;i_{k+1};i_k}f(x)$. Ну и всё, потом мы дифференцируем левую и правую часть $r-k-1$ раз, получаются одинаковые функции.
        \end{Proof}
        \begin{Comment}
            Можно заметить, что чистые производные не играли никакой роли в первой теореме, впрочем, и тут нам также нужна непрерывность и существование не всех частных производных, а только тех, которые мы используем в доказательстве.
        \end{Comment}
        \thm Алгебраические операции и композиции не выводят из класса $C^{(r)}$.
        \begin{Proof}
            Для алгебраических операций это очевидно, так как алгебраические операции осуществляются с частными производными.\\
            Для композиции воспользуемся правилом цепочки:
            $$
            D_k(g\circ f)=\sum\limits_{i=1}^m(D_ig\circ f)D_kf_i
            $$
            Смотрите-смотрите, индукция, $(D_ig\circ f)$ --- композиция как минимум $r-1$-гладких функций.
        \end{Proof}
        \begin{Comment}
            В условиях теоремы о независимости частных производных высших порядков от очерёдности дифференцирования можно писать что-нибудь вида $f^{(5)}_{x^3y^2}(x;y)$ или $\frac{\partial^rf(x)}{\partial x_1^{k_1}\cdots\partial x_n^{k_n}}$, где $k_1+\cdots+k_n=r$.
        \end{Comment}
        \dfn $k=\matr{k_1 & \cdots & k_n}\in\mathbb Z^n_+$ называется \undercolor{red}{мультииндексом}, а $(k)=k_1+\cdots+k_n$ --- его \undercolor{red}{высотой}.
        \begin{Comment}
            И в дополнение к предыдущему комментарию также есть обозначение $f^{(k)}(x)=f^{(k_1;\ldots;k_n)}(x)$.
        \end{Comment}
        \dfn Если $h\in\mathbb R^n$, $k\in\mathbb Z^n_+$, то ${\color{red}\underline{{\color{black}h^k}}}=h_1^{k_1}\cdots h_n^{k_n}$, ${\color{red}\underline{{\color{black}k!}}}=k_1!\cdots k_n!$.
        \thm Пусть $r\in\mathbb Z_+$, $D$ открыто в $\mathbb R^n$, $f\in C^{(r)}(D)$, $x,h\in D$, $x,h\in\mathbb R^n$, $\overline{x,x+h}\subset D$, $F(t)=f(x+th)$. Тогда
        $F\in C^{(r)}[0;1]$ и
        $$\forall l\in[0:r]~\forall t\in[0;1]~F^{(l)}(t)=\sum\limits_{(k)=l}\frac{l!}{k!}f^{(k)}(x+th)h^k$$
        \begin{Proof}
            То что $F\in C^{(r)}[0;1]$, мы знаем.\\
            Формула по индукции. База: $l=0$.
            $$
            f(x+th)=f(x+th)
            $$
            Переход. Пусть формула верна для $l$, докажем для $l+1$.
            \[\begin{split}
                F^{(l+1)}(t)=\frac{\mathrm d}{\mathrm dt}F^{(l)}(t)&=\sum\limits_{(k)=l}\frac{l!}{k!}\frac{\mathrm d}{\mathrm dt}f^{(k)}(x+th)h^k=\\
                &\overset{(*)}=\sum\limits_{(k)=l}\frac{l!}{k!}\sum\limits_{i=1}^nD_if^{(k)}(x+th)h_ih^k=\\
                &=\sum\limits_{(k)=l}\frac{l!}{k!}\sum\limits_{i=1}^nf^{(k+e^i)}(x+th)h^{k+e^i}=\\
                &=\sum\limits_{i=1}^n\sum\limits_{(k)=l}\frac{l!}{k!}f^{(k+e^i)}(x+th)h^{k+e^i}\overset{\tilde k=k+e^i}=\\
                &=\sum\limits_{i=1}^n\sum\limits_{\substack{(\tilde{k})=l+1\\\tilde{k_i}\geqslant1}}\frac{l!\tilde k_i}{\tilde k!}f^{(\tilde k)}(x+th)h^{\tilde k}=\\
                &\overset{(**)}=\sum\limits_{(\tilde{k})=l+1}\left(\sum\limits_{i=1}^n\tilde k_i\right)\frac{l!}{\tilde k!}f^{(\tilde k)}(x+th)h^{\tilde k}=\\
                &\overset{(k)=l+1}=\sum\limits_{(\tilde{k})=l+1}(l+1)\frac{l!}{\tilde k!}f^{(\tilde k)}(x+th)h^{\tilde k}=\\
                &=\sum\limits_{(\tilde{k})=l+1}\frac{(l+1)!}{\tilde k!}f^{(\tilde k)}(x+th)h^{\tilde k}
            \end{split}\]
            $(*)$ --- это $\frac{\mathrm d}{\mathrm dt}g(x+th)=\sum\limits_{i=1}^nD_ig(x+th)h_i$\\
            $(**)$: условие $\tilde k_i\geqslant i$ можно убрать. Потому что когда мы его уберём, у нас появятся штуки, где $\tilde k_i=0$, которые дадут нам \textbf{нулевое} слагаемое.
        \end{Proof}
        \thm \undercolor{darkgreen}{Многомерная формула Тейлора с остатком в форме Лагранжа}. Пусть $r\in\mathbb Z_+$, $D$ открыто в $\mathbb R^n$, $f\in C^{(r+1)}(D)$, $x^0;x\in\mathbb R^n$ $\overline{x^0,x}\subset D$. Тогда
        $$
        \exists\theta\in(0;1)~f(x)=\sum\limits_{(k)\leqslant r}\frac{f^{(k)}(x^0)}{k!}(x-x^0)^k+\sum\limits_{(k)=r+1}\frac{f^{(k)}(x^0+\theta(x-x^0))}{k!}(x-x^0)^k
        $$
        \begin{Proof}
            Одной буквой обозначим $x-x^0=h$, чтобы было короче. А ещё то, что называлось $x^0$ теперь будет называться $x$.
            $$
            f(x+h)=\sum\limits_{(k)\leqslant r}\frac{f^{(k)}(x)}{k!}h^k+\sum\limits_{(k)=r+1}\frac{f^{(k)}(x+\theta h)}{k!}h^k
            $$
            Ну, введём $F=f(x+th)\mid t\in(0;1)$. Она принадлежит классу $C^{(r+1)}[0;1]$. По одномерной формуле Тейлора
            $$
            \exists\theta\in(0;1)~F(1)=\sum\limits_{l=0}^r\frac{F^{(l)}(0)}{l!}(1-0)^l+\frac{F^{(r+1)}(\theta)}{(r+1)!}(1-0)^{r+1}
            $$
            Осталось только подставить производные, которые мы нашли в лемме выше.
            \[\begin{split}
                f(x+h)&=\sum\limits_{l=0}^r\frac1{\cancel{l!}}\sum\limits_{(k)=l}\frac{\cancel{l!}}{k!}f^{(k)}(x)h^k+\frac1{\bcancel{(r+1)!}}\sum\limits_{(k)=r+1}\frac{\bcancel{(r+1)!}}{k!}f^{(k)}(x+\theta h)h^k\\
                &\sum\limits_{(k)\leqslant r}\frac{f^{(k)}(x)}{k!}h^k+\sum\limits_{(k)=r+1}\frac{f^{(k)}(x+\theta h)}{k!}h^k
            \end{split}\]
        \end{Proof}
        \dfn $T_{r;x^0}f(x)=\sum\limits_{(k)\leqslant r}\frac{f^{(k)}(x^0)}{k!}(x-x^0)^k$ --- \undercolor{red}{многочлен Тейлора}.
        \dfn $R_{r;x^0}f(x)=f(x)-T_{r;x^0}f(x)$ --- \undercolor{red}{остаток многочлена Тейлора}.
        \begin{Comment}
            Если $D$ выпукло, то формула верна для любой пары точек $x,x^0\in D$.
        \end{Comment}
        \begin{Comment}
            При $r=0$ получаем ещё один вариант формулы Лагранжа:
            $$
            f(x+h)-f(x)=\sum\limits_{k=1}^nD_kf(x+\theta h)h^k
            $$
            Потому что мультииндекс $k$ высоты 1 --- то же самое, что просто индекс.
        \end{Comment}
        \begin{Comment}
            Если применить к $F$ одномерную формулу Тейлора с интегральным остатком, получим многомерную формулу Тейлора с интегральным остатком.
        \end{Comment}
        \begin{Comment}
            В одномерном случае мы знали бином Ньютона --- частный случай формулы Тейлора. Тут же мы получаем полином:
        \end{Comment}
        \thm Если $r\in\mathbb Z_+$, $x\in\mathbb R^n$, то
        \[
        \left(\sum\limits_{i=1}^nx_i\right)^r=\sum\limits_{(k)=r}\frac{r!}{k!}x^k
        \]
        \begin{Proof}
            Эту формулу можно было бы доказать комбинаторно, но мы, как и обещалось выше, выведем её из Тейлора.\\
            Пусть левая часть --- $f_r(x)$. И разложим её в нуле ($x^0=\mathbb0_n$). Что будет, когда мы начнём диффать $f_r$? Во-первых, всё равно, по какой переменной, получится одно и то же. Во-вторых, получится вот это:
            \[
            D_jf_r(x)=rf_{r-1}(x)
            \]
            То есть нам лишь важно, сколько раз мы дифференцируем, но не важно, по каким переменным. Тогда
            \[
            f_r^{(k)}(x)=
            \begin{cases}
                r\cdot\cdots\cdot(r-(k)+1)f_{r-(k)}(x) & 0\leqslant(k)\leqslant r\\
                0 & (k)>r
            \end{cases}
            \]
            В частности, 
            \[
            f_r^{(k)}(\mathbb0)=
            \begin{cases}
                0 & (k)<r\\
                r! & (k)=r
            \end{cases}
            \]
        \end{Proof}
        \thm \undercolor{red}{Многомерная формула Тейлора с остатком в форме Пеано}. Пусть $n\in\mathbb N$, $D$ открыто в $\mathbb R^n$, $f\in C^{(r)}(D)$, $x\in D$. Тогда
        \[
        f(x+h)=\sum\limits_{(k)\leqslant r}\frac{f^{(k)}(x)}{k!}h^k+o(|h|^r)\qquad\qquad h\to\mathbb0_n
        \]
        \begin{Proof}
            При указанных в теореме условиях мы можем записать формулу Тейлора-Лагранжа для порядка $r-1$.
            $$f(x+h)=\sum\limits_{(k)\leqslant r-1}\frac{f^{(k)}(x)}{k!}h^k+\sum\limits_{(k)=r}\frac{f^{(k)}(x+\theta h)}{k!}h^k$$
            Тогда
            \[\begin{split}
                f(x+h)&=\sum\limits_{(k)\leqslant r-1}\frac{f^{(k)}(x)}{k!}h^k+\sum\limits_{(k)=r}\frac{f^{(k)}(x+\theta h)}{k!}h^k\\
                &=\sum\limits_{(k)\leqslant r}\frac{f^{(k)}(x)}{k!}h^k+\sum\limits_{(k)=r}\frac{f^{(k)}(x+\theta h)-f^{(k)}(x)}{k!}h^k
            \end{split}\]
            Надо доказать, что $\sum\limits_{(k)=r}\frac{f^{(k)}(x+\theta h)-f^{(k)}(x)}{k!}h^k=o(|h|^r)$. Ну,
            $$
            |h_1^{k_1}h_2^{k_2}\cdots h_n^{k_n}|\leqslant|h|^r
            $$
            (Каждая координата вектора не больше его длины.) $k!$ в знаменателе --- константа, а $f^{(k)}(x+\theta h)-f^{(k)}(x)$ --- бесконечно мало по непрерывности $f^{(k)}$.
        \end{Proof}
        \begin{Comment}
            Мы хотим записать формулу Тейлора через старшие дифференциалы, но хрен мы определим, что такое старший дифференциал, поэтому просто назовём старшим дифференциалом то, что написало в формуле Тейлора.\\
            Как было в одномерном случае? Так:
            $$
            f(x+h)=\sum\limits_{l=0}^r\frac1{l!}\mathrm d^lf(x,h)+\frac1{(r+1)!}\mathrm d^{r+1}f(x+\theta h,h)
            $$
        \end{Comment}
        \dfn Пусть $l\in\mathbb Z_+$, $x\in\mathbb R^n$, $f\in C^{(l)}(V_x)$, $h\in\mathbb R^n$. \undercolor{red}{Дифференциалом порядка $l$} функции $f$ в точке $x$ с приращением $h$ называется
        $$
        \mathrm d^l(f,h)=\sum\limits_{(k)=l}\frac{l!}{k!}f^{(k)}(x)h^k
        $$
        \thm \undercolor{darkgreen}{Формула Тейлора-Лагранжа в дифференциалах}. В условиях обычной формулы Тейлора-Лагранжа
        $$
        f(x+h)=\sum\limits_{l=0}^r\frac1{l!}\mathrm d^lf(x,h)+\frac1{(r+1)!}\mathrm d^{r+1}f(x+\theta h,h)
        $$
        \begin{Comment}
            Что будет при малых $l$?
            \begin{enumerate}
                \addtocounter{enumi}{-1}
                \item
                $$
                \mathrm d^0f(x,h)=f(x)
                $$
                Достаточно, чтобы $f$ была задана в $x$, непрерывность лишняя.
                \item
                $$
                \mathrm d^1f(x,h)=\mathrm df(x,h)=\sum\limits_{i=1}^nD_if(x)h_i
                $$
                Опять же, достаточно дифференцируемости в точке $x$, не требуется непрерывность производной.
                \item
                $$
                \mathrm d^2f(x;h)=\sum\limits_{i,j=1}^nD^2_{ij}f(x)h_ih_j
                $$
                Это квадратичная форма от $h$. И тут написаны элементы матрицы этой формы. Матрица даже название имеет --- матрица Гессе.
            \end{enumerate}
        \end{Comment}
        \dfn \undercolor{red}{Матрица Гессе} --- $(D^2_{ij}f(x))_{i,j=1}^n$.
        \thm Тривиально, матрица Гессе симметрична в случае $f\in C^{(2)}$.
        \dfn \undercolor{red}{Однородный многочлен} (или \undercolor{red}{форма}) степени $l$ --- такой многочлен $P$ от $n$ переменных, что
        $$\forall h\in\mathbb R^n~\forall\lambda\in\mathbb R~P(\lambda h)=\lambda^lP(h)$$
        \begin{Comment}
            $l$-й дифференциал --- это $l$-форма, как несложно заметить.
        \end{Comment}
        \thm В условиях определения дифференциалов
        $$
        \mathrm d^l(x;h)=\mathrm d\left(\mathrm d^{l-1}f(\cdot;h)\right)(x;h)
        $$
        \begin{Proof}
            Брать и что-то дифференцировать в определении мы не хотим, потому что мы это уже делали в лемме перед формулой Тейлора. Там была функция $F(t)=f(x+th)$, и мы доказали, что $F^{(l)}(t)=\sum\limits_{(k)=l}\frac{l!}{k!}f^{(k)}(x+th)h^k=\mathrm df(x+th;h)$. Чтобы получить дифференциал в точке $x$, надо взять $F(0)$.\\
            Теперь давайте доказывать формулу.
            \[
            \begin{split}
                \mathrm d\left(\mathrm d^{l-1}f(\cdot;h)\right)(x;h)&=\frac{\mathrm d}{\mathrm dt}\mathrm d^{l-1}f(x+th;h)\bigg|_{t=0}=\\
                &=\frac{\mathrm d}{\mathrm dt}F^{(l-1)}(t)\bigg|_{t=0}=F^{(l)}\bigg|_{t=0}=\mathrm d^lf(x;h)
            \end{split}
            \]
        \end{Proof}
        \thm \undercolor{darkgreen}{Двумерная формула Тейлора в координатах}.
        $$
        f(x,y)=\sum\limits_{l=0}^r\frac1{l!}\sum\limits_{\nu=0}^l\Cnk{l}{\nu}\frac{\partial^lf(x^0;y^0)}{\partial x^\nu\partial y^{l-\nu}}(x-x^0)^\nu(y-y^0)^{l-\nu}+o(\sqrt{(x-x^0)^2+(y-y^0)^2}^r)
        $$
        \begin{Comment}
            Уже очень громоздко.
        \end{Comment}
        \begin{Example}
            Не всегда надо явно дифференцировать, а иногда надо пользоваться одномерным случаем. Например,
            $$
            \frac1{1+x+y^2}=\sum\limits_{k=0}^r(-1)^l(x+y^2)^k+o(\sqrt{x^2+y^2}^r)
            $$
        \end{Example}
        \dfn Пусть $X$ --- метрическое пространство, $D\subset X$, $x^0\in D$, $\scriptA\colon D\to\scriptL(\mathbb R^n\to\mathbb R^m)$. Тогда непрерывность $\scriptA$ в точке $x^0$ равносильна непрерывности всех его матричных элементов в той же точке.
        \begin{Proof}
            Мы знаем, что в конечномерном пространстве ($\scriptL(\mathbb R^n\to\mathbb R^m)$) все нормы эквивалентны. А значит вместо операторной нормы можно ввести евклидову (в $\mathbb R^{nm}$). А для евклидовой нормы мы знаем, что непрерывность равносильна покоординатной непрерывности (это мы знаем).
        \end{Proof}
        \dfn \undercolor{darkgreen}{Равносильность определений непрерывной дифференцируемости}. Пусть $D$ открыто в $\mathbb R^n$, $f\colon D\to\mathbb R^m$. Тогда следующие утверждения равносильны:
        \begin{enumerate}
            \item Отображение $f$ дифференцируемо на $D$ и $f'$ непрерывно на $D$.
            \item Все частные производные всех координатных функций $f$ существуют и непрерывны на $D$.
        \end{enumerate}
        \begin{Proof}
            \begin{itemize}
                \item[$1\to2$] Раз $f$ дифференцируема на $D$, все координатные функции дифференцируемы, а значит у всех из них есть частные производные. Также мы знаем, что матрица $f'$ --- матрица Якоби --- состоит из частных производных. А они непрерывны, поскольку непрерывно само $f'$ по доказанной выше лемме.
                \item[$2\to1$] По известной теореме все координатные функции $f$ дифференцируемы на $D$. А значит и само $f$ дифференцируемо. А дальше снова лемма, но в другую сторону.
            \end{itemize}
        \end{Proof}
        \begin{Comment}
            Окей, мы знаем, что такое дважды непрерывно дифференцируемая функция, знаем, что такое непрерывно дифференцируемая функция и знаем, что такое просто дифференцируемая функция. А что же такое просто 2 раза дифференцируемая функция, без непрерывности? Мы могли бы, конечно, отождествить производную с элементов $\mathbb R^{nm}$, но это не очень естественно. Поэтому ничего не доказывая, дадим серию определений.
        \end{Comment}
        \dfn Пусть $X$, $Y$ --- нормированные пространства. Пусть $F\colon\underset{\subset X}D\to\mathbb Y$, $x\in\Int D$. Отображение $f$ называется \undercolor{red}{дифференцируемым в точке} $x$, если существует $\scriptA\in\scriptL(X\to Y)$, что $f(x+h)=f(x)+\scriptA h+o(h)$ при $h\to\mathbb\theta_X$. При этом $\scriptA$ называется \undercolor{red}{производным оператором} $f$ в точке $x$ и обозначается $f'(x)$.
        \dfn Пусть выполнены условия предыдущего определения, $D_1$ --- множество всех внутренних точек $D$, где $f$ дифференцируемо. Отображение $f'\colon\substack{D_1\to\scriptL(X\to Y)\\x\mapsto f'(x)}$ называется \undercolor{red}{производным отображением} $f$.
        \dfn Если определены множества $D_1;\ldots;D_{r-1}$ и производные $f';\ldots;f^{(r-1)}$, полагаем $D_r$ --- множество дифференцируемости $f^{(r-1)}$, $f^{(r)}=(f^{r-1})'$.
        \dfn Пусть $D$ открыто в $X$. Отображение $f$ \undercolor{red}{$r$ раз дифференцируемо} на $D$, если оно там $r$ раз дифференцируемо и $r$-тая производная непрерывна.
        \begin{Comment}
            Так вот, вторая производная действует откуда-то $D_2$ в $\scriptL(X\to\scriptL(X\to Y))$. То есть $\scriptL_r=\scriptL(X\to\scriptL_{r-1})$.
        \end{Comment}
        \thm Можно доказать, что определение $C^{(r)}$ отсюда совпадают с данным много выше. Но мы не будем, нам лень.
        \begin{Comment}
            Ещё стоит заметить, что у нас слишком много условий в различных теоремах. Например, в формуле Тейлора-Пеано достаточно дифференцируемости $r$ раз в точке, а не $r$ раз непрерывной дифференцируемости в окрестности. Но там мы не могли иначе, мы не знали, что такое $r$-тая производная. Более того, мы не хотели думать, что делать, если смешанные производные не существуют в окрестности. Вместо этого можно было бы требовать двукратной дифференцируемости в точке. Но нам вообще поебать. Если мы не будем искать неприятностей на свою голову сами, то только с непрерывно дифференцируемыми функциями мы работать и будем.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Экстремумы и неявные отображения}.}
    \begin{itemize}
        \dfn $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R^m$, $x^0\in D$. Если $\exists V_{x^0}~\forall x\in V_{x^0}\cap D~f(x)\leqslant f(x^0)$, то $x^0$ называется \undercolor{red}{точкой (локального) максимума} $f$. Если вдя всех $x\neq x^0$ неравенство строгое, то $x^0$ называется \undercolor{red}{точкой строгого (локального) максимума}.
        \dfn Аналогично определяется \undercolor{red}{точка (локального) минимума} и \undercolor{red}{точка строгого (локального) минимума}.
        \dfn \undercolor{red}{Точка (локального) экстремума} --- точка локального минимума либо максимума, \undercolor{red}{точка строгого (локального) экстремума} --- точка строгого локального минимума либо максимума.
        \begin{Comment}
            Если $x\in\Int D$, $V_{x^0}\cap D$ можно заменить на $V_{x^0}$.
        \end{Comment}
        \thm \undercolor{darkgreen}{Необходимое условие экстремума функции нескольких переменных}. Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R$, $x^0\in\Int D$, $x^0$ --- точка экстремума $f$. Пусть $k\in[1:n]$, $\exists D_kf(x^0)$. Тогда $D_kf(x^0)=0$.
        \begin{Proof}
            Давайте введём $\varphi_k(t)=f(x^0+te^k)$. Функция $\varphi_k$ задана в некоторой окрестности нуля $(-\delta;\delta)$ так как $x\in\Int D$. Тогда $0$ --- точка экстремума $\varphi_k$, а значит по одномерному случаю $\varphi'_k(0)=0$, а $\varphi'_k=D_kf(x^0)$.
        \end{Proof}
        \begin{Comment}
            Если есть все частные производные, то все они обязаны обнулиться. В частности, если $f$ дифференцируема в $x^0$, её производный оператор нулевой. И её градиент, кстати, тоже.
        \end{Comment}
        \dfn Точки, в которых $\grad f(x^0)=\mathbb0_n$ называются \undercolor{red}{стационарными}.
        \begin{Comment}
            Как мы знаем, стационарные точки --- точки, <<подозрительные на экстремум>>. А значит, как и в одномерном случае, нам нужны какие-то методы исследования стационарных точек. Но более того, нам надо ещё и анализировать границу, ведь в многомерном случае она может быть сложно устроена, нежели в одномерном.\\
            Хорошо, что было в одномерном случае с достаточными условиями? Если производная непрерывна с двух сторон и сохраняет с них обеих знак, то мы делали вывод. Тут эе вообще непонятно, как с этим жить, ведь сторон бесконечно много. Если вы исследуете все прямые, этого всё равно не будет достаточно. Поэтому этот способ мы переносить не будем. Второй же способ использовал старшие производные в точке. Там мы делали вывод о чётности первой необнулённой производной. И тут уже есть какой-то шанс перенести это на многомерный случай, но поскольку высшие производные --- это что-то неописуемо ужасное, мы перенесём её со второй производной. НО вместо второй производной мы возьмём второй дифференциал. В одномерном случае это было одним и тем же про сути. А тут только с дифференциалом и можно работать.
        \end{Comment}
        \dfn Пусть $K$ --- квадратичная форма в $\mathbb R^n$. Если $\forall h\neq\mathbb0_n~K(h)>0$, то $K$ \undercolor{red}{положительно определённая форма}.
        \dfn Пусть $K$ --- квадратичная форма в $\mathbb R^n$. Если $\forall h\neq\mathbb0_n~K(h)<0$, то $K$ \undercolor{red}{отрицательно определённая форма}.
        \dfn Пусть $K$ --- квадратичная форма в $\mathbb R^n$. Если $\exists h^*,h^{**}\in\mathbb R^n~K(h^*)<0\land H(h^{**})$, $K$ называется \undercolor{red}{неопределённой формой}.
        \dfn Пусть $K$ --- квадратичная форма в $\mathbb R^n$. Если $\forall h\neq\mathbb0_n~K(h)\geqslant0$ и $\exists h'\neq\mathbb0_n~K(h')=0$, то $K$ называется \undercolor{red}{положительно полуопределённой формой}.
        \dfn Пусть $K$ --- квадратичная форма в $\mathbb R^n$. Если $\forall h\neq\mathbb0_n~K(h)\leqslant0$ и $\exists h'\neq\mathbb0_n~K(h')=0$, то $K$ называется \undercolor{red}{отрицательно полуопределённой формой}.
        \thm Если $K$ --- положительно определённая форма, то $\exists\gamma>0~\forall h\in\mathbb R^n~K(h)\geqslant\gamma|h|^2$.
        \begin{Proof}
            Вейерштрасс!\textbackslash\textbackslash Только свистни, он появится!\\
            Пусть $\gamma=\min\limits_{\mathbb S^{n-1}}K$ (минимум по единичной сфере пространства $\mathbb R^n$). Минимум существует по теореме Вейерштрасса. И он больше нуля потому что. Тогда
            \[
            \forall h\neq\mathbb0_n~K(h)=K(|h|\frac h{|h|})=|h|^2K(\frac h{|h|})\geqslant\gamma|h|^2
            \]
            Для $h=\mathbb0_n$ и так ясно.
        \end{Proof}
        \thm Достаточное условие экстремума функции нескольких переменных. Пусть $D$ открыто в $\mathbb R^n$, $f\in C^{(2)}(D)$, $x^0\in D$ --- стационарная точка $f$.
        \begin{itemize}
            \item Если $\mathrm d^2f(x^0)$ положительно определена, то $x^0$ --- точка строгого минимума.
            \item Если $\mathrm d^2f(x^0)$ отрицательно определена, то $x^0$ --- точка строгого максимума.
            \item Если $\mathrm d^2f(x^0)$ не определена, то $x^0$ --- не точка экстремума.
        \end{itemize}
        \begin{Comment}
            Для полуопределённой формы терема ответа не даёт. В частности, для тождественно равной нулю формы (см. пример).
        \end{Comment}
        \begin{Example}
            $f_1(x;y)=x^4+y^4$. $0$ --- стационарная точка, там и первые, и вторые производные обращаются в ноль. Очевидно, точка строгого минимума.\\
            $f_2(x;y)=x^4-y^4$. $0$ --- тоже стационарная точка, там тоже и первые, и вторые производные обращаются в ноль. Однако, очевидно, точка не экстремальная.
        \end{Example}
        \begin{Proof}
            Запишем Тейлора:
            \[
            f(x^0+h)=f(x^0)+\mathrm d^1f(x^0;h)+\frac12\mathrm d^2f(x^0;h)+o(|h|^2)
            \]
            Нужно сравнить значение в точке $x^0$ и достаточно близких в ней точках. Эта хрень: $\mathrm d^1f(x^0;h)$ --- равна нулю, поскольку точка стационарная. Пусть $R=2(f(x^0+h)-f(x^0))=\mathrm d^2f(x^0;h)+\alpha(h)|h|^2$, где $\alpha(h)\underset{h\to\mathbb0_n}\longrightarrow0$. Хочется узнать знак $R$, что нам, собственно, и даст доказательство теоремы.\\
            Известно, что $R\geqslant(\gamma+\alpha(h))|h|^2\geqslant\frac\gamma2|h|^2\geqslant0$. Это верно для любого $h\in\dot V_{\mathbb0_n}$. Второе неравенство верно так как $\exists V_{\mathbb0_n}~\forall h\in V_{\mathbb0_n}~|\alpha(h)|\leqslant\frac\gamma2$.\\
            Для доказательства второго утверждения, домножим первое на $-1$, всё.\\
            Для доказательства третьего, докажем, что это не минимум и не максимум. Пусть $h=th^*$, где $t>0$. Тогда за знак квадратичной формы $t$ вынесется в квадрате (по определению). И из нормы тоже вынесется. А значит $R=t^2(\mathrm df(x^0;h^*)+\alpha(th^*)|h^*|^2)$. Тогда по малости $\alpha$ известно, что $\exists\delta>0~\forall t\in(0;\delta)~|\alpha(th^*)|<\frac{\mathrm d^2f(x^0;h^*)}{2|h^*|^2}$, следовательно $R>\frac{\mathrm d^2f(x^0;h^*)}2t^2>0$. Это значит, что $x^0$ не является точкой максимума. Аналогично можно доказать, что $x^0$ не является точкой максимума.
        \end{Proof}
        \begin{Comment}
            На самом деле достаточно двукратной дифференцируемости $f$ в точке $x^0$, непрерывная дифференцируемость не нужна. Но поскольку формулу Тейлора мы доказывали для $r$-гладких функций, тут делаем то же самое.
        \end{Comment}
        \thm \undercolor{darkgreen}{Критерий Сильвестра}. Пусть $K$ --- квадратичная форма с симметричной матрицей. Тогда $K$ положительно определена тогда и только тогда когда $\forall k\in[1:n]~\Delta_k>0$, где $\Delta_k$ --- главные миноры, а отрицательно определена тогда и только тогда, когда $(-1)^k\Delta_k>0$.
        \begin{Proof}
            Что-то из линейной алгебры.
        \end{Proof}
        \begin{Comment}
            Окей, а что можно сделать, когда этот критерий не работает? Да хер знает, но запишем пару примеров.
        \end{Comment}
        \begin{Example}
            $f(x;y)=x^4_3xy^2+y^4$.
            $f(\mathbb0)=\mathrm d^1f(\mathbb0)=\mathrm d^2f(\mathbb0)=0$. Тут можно попытаться взять $\mathrm d^3f(\mathbb0)$. Тут, понятно, не обнулится частная производная только если один раз диффать по $x$ и два --- по $y$. И $f'''_{xy^2}(\mathbb0)=6$. То есть $\mathrm d^3(\mathbb0;\mathrm dx;\mathrm dy)=\Cnk31f'''_{xy^2}(\mathbb0)\mathrm dx\mathrm dy^2=18\mathrm dx\mathrm dy^2$. Оп, не ноль. И как и в одномерном случае это не точка экстремума, потому что третья не обнуляется. Чтобы это честно доказать, нужно сузить функцию на какую-то прямую (например, $x=y$). То есть $h(1;1)$, $\varphi(t)=f(th)=2t^4+3t^3$. Тогда $\mathrm d^3f(\mathbb0;h)\neq0$, то есть $0$ --- не экстремум $\varphi$, а тогда уж и $\mathbb0$ --- не экстремум $f$.
        \end{Example}
        \begin{Example}
            $f(x;y)=x^2e^y-2xy+y^2e^x$. Тогда $f'_x=2xe^y-2y+y^2e^x$, $f'_y=x^2e^y-2x+2ye^x$. $f''_{x^2}=2e^y+y^2e^x$, $f''_{xy}=2xe^y-2+2ye^x$, $f''_{y^2}=x^2e^y+2e^x$. Нам это всё надо считать в нуле и получить соответственно $2$, $-2$ и $2$. То есть матрица второго дифференциала выглядит так:
            \[\matr{
                2 & -2\\
                -2 & 2
            }\]
            Или $\mathrm d^2f(\mathbb0;\mathrm dx;\mathrm dy)=2\mathrm dx^2-4\mathrm dx\mathrm dy+2\mathrm dy^2$. Это полуопределённая форма (она неотрицательна, но обнуляется при $\mathrm dx=\mathrm dy$). Ну так $\varphi(t)=f(t;t)=2t^2(e^t-1)$. И тут видно, что $0$ --- не экстремум $\varphi$, а значит $\mathbb0$ --- не экстремум $f$.\\
            При этом сужение $f$ на любую другую прямую через начало координат имеет в нуле строгий минимум.
        \end{Example}
        \begin{Example}
            $f(x;y)=y^4-y^2+2x^2y$. Тогда $f''_{x^2}=4y$, $f''_{xy}=4x$, $f''_{y^2}=12y^2-2$. Подставив точку $\mathbb0$, получим $0$, $0$ и $-2$ соответственно. То есть $\mathrm d^2f(\mathbb0;\mathrm dx;\mathrm dy)=-2\mathrm dy^2$ --- полуопределённая форма. Давайте попробуем сузить это на прямую $x=t$, $y=\alpha t$. Тогда $\varphi(t;\alpha t)=\alpha^4t^4-\alpha^2ty^2+2\alpha t^2$. При $\alpha\neq0$ получаем строгий максимум в нуле, при $\alpha=0$ имеет тождественно равную нулю функцию, то есть нестрогий максимум, а на прямой $x=0$ --- тоже строгий максимум. Но, к сожалению, отсюда не делается вывод, что сама $f$ имеет в нуле максимум. И чтобы это увидеть, достаточно сузить $f$ на параболу $y=x^2$, и получить $\varphi(t;t^2)=t^4+t^8$, где получаем строгий минимум, а значит это не экстремум $f$.
        \end{Example}
        \begin{Example}
            $f(x;y)=2\sin xy-(x+y)^2$. Тут уже сами проверьте, что форма полуопределена и имеет матрицу $\matr{-2 & 2\\2 & -2}$. И тут можно взять $\sqrt{x^2+y^2}<1$, после чего разобрать два случая: $xy\leqslant0\Rightarrow\sin xy\leqslant0\Rightarrow f(x;y)\leqslant0$ и $xy\geqslant0\Rightarrow\sin xy\leqslant xy\Rightarrow f(x;y)\leqslant 4xy-(x+y)^2=-(x-y)^2\leqslant0$, и получить, что форма имеет экстремум, но нормально это никак не выяснить.
        \end{Example}
        \begin{Comment}
            Потом мы хотим говорить про обратные отображения, а значит сначала надо поговорить про обратимые линейные операторы.
        \end{Comment}
        \thm Пусть $\scriptB\in\scriptL(\mathbb R^n)$ и $\exists m>0~\forall x\in\mathbb R^n~|\scriptB x|\geqslant m|x|$. Тогда $\scriptB$ обратим и $\|\scriptB^{-1}\|\leqslant\frac1m$.
        \begin{Proof}
            У нас было следствие теорема о ранге и дефекте, и тут если $x\neq\mathbb0_n$, то и $\scriptB x\neq\mathbb0_n$, то есть $\scriptB$ обратима.\\
            Как писать норму? Обозначим $y=\scriptB x$, откуда $\scriptB^{-1}y=x$. И тогда $|\scriptB^{-1}y|\leqslant\frac1m|y|$ для любого $y\in\mathbb R^n$.
        \end{Proof}
        \dfn Мы будем обозначать $\Omega(\mathbb R^n)$ --- множество всех обратимых линейный операторов в $\mathbb R^n$.
        \thm Если $\scriptA\in\Omega(\mathbb R^n)$, то $\forall x\in\mathbb R^n~|\scriptA x|\geqslant\frac1{\|\scriptA^{-1}\|}|x|$.
        \begin{Proof}
            Ну, $|x|=|\scriptA^{-1}\scriptA x|\leqslant\|\scriptA^{-1}\|\cdot|\scriptA x|$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Обратимость оператора, близкого к обратимому}. Пусть $\scriptA\in\Omega(\mathbb R^n)$, $\scriptB\in\scriptL(\mathbb R^n)$. Пусть $\|\scriptB-\scriptA\|<\frac1{\|\scriptA^{-1}\|}$. Тогда
        \begin{enumerate}
            \item $\scriptB\in\Omega(\mathbb R^n)$.
            \item $\|\scriptB^{-1}\|\leqslant\frac1{\|\scriptA^{-1}\|^{-1}-\|\scriptB-\scriptA\|}$.
            \item $\|\scriptB^{-1}-\scriptA^{-1}\|\leqslant\frac{\|\scriptA^{-1}\|}{\|\scriptA^{-1}\|^{-1}-\|\scriptB-\scriptA\|}\|\scriptB-\scriptA\|$.
        \end{enumerate}
        \begin{Proof}
            \begin{enumerate}
                \item[1--2.] Возьмём $x\in\mathbb R^n$. Тогда $|\scriptB x|\geqslant|\scriptA x|-|(\scriptB-\scriptA)x|\geqslant\underbrace{(\|\scriptA^{-1}\|^{-1}-\|\scriptB-\scriptA\|)}_{m}x$.
                \item[3.] $\|\scriptB^{-1}-\scriptA^{-1}\|=\|\scriptB^{-1}(\scriptA-\scriptB)\scriptA^{-1}\|\leqslant\|\scriptB^{-1}\|\|\scriptB-\scriptA\|\|\scriptA^{-1}\|\overset{\text{п. 2}}\leqslant\frac{\|\scriptA^{-1}\|}{\|\scriptA^{-1}\|^{-1}-\|\scriptB-\scriptA\|}\|\scriptB-\scriptA\|$.
            \end{enumerate}
        \end{Proof}
        \thm $\Omega(\mathbb R^n)$ открыто в $\scriptL(\mathbb R^n)$.
        \begin{Proof}
            Переформулированный пункт 1 теоремы.
        \end{Proof}
        \thm Пусть $\scriptA\in\Omega(\mathbb R^n)$, $\{\scriptB_k\}_{k=1}^\infty$ --- последовательность линейный операторов в $\mathbb R^n$. И пусть $\scriptB_k\underset{k\to\infty}\longrightarrow\scriptB$. Тогда с некоторого момента $\scriptB_k$ обратимы и последовательность $\scriptB^{-1}_k\underset{k\to\infty}\longrightarrow\scriptA^{-1}$.
        \begin{Proof}
            Почему обратимы, понятно, почему стремится ---
            $$
            \|\scriptB^{-1}_k-\scriptA^{-1}\|\leqslant\frac{\|\scriptA^{-1}\|}{\|\scriptA^{-1}\|^{-1}-\|\scriptB_k-\scriptA\|}\|\scriptB_k-\scriptA\|\underset{k\to\infty}\longrightarrow0
            $$
        \end{Proof}
        \begin{Comment}
            Это непрерывность перехода к обратному оператору на языке последовательностей. То есть $T\colon\substack{\Omega(\mathbb R^n)\to\Omega(\mathbb R^n)\\\scriptA\mapsto\scriptA^{-1}}$ непрерывно.
        \end{Comment}
        \begin{Comment}
            На самом деле $T$ из комментария выше даже дифференцируемо, притом бесконечно дифференцируемо. Доказывать это мы не будем, это сложно, но первую производную запишем:
            $$
            T'(\scriptX)\scriptH=-\scriptX^{-1}\scriptH\scriptX^{-1}
            $$
            Это можно доказать, взять как базу индукции и по ней доказать бесконечную дифференцируемость.
        \end{Comment}
        \begin{Comment}
            О'кей, следующий сюжет: хочется доказать теорему об обратном отображении. Но сначала несколько комментариев, замечаний, etc.
        \end{Comment}
        \thm Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R^n$, $f$ обратима. Пусть $f$ дифференцируема в точке $x\in\Int D$, а $f^{-1}$ дифференцируема в $y=f(x)\in\Int f(D)$. Тогда оператор $f'(x)$ обратим и $(f^{-1})'(y)=(f'(x))^{-1}$.
        \begin{Proof}
            $f^{-1}\circ f$ --- тождественное отображение на множестве $D$. Тогда $(f^{-1})'(y)f'(x)=I$. Это ли не то что нам надо.
        \end{Proof}
        \begin{Comment}
            Если нам дано так много всего, формула у нас есть. А хочется чтобы было дано чуть меньше. Хочется дифференцируемость $f'$ иметь в заключении, а не в посылке.
        \end{Comment}
        \begin{Comment}
            Что было в одномерном случае? $D$ был невырожденным промежутком в $\mathbb R$, $f\colon D\to\mathbb R$ --- дифференцируема. И $\forall x\in D~f'(x)\neq0$. Наших одномерных знаний хватает для того, чтобы понять, что производная или всюду положительна, или всюду отрицательна (по теорему Дарбу). В таком случае $f$ строго монотонна, следовательно, обратима. Также из непрерывности мы знаем, что $f(D)$ --- невырожденный промежуток (притом того же типа, что и $D$), а значит на этом невырожденном промежутке задано $f^{-1}$. И тем самым верна формула для производной обратной функции
            $$
            (f^{-1})'(y)=\frac1{f'(x)}=\frac1{f'(f^{-1}(y))}
            $$
            После этого можно сказать, что если $f'$ непрерывна в $x^0$, то $(f^{-1})'$ непрерывна в $y^0=f(x^0)$.\\
            Что из этого всего проходит в многомерном случае? На первом же шаге мы обламываемся по отсутствию монотонности, а значит не можем вывести обратимость $f$. А даже если и выведем, то непонятно, будет ли образ внутренней точки внутренней точкой? И этого всего ещё не достаточно для обратимости $f'$. Более того, подобный вывод будет неправдив в общем случае. Первый пункт просто неправда. Второй переход мы докажем, и докажем всё остальное. Это всё план, приступим же к нему.
        \end{Comment}
        \thm Пусть $D$ открыто в $\mathbb R^n$, $f\in C^{(1)}(D\to\mathbb R^n)$, $a\in D$. Пусть оператор $\scriptA=f'(a)$ обратим. Пусть $\lambda=\frac1{4\|\scriptA^{-1}\|}$. Тогда $\exists U$ --- такая окрестность $a$, что
        \begin{enumerate}
            \item $\forall x\in U~f'(x)$ обратим.
            \item $\forall x,x+h\in U~|f(x+h)-f(x)-\scriptA h|\leqslant 2\lambda|h|$ и $|f(x+h)-f(x)|\geqslant2\lambda|h|$.
        \end{enumerate}
        \begin{Proof}
            Нам дано, что $f\in C^{(1)}(D\to\mathbb R^n)$. У нас было два определения $C^{(1)}$. Одно из них --- $f$ дифференцируемо на $D$ и её производная непрерывна. Им и воспользуемся.
            $$\exists r>0~\forall x\in B(a;r)~\|f'(x)-\scriptA\|<2\lambda=\frac1{2\|\scriptA^{-1}\|}$$
            $B(a;r)$ и подойдёт в качестве $U$. Первое утверждение выполнено по теореме об операторе близкого к обратимому. Второе ещё надо доказать.\\
            Первое неравенство докажем по формуле Лагранжа. Пусть $F(u)=\substack{u\to\mathbb R^n\\u\mapsto f(u)-\scriptA u}$. Найдём его производную: $F'(u)=f'(u)-\scriptA$. Оценим норму производной: $\|F'(u)\|=\|f'(u)-\scriptA\|<2\lambda$ по утверждению выше (которое определение $C^{(1)}$). Тогда по формуле Лагранжа
            $$\forall x,x+h\in U~|F(x+h)-F(x)|<2\lambda|h|$$
            Левое как раз и есть $f(x+h)-f(x)-\scriptA h$.\\
            Теперь второе неравенство. Его мы выведем из первого:
            $$
            |f(x+h)-f(x)|\geqslant|\scriptA h|-|f(x+h)-f(x)-\scriptA h|\geqslant\underbracket{\|\scriptA^{-1}\|^{-1}}_{4\lambda}|h|-2\lambda|h|
            $$
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема об обратном отображении}. Пусть $D$ открыто в $\mathbb R^n$, $f\in C^{(1)}(D\to\mathbb R^n)$, $a\in D$, $f'(a)$ обратим. Тогда $\exists U$ --- окрестность $a$ со следующими свойствами:
        \begin{enumerate}
            \item $f\Big|_U$ обратимо.
            \begin{Comment}
                Глобальной обратимости не получится никак.
            \end{Comment}
            \item $V=f(U)$ открыто.
            \item $f^{-1}\in C^{(1)}(V\to U)$.
            \begin{Comment}
                Здесь и далее $f^{-1}$ --- это не $f^{-1}$, а $\left(f\Big|_U\right)^{-1}$.
            \end{Comment}
            \item $\forall y\in V~(f^{-1})'(y)=(f'(f^{-1}(y)))^{-1}$.
        \end{enumerate}
        \pagebreak
        \begin{Proof}
            Обозначим $\scriptA=f'(a)$, $\lambda=\frac1{4\|\scriptA\|^{-1}}$. В качестве $U$ возьмём, что бы вы думали, окрестность из леммы выше.\\
            Обозначим неравенство:
            $$
            \forall x,x+h\in U~|f(x+h)-f(x)|\geqslant2\lambda|h|
            $$
            ...за $(*)$, и будем на него ссылаться.
            \begin{enumerate}
                \item $(*)$ обеспечивает нам обратимость, потому что если $h\neq\mathbb0$, то $f(x)$ уж всяко не равно $f(x+h)$.
                \item Как мы знаем, $U=B(a;\rho)$ --- открытый шар. Чтобы доказать второй пункт, возьмём $y^0\in V$ и докажем, что она внутренняя. Что ж, если $y^0\in V$, то $\exists x^0\in U~y^0=f(x^0)$. В таком случае $\exists r>0~\overline B(x^0;r)\subset U$, потому что $U$ открыто. Хочется доказать, что $B(y^0;\lambda r)\subset V$.\\
                Что ж, возьмём точку $y\in B(y^0;\lambda r)$. Хочется доказать, что $y\in V$, то есть что $y$ является образом некоторой $x\in U$. Рассмотрим $F(x)=|f(x)-y|$, $x\in\overline B(x^0;r)$. По теореме Вейерштрасса $F$ на компакте принимает наименьшее значение, то есть $\exists x^*\in\overline B(x^0;r)~F(x^*)=\min\limits_{\overline B(x^0;r)}F$. Если $x^*$ лежит на границе $\overline B(x^0;r)$, мы проиграли, потому что ни дифференцировать, ничего нельзя. Хорошо, проверим, что если $x\in\partial B(x^0;r)$, то $F(x)$ --- не минимум на $\overline B(x^0;r)$. Сначала заметим, что
                $$F(x^0)=|f(x^0)-y|=|y^0-y|<\lambda r$$
                Теперь докажем, что на границе не минимум.
                \[\begin{split}
                    F(x)&>F(x)+F(x^0)-\lambda r=|f(x)-y|+|f(x^0)-y|-\lambda r\geqslant|f(x)-f(x^0)|-\lambda r\overset{(*)}\geqslant\\
                    &\overset{(*)}\geqslant2\lambda|x-x^0|-\lambda r\overset{x^0\in\partial B(x^0;r)}=2\lambda r-\lambda r>F(x^0)
                \end{split}\]
                Ура! Отсюда $x^*\in B(x^0;r)$, а не просто $\overline B(x^0;r)$. Следующее: $F^2(x)$ достигает минимума там же, где и $F(x)$, просто потому что нам лень дифференцировать квадратный корень. Значит $\grad F^2(x^*)=\mathbb 0_n$. При этом дифференцировать $F^2(x)=|f(x)-y|^2=\dotprod{f(x)-y}{f(x)-y}$ мы умеем:
                $$
                \grad F^2(x^*)=2(f(x^*)-y)^Tf'(x^*)
                $$
                По первому отверждению леммы оператор $f'(x^*)$ обратим (т.к. $x^*\in U$), а значит $f(x^*)-y=(f'(x^*))^{-1}\mathbb0_n=\mathbb0_n$. Бинго, мы доказали второе утверждение.
                \item Доказывать будем по пунктам
                \begin{enumerate}
                    \item[3.1] Сначала докажем, что обратное отображение непрерывно. Возьмём $y\in V$ и докажем, что $f^{-1}$ непрерывно в $y$. Пусть $y+k\in V$. Тогда $\exists x,h~f(x)=y,f(x+h)=y+k$. По утверждению $(*)$
                    $$
                    |f^{-1}(y+k)-f^{-1}(y)|=|h|\leqslant\frac1{2\lambda}|f(x+h)-f(x)|=\frac1{2\lambda}|k|\underset{k\to\mathbb0}\longrightarrow0
                    $$
                \end{enumerate}
                Для доказательства непрерывной дифференцируемости сначала докажем пункт 4.
                \item Сначала нам хочется дифференцируемость $f^{-1}$ в точке $y$, а потом хочется формулу $(f^{-1})'(y)$. Итак, пусть $y=f(x)$. Как известно,
                $$
                f(x+h)=f(x)+f'(x)h+\alpha(h)|h|
                $$
                где $\alpha$ в нуле непрерывна и равна нулю. Тогда, поскольку $k=f(x+h)-f(x)$,
                \[\begin{split}
                    k&=f'(x)\left(f^{-1}(y+k)-f^{-1}(y)\right)+\alpha(h)|h|\Leftrightarrow\\
                    (f'(x))^{-1}k&=f^{-1}(y+k)-f^{-1}(y)+(f'(x))^{-1}\alpha(h)|h|\Leftrightarrow\\
                    f^{-1}(y+k)&=f^{-1}(y)+(f'(x))^{-1}k-(f'(x))^{-1}\alpha(h)|h|
                \end{split}\]
                Сначала скажем, что $f'(x)$ действительно обратим, потому что $x\in U$, а в лемме было про обратимость $f'(x)$.\\
                Далее --- это утверждение уже почти то, что нам нужно, но хочется узнать что-то про $-(f'(x))^{-1}\alpha(h)|h|$. А точнее, если мы докажем, что оно равно $\beta(k)|k|$, где $\beta$ непрерывна и равна нулю в нуле, то всё будет хорошо. О'кей, пусть $h=\tau(k)$. Тогда по пункту 3.1 $\tau$ непрерывна и равна нулю в нуле. В таком случае
                $$
                \beta(k)=-(f'(x))^{-1}\alpha(\tau(k))\frac{|h|}{|k|}
                $$
                $\alpha(\tau(k))$ в нуле непрерывна (как композиция непрерывных) и равна нулю. По утверждению $(*)$ $\frac{|h|}{|k|}\leqslant\frac1{2\lambda}$, а значит
                $$
                \beta(k)\leqslant-\frac{(f'(x))^{-1}}{2\lambda}\alpha(\tau(k))\underset{k\to\mathbb0}\longrightarrow0
                $$
                \begin{enumerate}
                    \item[3.2] Теперь осталось лишь доказать, что $f^{-1}\in C^{(1)}$. То есть мы уже в 4 пункте поняли, что $f^{-1}$ дифференцируемо, осталось лишь сказать, что $(f^{-1})'$ непрерывно. Ну, что ж. На самом деле, у нас всё уже есть. Мы берём $y\in V$, строим по нему $x=f^{-1}(y)$, потом $f'(x)$, а потом берём $(f'(x))^{-1}$, которое, по формуле, равно тому, чему надо (т.е. $(f^{-1})'(y)$). Хм, ну, смотрите. Получается, что $(f^{-1})'$ является композицией трёх отображений: $f^{-1}$, $f'$ и $T$ (которое обращает оператор). Первое непрерывно по пункту 3.1, второе --- по условию, третье --- из теоремы об обратимости оператора близкого к обратимому. А значит получается, что $(f^{-1})'$ --- композиция трёх непрерывных отображений.
                \end{enumerate}
            \end{enumerate}
        \end{Proof}
        \thm Пусть $f\colon\underset{\subset\mathbb R^n}D\to\mathbb R^n$, $f$ --- дифференцируемо в точке $x\in\Int D$, $f$ --- обратимо, $f'(x)$ обратимо, $f^{-1}$ непрерывно в точке $y=f(x)\in\Int f(D)$. Тогда $f^{-1}$ дифференцируемо в точке $y$ и
        $$
        (f^{-1})'(y)=(f'(x))^{-1}
        $$
        \begin{Proof}
            Остаётся как упражнение читателю.
        \end{Proof}
        \thm $f'(a)$ обратимо тогда и только тогда, когда $\det f'(a)\neq0$.
        \dfn $\det f'(a)$ (определитель матрицы Якоби) называется \undercolor{red}{якобианом} $f$ в точке $a$.
        \thm К теореме об обратном отображении был дан комментарий, что даже если оператор $f'(a)$ обратим для любой точки $D$, глобальной обратимости нет.
        \begin{Example}
            Возьмём отображение плоскости в плоскость:
            $$
            f(x;y)=(e^x\cos y;e^x\sin y)
            $$
            Тогда чему равен его якобиан?
            $$
            \det f'(x;y)=\matrd{
                e^x\cos y & -e^x\sin y\\
                e^x\sin y & e^x\cos y
            }=e^{2x}
            $$
            Это не равно нулю никогда. То есть условие теоремы об обратном отображении выполнено в каждой точке плоскости. То есть у каждой точки $a$ существует его собственное $U_a$, в котором $f$ обратимо. Но глобально оно, разумеется, таковым не является, так как $2\pi$-периодично по второй переменной. Локально же, очевидно, полярная замена взаимооднозначна.
        \end{Example}
        \thm
        $$
        D_k(f^{-1})_i(y)=\left(\frac{\Delta_{ki}}{\Delta}\circ f^{-1}\right)(y)
        $$
        Где $\Delta=\det f'(x)$, $\Delta_{ki}$ --- алгебраическое дополнение в $k$-той строке $i$-том столбце матрицы $f'(x)$.
        \begin{Proof}
            Смотри линейную алгебру.
        \end{Proof}
        \begin{Comment}
            Отсюда тоже следует непрерывность $f^{-1}$.
        \end{Comment}
        \thm Если $r\in\mathbb N\cup\{\infty\}$, $f\in C^{(r)}$ в условиях теоремы об обратном отображении, то $f^{-1}\in C^{(r)}$.
        \begin{Proof}
            Для $r=1$ доказали. Дальше по индукции. Пусть мы доказали для $r-1$. Чтобы доказать для $r$, частные производные первого порядка должны принадлежать классу $C^{(r-1)}$. А это следует из предыдущего утверждения, ведь арифметические действия и композиция не выводит из класса $C^{(r-1)}$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Следствие об открытом отображении}. Пусть $D$ открыто в $\mathbb R^n$, $f\in C^{(1)}(D\to\mathbb R^n)$, $\forall a\in D~f'(a)$ обратим. Тогда образ любого открытого подмножества $D$ открыт. То есть если $G\subset D$ открыто, то $f(G)$ открыто.
        \begin{Proof}
            Возьмём $G$. Проверим, что $b\in f(G)$ --- внутренняя точка. Раз $b\in f(G)$, $\exists a\in G~f(a)=b$. Теорема гарантирует нам, что у точки $a$ есть окрестность $U$, такая что $V=f(U)$ открыто. Будем считать, что $U\subset G$, иначе просто вместо $U$ рассмотрим $U\cap G$. Тогда, поскольку $V=f(U)$ открыто, $b\in V\subset f(G)$, то есть $b$ содержится в $f(G)$ с окрестностью (в широком смысле).
        \end{Proof}
        \dfn $f\colon X\to Y$ называется \undercolor{red}{открытым}, если $\forall G$ --- открыто в $X$ $f(G)$ открыто в $Y$.
        \begin{Comment}
            Не путать <<\textbf{образ} любого открытого множества открыт>> (т.е. открытость) и  <<\textbf{прообраз} любого открытого множества открыт>> (т.е. непрерывность).
        \end{Comment}
        \dfn Пусть $U$, $V$ открыты в $\mathbb R^n$. $f\colon U\to V$ называется \undercolor{red}{диффеоморфизмом} множеств $U$ и $V$, если $f$ биективно, $f\in C^{(1)}(U\to V)$, $f^{-1}\in C^{(1)}(V\to U)$.
        \thm Если $f$ --- диффеоморфизм, $\forall x\in U~\det f'(x)\neq0$.
        \thm Если $f\in C^{(1)}(U)$, $f$ обратимо, $\forall x\in U~\det f'(x)\neq0$, то $f$ --- диффеоморфизм.
        \begin{Proof}
            $f(U)=V$ открыто по следствию об открытом отображении, $f^{-1}\in C^{(1)}$ по теореме.
        \end{Proof}
        \begin{Comment}
            Теперь поговорим о неявных отображениях.\\
            Отождествим $\mathbb R^n\times\mathbb R^m$ и $\mathbb R^{n+m}$. Отождествлять будем понятным образом: если $x=(x_1;\ldots;x_n)\in\mathbb R^n$, $y=(y_1;\ldots;y_m)\in\mathbb R^m$, то $(x;y)$ отождествим с $(x_1;\ldots;x_n;y_1;\ldots;y_m)\in\mathbb R^{n+m}$.\\
            Теперь пусть у нас имеется $F\colon\underset{\subset\mathbb R^{n+m}}D\to\mathbb R^l$ --- дифференцируемо в $a=(x;y)\in\Int D$. Тогда
            $$F'(a)=\arr{ccc|ccc}{
                D_{x_1}F_1 & \cdots & D_{x_n}F_1 & D_{y_1}F_1 & \cdots & D_{y_m}F_1\\
                \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
                D_{x_1}F_l & \cdots & D_{x_n}F_l & D_{y_1}F_l & \cdots & D_{y_m}F_l
            }(a)$$
            Давайте левую половину этой матрицы обозначим $F'_x$, а правую --- $F'_y$. Тогда $F'(a)=\arr{c|c}{F'_x&F'_y}(a)$. Это же можно и без координат записать:
            $$
            F'(a)\matr{h\\\hline k}=F'_x(a)h+F'_y(a)k
            $$
            Пусть $\Phi\colon\underset{\subset\mathbb R^{n+m}}D\to\mathbb R^m$. Рассмотрим уравнение $\Phi(x;y)=\mathbb0$. Его можно записать как систему уравнений
            \[\left\{\begin{aligned}
                &\Phi_1(x_1;\ldots;x_n;y_1;\ldots;y_m)=0\\
                &\qquad\qquad\qquad\vdots\\
                &\Phi_m(x_1;\ldots;x_n;y_1;\ldots;y_m)=0
            \end{aligned}\right.\]
            Мы считаем $y$ неизвестными, а $x$ --- параметрами. И хочется узнать разрешимость, зависимость $y$ от $x$. Второе --- неявно заданное отображение.
        \end{Comment}
        \dfn $\Phi\colon\overset{\supset U\times V}D\to\mathbb R^m$. Если
        $$\forall x\in U~\exists!y\in V~\Phi(x;y)=\mathbb0_m$$
        то говорят, что уравнение $\Phi(x;y)=\mathbb0_m$ определят \undercolor{red}{неявное отображение} $\varphi\colon U\to V$, которое каждому $x\in U$ сопоставляет то единственное $y\in V$, что $\Phi(x;y)=\mathbb0_m$.
        \begin{Comment}
            В нашем случае будет столько же уравнений, сколько неизвестных.
        \end{Comment}
        \begin{Example}
            $$
            \Phi(x;y)=x^2+y^2-1\qquad\qquad n=m=1
            $$
            Если мы возьмём $-1<x^0<1$, уравнение будет иметь два решения $y^0=\pm\sqrt{1-(x^0)^2}$. Если зафиксировать $x^0$ и одно из решений $y^0$, то в окрестности $U_{x^0}$, $V_{y^0}$ функция действительно задаётся. Если $|x^0|>1$, там совсем никак ничего не задаётся. А вот если $x=\pm1$, то какую бы окрестность точки $\pm1$ мы не взяли, там ничего задаваться не будет.\\
            С другой же стороны, если поменять ролями переменные, то другие точки будут проблемными. Какие свойства за это отвечают -- нам предстоит выяснить.
        \end{Example}
        \begin{Example}
            Возьмём линейное отображение $\Phi$. Тогда его матрица делится на 2 блока:
            $$\arr{c|c}{A&B}=\arr{ccc|ccc}{
                a_{11} & \cdots & a_{1n} & b_{11} & \cdots & b_{1m}\\
                \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
                a_{n1} & \cdots & a_{nn} & b_{m1} & \cdots & b_{mm}
            }$$
            Однозначная разрешимость этой штуки равносильна обратимости матрицы $B$. А что такое $B$? А $B$ --- это $\Phi'_y$. То есть хочется $\det\Phi'_y\neq0$\\
            Вернувшись к первому примеру, также можем записать
            $$\Phi'_y=2y\neq0\qquad\forall(x;y)\in\mathbb S\setminus\{(1;0);(-1;0)\}$$
            Неспроста.
        \end{Example}
        \begin{Comment}
            Что же в общем случае? Тогда $\Phi(x;y)\approx\Phi(x^0;y^0)+\Phi'_x\Phi(x^0;y^0)(x-x^0)+\Phi'_y\Phi(x^0;y^0)(y-y^0)$. Если бы вместо левой части была правая, мы бы знали ответ --- необходима обратимость $\Phi'_y$. На то, что отброшенный остаточный член ни на что не повлияет, мы можем надеяться, только в окрестности точки $(x^0;y^0)$.
        \end{Comment}
        \begin{Comment}
            Другой частный случай --- вот такой: $\Phi(x;y)=f(y)-x$. Тут нам необходима всего лишь обратимость функции $f$, тогда мы знаем ответ $y=f^{-1}(x)$. А обратимость $f$ равносильна $\det f'(y^0)\neq0$ или в терминах $\Phi$ $\det\Phi'_y(x^0;y^0)\neq0$.
        \end{Comment}
        \thm \undercolor{darkgreen}{Теорема о неявном отображении}. Пусть $D$ открыто в $\mathbb R^{n+m}$, $\Phi\in C^{(1)}(D\to\mathbb R^m)$. Пусть $(x^0;y^0)\in D$ --- такая точка, что $\Phi(x^0;y^0)=\mathbb0_m$. Пусть $\det\Phi'_y(x^0;y^0)\neq0$. Тогда существуют $U$ --- окрестность $x^0$, $V$ --- окрестность $y^0$ такие что
        \begin{enumerate}
            \item $\forall x\in U~\exists!y\in V~\Phi(x;y)=0$ (тем самым определено неявное отображение, которое обозначим $\varphi\colon U\to V$).
            \item $\varphi\in C^{(1)}(U\to V)$.
            \item $\forall x\in U~\varphi'(x)=-(\Phi'_y(x;y))^{-1}\Phi'_x(x;y)\Big|_{y=\varphi(x)}$
        \end{enumerate}
        \begin{Proof}
            Пусть $F(x;y)=(x;\Phi(x;y))$. Это самое $F\in C^{(1)}(D\to\mathbb R^{n+m})$. У $F$ матрица Якоби квадратная, в отличие от $\Phi$. Давайте найдём её.
            $$F'(x;y)=\arr{c|c}{E_n & \mathbb0_{n\times m}\\\hline\Phi'_x(x;y) & \Phi'_y(x;y)}$$
            Определитель блочной матрицы мы знаем чему равен. $\det F'(x;y)=\det\Phi'_y(x;y)$. То есть по условию $\det F'(x^0;y^0)\neq0$. А значит оператор $F(x^0;y^0)$ обратим. К $F$ можно применить теорему об обратном отображении. Она даёт нам, что существует такое $r>0$, что
            \begin{enumerate}
                \item $\forall (x;y)\in W=B((x^0;y^0);r)~F'(x;y)$ обратим.
                \item $F\big|_W$ обратимо.
            \end{enumerate}
            Это то что нужно нам прямо сейчас, остальными утверждениями теоремы мы будем пользоваться по необходимости. Дальше пусть $U_1=B\left(x^0;\frac r{\sqrt2}\right)$, $V=B\left(y^0;\frac r{\sqrt2}\right)$. Первое --- $n$-мерный шар, второе --- $m$-мерный. Тривиально, $U_1\times V\subset W$. $U_1\times V$ открыто, поэтому при действии $F$ его образ $W_1=F(U_1\times V)$ открыт (по следствию об открытом отображении).\\
            Мы знаем, что $(x^0;y^0)\in U_1\times V$, $\Phi(x^0;y^0)=\mathbb0_m$. Отсюда $F(x^0;y^0)=(x^0;\mathbb0_m)\in W_1$. Значит, поскольку $W_1$ открыто, $F(x^0;y^0)$ лежит в этом множестве с некоторой окрестностью. В частности, $\exists U$ --- окрестность $x^0~\forall x\in U~(x;\mathbb0_m)\in W_1$. Хочется сказать, что $U$ и $V$ искомые. Проверим утверждения.\\
            \begin{enumerate}
                \item Если $x\in U$, то $(x;\mathbb0_m)\in W_1$, тогда (поскольку $F\big|_W$ обратимо) $F^{-1}(x;\mathbb0_m)=(u;y)\in U_1\times V$. Что это означает? Что $F(u;y)=(x\mathbb0_m)$. Это значит, что $u=x$ и $\Phi(u;y)=\mathbb0$. Мы нашли такой $y\in V$, что он является корнем уравнения. Почему он единственный такой? Пусть есть ещё один --- $\tilde y$ такой что он тоже корень. Тогда, поскольку $\Phi(x;\tilde y)=\mathbb0$, $F(x;\tilde y)=(x;\mathbb0_m)$. Но и $F(x;\tilde y)=(x;\mathbb0_m)$, а $F\big|_W$ обратимо. Что-то не сходится.
                \item Давайте посмотрим на то, что мы делали, чтобы получить $y$ из $x$. Мы сначала приписали к нему $\mathbb0_m$, получив точку из $W_1$. Потому применили $F^{-1}$, получив $(u;v)$, из которого оставили только $v$. То есть
                $$\varphi\colon \underset{\in U}x\longmapsto\underset{\in W_1}{(x;\mathbb0_m)}\overset{F^{-1}}\longmapsto\underset{\in U_1\times V}{(u;v)}\longmapsto\underset{\in V}{v}$$
                Ну, первое и третье преобразования лежат в $C^{(\infty)}$ (они линейны), а $F^{-1}\in C^{(1)}$. Значит и $\varphi\in C^{(1)}$.
                \item Известно, что $\Phi(x;\varphi(x))\equiv\mathbb0_m$ на множестве $U$. Давайте продифференцируем это равенство:
                $$
                \Phi'_x(x;\varphi(x))+\Phi'_y(x;\varphi(x))\varphi'(x)=\mathbb0_{m\times n}
                $$
                Это ли не то, что нам надо?
            \end{enumerate}
        \end{Proof}
        \thm Если $\Phi\in C^{(r)}$, то и $\varphi\in C^{(r)}$.
        \begin{Proof}
            Из доказательства второго пункта, где $F^{-1}$ имеет тот же порядок гладкости, что и $F$.
        \end{Proof}
        \thm При $n=m=1$ $\varphi'(x)=-\frac{\Phi_x(x;y)}{\Phi_y(x;y)}\bigg|_{y=\varphi(x)}$.
        \begin{Comment}
            Давайте жить в $n=m-1$. Как найти вторую производную $\varphi$? А вот так:
            $$
            \Phi'_x(x;y)+\Phi'_y(x;y)\varphi'(x)=0
            $$
            $$
            \Phi''_{x^2}(x;y)+2\Phi''_{xy}(x;y)\varphi'(x)+\Phi''_{y^2}(x;y){\varphi'}^2(x)+\Phi'_y(x;y)\varphi''(x)=0
            $$
        \end{Comment}
        \begin{Comment}
            Теперь как при помощи этого искать экстремум?
        \end{Comment}
        \dfn Пусть $F\colon\underset{\subset\mathbb R^{n+m}}D\to\mathbb R$, $\Phi\colon D\to\mathbb R^m$. $x^0\in D$. Тогда $x^0$ называется \undercolor{red}{точкой условного/относительного максимума} $f$ при \undercolor{red}{условии/уравнении связи} $\Phi(x)=\mathbb0$, если $\Phi(x^0)=\mathbb0_m$ и $\exists V_{x^0}~\forall x\in V_{x^0}\cap D~\Phi(x)=\mathbb0$ и $f(x)\leqslant f(x^0)$.
        \dfn Аналогично определяются \undercolor{red}{точка условного минимума}, \undercolor{red}{точка строгого условного максимума},\\\undercolor{red}{точка строгого условного минимума}, \undercolor{red}{точка условного экстремума}.
        \begin{Comment}
            Когда $D$ открыто, во всех определениях можно не писать $V_{x^0}\cap D$, а можно $V_{x^0}$.
        \end{Comment}
        \begin{Comment}
            Что это значит геометрически? В хороших случаях уравнение связи задаёт некоторый геометрический объект (например, при $n=m=1$ это кривая на плоскости, при $n=2,m=1$ --- поверхность в пространстве, при $n=1,m=2$ --- кривая в пространстве). И экстремум мы ищем на этой самой кривой/поверхности.\\
            Откуда такая задача возникает (кроме как если дана сама по себе)? Представим себе некоторый компакт, на котором хочется найти наибольшее и наименьшее значение функции. Во внутренних точках мы знаем, что делать, ищем стационарные точки. Но нужно исследовать границу. А граница состоит из кусков поверхностей (в случае цилиндра, например, боковая поверхность и две крышки). У каждой поверхности есть внутренние точки, которые мы изучаем исходя отсюда, а потом смотрим на границу поверхности (две окружности). С двумя окружностями всё хорошо, у них нет границ. А если бы мы жили на конусе, у нас возникла бы ещё одна подозрительная точка --- его вершина.
        \end{Comment}
        \begin{Comment}
            Пусть $D$ открыто, $f,\Phi\in C^{(1)}$. Пусть $\rank\Phi'(x^0)=m$. Не уменьшая общности будем считать, что ненулевой минор порядка $m$ образован частными производными по последним $m$ переменным. Обозначим их $y^1,\ldots,y^m$. Первые $n$ переменных оставим называться $x_1;\ldots;x_n$. Тогда $\det\Phi_y'(x;y)\neq0$ (ненулевой минор). А значит по теореме о неявном отображении $\exists U$ --- окрестность $x^0$, $\exists V$ --- окрестность $y^0$, $\exists\varphi\in C^{(1)}(U\to V)$ --- неявное отображение, $\Phi(x;\varphi(x))=\mathbb0_m$. Пусть $g(x)=f(x;\varphi(x))$. Тогда $(x^0;y^0)$ --- точка условного экстремума для $f$ при условии $\Phi(x;y)=\mathbb0_m$, тогда и только тогда, когда $x^0$ --- точка безусловного экстремума $g$.\\
            Тут есть проблема в том, что нам нужно брать экстремум $g$, а она зависит от неявно заданной функции, не очень удобно дифференцировать. Хочется как-то проще жить с этим и для этого есть...
        \end{Comment}
        \begin{Comment}
            Метод неопределённый множителей Лагранжа. Что такое $g'(x^0)=\mathbb0_n$? Это
            $$
            f'_x(x^0;y^0)+f'_y(x^0;y^0)\varphi(x^0)=\mathbb0_n
            $$
            А что даёт нам условие $\Phi(x;\varphi(x))=\mathbb0_m$? Его можно продифференцировать и получить
            $$
            \Phi'_x(x^0;y^0)+\Phi'_y(x^0;y^0)\varphi'(x^0)=\mathbb0_{m\times n}
            $$
            Заведём себе произвольный вектор-строку $\lambda\in\mathbb R^m$, и умножим на него уравнение выше:
            $$
            \lambda\Phi'_x(x^0;y^0)+\lambda\Phi'_y(x^0;y^0)\varphi'(x^0)=\mathbb0_n
            $$
            Теперь из первого равенства вычтем последнее:
            $$
            (f'_x(x^0;y^0)-\lambda\Phi'_x(x^0;y^0))+(f'_y(x^0;y^0)-\lambda\Phi'_y(x^0;y^0))\varphi'(x^0)=\mathbb0_n
            $$
            Хочется избавиться от второго слагаемого, чтобы не оперировать отображением, заданным неявно. Ну, так мы можем подобрать $\lambda=f'_y(x^0;y^0)(\Phi'_y(x^0;y^0))^{-1}$, раз $\Phi'_y(x^0;y^0)$ --- обратимый оператор. Тогда от нашего уравнения останется
            $$
            (f'_x(x^0;y^0)-\lambda\Phi'_x(x^0;y^0))=\mathbb0_n
            $$
            Что всё это нам дало? А вот что:
        \end{Comment}
        \thm \undercolor{darkgreen}{Необходимое условие относительного экстремума}. Пусть $D$ открыто в $\mathbb R^{n+m}$, $f\in C^{(1)}(D\to\mathbb R)$, $\Phi\in C^{(1)}(D\to\mathbb R^m)$, $x^0\in D$, $\rank\Phi'(x^0)=m$, $x^0$ --- точка относительного экстремума $f$ при условии связи $\Phi(x)=\mathbb0_m$. Тогда $\Phi(x^0)=\mathbb0_m$ и $\exists\lambda\in\mathbb R^m~f'(x^0)-\lambda\Phi'(x^0)=\mathbb0_{n+m}$
        \begin{Proof}
            Доказательство в двух комментариях выше.
        \end{Proof}
        \dfn $\lambda=\matr{\lambda_1 & \cdots & \lambda_m}$, $\lambda_1$ называются множителями Лагранжа.
        \begin{Comment}
            Итого имеем систему
            $$
            \left\{\begin{aligned}
                &f'(x^0)-\lambda\Phi'(x^0)=\mathbb0_{n+m}\\
                &\Phi(x^0)=\mathbb0_m
            \end{aligned}\right.
            $$
            В ней $n+2m$ уравнений и $n+2m$ неизвестных (все $x$ и все $\lambda$). Решаем --- получаем точки, подозрительные на экстремум. И ещё подозрительными на экстремум являются те точки, где $\rank\Phi'(x^0)<m$.
        \end{Comment}
        \dfn $F(x;\lambda)=f(x)-\lambda\Phi(x)$ --- \undercolor{red}{функция Лагранжа}.
        \begin{Comment}
            Тогда систему $$
            \left\{\begin{aligned}
                &f'(x^0)-\lambda\Phi'(x^0)=\mathbb0_{n+m}\\
                &\Phi(x^0)=\mathbb0_m
            \end{aligned}\right.
            $$
            можно трактовать как равенство нулю всех частных производных функции Лагранжа (первые $n+m$ уравнений --- производные по $x$, последние $m$ --- по $\lambda$). Хотя большой пользы в этой трактовке нет, в целом почему бы и не трактовать так.
        \end{Comment}
        \begin{Example}
            Максимум и минимум квадратичной формы на единичной сфере.\\
            Пусть $f(x)=\dotprod{Ax}x=\sum\limits_{i,j=1}^na_ijx_ix_j$. Матрицу $A$ считаем симметричной. Хочется найти её минимум и максимум на $D=\mathbb S^{n-1}\colon\sum\limits_{i=1}^nx_i^2=1$. По теореме Вейерштрасса минимум и максимум на компакте достигаются. Что тогда такое $\Phi(x)$? Это
            $$
            \Phi(x)=\sum\limits_{i=1}^nx_i^2-1
            $$
            Составим функцию Лагранжа:
            $$
            F(x;\lambda)=f(x)-\lambda\Phi(x)=\sum\limits_{i,j=1}^na_{ij}x_ix_j-\lambda\left(\sum\limits_{i=1}^nx_i^2-1\right)
            $$
            Было условие на ранг матрицы Якоби. $\Phi'_{x_k}(x)=2x_k$, а значит $\grad\Phi(x)=2x$, что на сфере не равно нулевому вектору. Теперь давайте дифференцировать $F$:
            $$
            F'_{x_k}(x;\lambda)=2\sum\limits_{\substack{i=1\\i\neq k}}^na_{ki}x_i+2a_{kk}x_k-\lambda2x_k=2\sum\limits_{i=1}^na_{ki}x_i-2\lambda x_k
            $$
            Это должно равняться нулю для любого $k\in[1:n]$. То есть для любого $k$
            $$
            \sum\limits_{i=1}^na_{ki}x_i=\lambda x_k
            $$
            Да это же значит, что $Ax=\lambda x$. То есть точки, подозрительные на экстремум --- собственные числа $A$. Тогда чему равно значение в экстремальной точке?
            $$\dotprod{Ax}{x}=\dotprod{\lambda x}{x}=\lambda|x|^2=\lambda$$
        \end{Example}
        \thm Наибольшее/наименьшее значение квадратичной формы на единичной сфере равно наибольшему/наименьшему собственному числу симметричной матрицы формы и достигается на соответствующем собственном векторе.
        \thm $\|\scriptA\|=\max\limits_{\lambda\text{ --- собственное число }\scriptA^*\scriptA}\sqrt\lambda$.
        \begin{Proof}
            Пусть $\scriptA\in\scriptL(\mathbb R^n\to\mathbb R^m)$. Рассмотрим $\scriptA^*\scriptA\in\scriptL(\mathbb R^n)$. Это оператор, тривиально, симметричный. Тогда
            $$
            \|\scriptA\|^2=\max\limits_{|x|=1}|\scriptA x|^2=\max\limits_{|x|=1}\dotprod{\scriptA x}{\scriptA x}=\max\limits_{|x|=1}\dotprod{\scriptA^*\scriptA x}{x}
            $$
            А это, как мы знаем, равно наибольшему собственному числу матрицы $A^TA$.
        \end{Proof}
        \begin{Comment}
            Предыдущий пример был для компакта (сферы). А что делать, если множество не компактно?
        \end{Comment}
        \begin{Example}
            Расстояние от точки до гиперплоскости.\\
            Пусть $a\in\mathbb R^n$, $a\neq\mathbb0_n$. Пусть $b\in\mathbb R$. Пусть $L=\left\{x\in\mathbb R^n~\middle|~\dotprod{a}{x}+b=0\right\}$. Пусть $c\in\mathbb R^n$. Хочется найти $\rho(c;L)=\inf\limits_{x\in L}|x-c|$.\\
            Это задача на условный экстремум:
            $$
            \Phi(x)=\dotprod{x}{a}+b=\sum\limits_{i=1}^na_ix_i+b
            $$
            Но минимизировать мы хотим не $|x-c|$, а $f(x)=|x-c|^2$. Тогда
            $$
            \Phi'_{x_k}(x)=a_k\qquad\qquad\grad\Phi(x)=a\neq\mathbb0_n
            $$
            $$
            F(x;\lambda)=f(x)-\lambda\Phi(x)=\sum\limits_{i=1}^n(x_i-c_i)^2-\lambda\left(\sum\limits_{i=1}^na_ix_i+b\right)
            $$
            $$
            F'_{x_k}(x;\lambda)=2(x_i-c_i)-\lambda a_k=0
            $$
            Это верно для всех $k\in[1:n]$. Умножим это на $a_k$, после чего просуммируем по $k$:
            \[\begin{split}
                2\underbrace{\sum\limits_{k=1}^na_kx_i}_{=-b}-2\sum\limits_{k=1}^na_kc_i-\lambda\sum\limits_{k=1}^na_k^2&=0\Leftrightarrow\\\Leftrightarrow
                -2b-2\sum\limits_{k=1}^na_kc_i-\lambda\sum\limits_{k=1}^na_k^2&=0\Leftrightarrow\\\Leftrightarrow
                \frac\lambda2=\frac{-\sum\limits_{i=1}^na_ic_i-b}{-\sum\limits_{j=1}^na_j^2}&
            \end{split}\]
            Отсюда одно решение:
            $$
            x_k=c_k+\frac\lambda2a_k
            $$
            Обозначим то одно решение за $x^0$. Тогда
            $$M=f(x^0)=\sum\limits_{k=1}^n(x_k-c_k)^2=\sum\limits_{k=1}^n\left(\frac\lambda2a_k\right)^2=\frac{\left(\sum\limits_{i=1}^na_ic_i+b\right)^2}{\sum\limits_{j=1}^na_j^2}$$
            Дальше надо понять, как жить с этим экстремумом. Ну, тривиально, $f(x)\underset{x\to\infty}\longrightarrow+\infty$, значит при $x:|x|\geqslant R$ функция принимает значение больше $M$. При этом $\inf_Lf=\inf\limits_{\underbrace{L\cap B(\mathbb0;R)}_{\text{компакт}}}f=\min\limits_{L\cap B(\mathbb0;R)}f=f(x^*)$. Если $|x^*|=R$, то $f(x^*)>M=f(x^0)$, противоречие. Если $|x^*|<R$, то $x^*$ удовлетворяет системе из метода Лагранжа. А такая точка ровно одна, $x^0$. Отсюда $x^*=x^0$, значит $x^0$ действительно минимум.\\
            Итого ответ ---
            $$
            \rho(c;L)=\frac{\left|\sum\limits_{i=1}^na_ic_i+b\right|}{\sqrt{\sum\limits_{j=1}^na_j^2}}
            $$
            Несмотря на то что сразу теорема Вейерштрасса не применялась, мы чуть-чуть подумали и свели задачу к задаче минимума на компакте.
        \end{Example}
        \thm \undercolor{darkgreen}{Достаточные условия относительного экстремума}. Пусть $D$ открыто в $\mathbb R^{n+m}$, $f\in C^{(2)}(D\to\mathbb R)$, $\Phi\in C^{(2)}(D\to\mathbb R^m)$. Пусть $x^0\in D$, $\Phi(x^0)=0$, $\rank\Phi'(x^0)=m$. Пусть $\lambda\in\mathbb R^m$, $f'(x^0)-\lambda\Phi'(x^0)=\mathbb0_{n+m}$. Обозначим $F(x)=f(x)-\lambda\Phi(x)$, $T=\{h\in\mathbb R^{n+m}\mid\Phi'(x^0)h=\mathbb0_m\}$. Тогда
        \begin{enumerate}
            \item Если $\mathrm d^2F>0$ на $T$, то $x^0$ --- точка строгого условного минимума на $f$.
            \item Если $\mathrm d^2F<0$ на $T$, то $x^0$ --- точка строгого условного максимума на $f$.
            \item Если $\mathrm d^2F\lessgtr0$ на $T$, то $x^0$ --- не точка условного экстремума на $f$.
        \end{enumerate}
        \begin{Proof}
            Вернёмся к обозначениям $(x^0;y^0)$. Как было при выводе необходимых условий. Теперь у нас не одно приращение $h$, а два --- $h\in\mathbb R^n$, $k\in\mathbb R^m$. Тогда $\Phi(x;\varphi(x))=\mathbb0_m$, $g(x)=f(x;\varphi(x))$. Пусть $\psi(x)=(x;\varphi(x))$, чтобы меньше писать.\\
            Безусловные экстремумы $g$ являются условными $f$. Для этого найдём дифференциал $g$ в точке $x^0$.
            $$
            \mathrm d^1g(x;h)=\mathrm df(\psi(x);\mathrm d\psi(x;h))
            $$
            \[\begin{split}
                \mathrm d^2g(x;h)&=\mathrm d(\mathrm d^1g(\cdot;h))(x;h)=\\
                &=\mathrm d^2f(\psi(x);\mathrm d\psi(x;h))+\mathrm df(\psi(x);\mathrm d^2\psi(x;h))
            \end{split}\]
            То же самое для $\mathrm d^2(\Phi\circ\psi)(x;h)=0$. То есть
            $$
            0\equiv\mathrm d^2(\lambda(\Phi\circ\psi))(x;h)+\mathrm d\lambda\Phi(\psi(x);\mathrm d^2\psi(x;h))
            $$
            Давайте вычтем предыдущее из этого и подставим $x=x^0$, потому что всё считаем в подозрительной точке. При этом $\mathrm d^1F((x^0;y^0);\cdot)\equiv0$, а значит.
            $$
            \mathrm d^2g(x^0;h)=\mathrm d^2F((x^0;y^0);\mathrm d\psi(x^0;h))
            $$
            То есть нам нужно посмотреть на определённость формы $\mathrm d^2F$, но не везде, а только с приращениями вида $\mathrm d\psi(x^0;h)$.
            $$
            \mathrm d\psi(x^0;h)=\psi'(x^0)h=\matr{h\\\varphi'(x^0)h}
            $$
            То есть $k=\varphi'(x^0)h$. Отсюда
            \[\begin{split}
                \Phi'(x^0;y^0)\matr{h\\k}=\Phi'_x(x^0;y^0)h+\Phi'_y(x^0;y^0)k=\mathbb0\Leftrightarrow\\\Leftrightarrow
                k=-(\Phi'_y(x^0;y^0))^{-1}\Phi'_x(x^0;y^0)h
            \end{split}\]
            Также
            \[\begin{split}
                \varphi'(x)=-(\Phi'_y(x;y))^{-1}\Phi'_x(x;y)\Big|_{y=\varphi(x)}\Rightarrow\\\Rightarrow
                \varphi'(x^0)=-(\Phi'_y(x^0;y^0))^{-1}\Phi'_x(x^0;y^0)
            \end{split}\]
            Тут мы получили, что приращения вида $\mathrm d\psi(x^0;h)$ --- это точно множество $T$, то есть то, что мы и хотели.
        \end{Proof}
    \end{itemize}
\end{document}
