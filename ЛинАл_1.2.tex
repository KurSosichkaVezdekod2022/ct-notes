\documentclass{article}
\input{Headers/header}
\input{Headers/old-formal}

\usepackage[e]{esvect}
\let\vec\vv

% \usepackage[e]{adjustbox}

\usepackage[customcolors]{hf-tikz}

\geometry{legalpaper, paperheight=16383pt, margin=1in}
\setcounter{totalnumber}{100}
\pagestyle{empty}

\let\complexIm\Im
\DeclareMathOperator{\operIm}{Im}
\undef{\Im}
\let\Im\operIm

\undef\id
\newcommand{\id}{{\mathcal{E}}}

\begin{document}
    \section{Линейные отображения.}
    \paragraph{\undercolorblack{orange}{Основные определения}. \undercolorblack{orange}{Теорема о ранге и дефекте линейного отображения}.}
    \begin{itemize}
        \dfn Пусть $V$ и $U$ --- линейные пространства над $\scriptK$. Тогда $\scriptA\colon U\to V$ называется \undercolor{red}{линейным отображением}, если оно однородно ($\forall\lambda\in\scriptK~\forall u\in U~\scriptA(\lambda u)=\lambda\scriptA(u)$) и аддитивно ($\forall u_1;u_2\in U~\scriptA(u_1+u_2)=\scriptA(u_1)+\scriptA(u_2)$).
        \begin{Comment}
            В линейной алгебре не принято писать $\scriptA(u)$, а принято $\scriptA u$.
        \end{Comment}
        \thm Если $\mathbb0_U$ --- ноль в $U$, $\mathbb0_V$ --- ноль в $V$, то $\scriptA\mathbb0_U=\mathbb0_V$.
        \begin{Proof}
            Ну, очевидно, $\scriptA\mathbb0_U=\scriptA(u-u)=\scriptA u-\scriptA u=\mathbb0_V$.
        \end{Proof}
        \begin{Example}
            Очевидно, тождественное отображение $\mathrm{id}_U\colon\substack{U\to U\\u\mapsto u}$ является линейным.
        \end{Example}
        \begin{Example}
            Ещё можно рассмотреть нулевой оператор, который всегда возвращает ноль в поле $V$. Сам этот оператор также обозначается $\mathbb0$. То есть $\mathbb0u=\mathbb0_V$.
        \end{Example}
        \begin{Example}
            Очевидно, изоморфизм между двумя пространствами является линейным отображением между ними.
        \end{Example}
        \begin{Example}
            На пространствах $P_n$ и $P_{n-1}$ (многочлены степени не выше $n$/$n-1$) можно считать $\frac{\mathrm d}{\mathrm dt}$ линейным оператором. Как мы знаем, $(\lambda f(t))'=\lambda f'(t)$ и $(f(t)+g(t))'=f'(t)+g'(t)$.
        \end{Example}
        \begin{Example}
            Ещё давайте рассмотрим матрицу $A\in\scriptK^{m\times n}$. Тогда $\scriptA\colon\substack{\scriptK^n\to\scriptK^m\\x\mapsto Ax}$. Очевидно, так умножать можно, очевидно, умножение на матрицу линейно. Значит $\scriptA$ --- линейное отображение.
        \end{Example}
        \begin{Comment}
            Множество всех линейных отображений $U\to V$ обозначают $L(U;V)$, либо $\Hom(U;V)$, либо $\Hom_\scriptK(U;V)$, если нужно указать поле, над которым взяты $U$ и $V$.
        \end{Comment}
        \dfn \undercolor{red}{Суммой линейных отображений} $\scriptA$ и $\scriptB\in\Hom(U;V)$ называют отображение $\scriptA+\scriptB$ такое, что $(\scriptA+\scriptB)u=\scriptA u+\scriptB u$.
        \dfn \undercolor{red}{Умножением линейного отображения} $\scriptA$ \undercolor{red}{на скаляр} $\lambda\in\scriptK$ называется отображение $\lambda\scriptA$ такое, что $(\lambda\scriptA)u=\lambda(\scriptA u)$.
        \thm Очевидно, что $\scriptA+\scriptB\in\Hom(U;V)$ и $\lambda\scriptA\in\Hom(U;V)$.
        \thm Также очевидно, что с указанными сложением и умножением на скаляр множество $\Hom_\scriptK(U;V)$ является линейным пространством над $\scriptK$.
        \dfn \undercolor{red}{Образ линейного отображения} $\scriptA$ --- множество $\Im\scriptA=\{\scriptA u\mid u\in U\}$.
        \dfn \undercolor{red}{Ранг линейного оператора} ($\rank\scriptA$) --- размерность его образа (если тот конечномерный).
        \dfn \undercolor{red}{Ядро линейного отображения} $\scriptA$ --- множество $\Ker\scriptA=\{u\in U\mid\scriptA u=\mathbb0_V\}$.
        \dfn Размерность ядра называется \undercolor{red}{дефектом линейного оператора} ($\Def\scriptA$).
        \thm $\scriptA$ является изоморфизмом тогда и только тогда, когда $\Im\scriptA=V$ и $\Ker A=\{\mathbb0_U\}$.
        \begin{Proof}
            Что такое изоморфизм? Это линейная биекция. Линейность у нас есть, сюръективность и инъективность. Сюръективность равносильна $\Im\scriptA=V$ (просто по определению). С инъективностью чуть сложнее. Если $\scriptA$ инъективно, значит в $\mathbb0_V$ переходит только один элемент (его мы знаем, это $\mathbb0_U$). Если $\Ker\scriptA=\{\mathbb0_U\}$, то нужно доказать инъективность. Если инъективности нет и $u_1$ и $u_2$ переходят в один и тот же $v$, то $\scriptA(u_1-u_2)=v-v=\mathbb0_V$, а это противоречие с $\Ker\scriptA=\{\mathbb0_U\}$.
        \end{Proof}
        \dfn \undercolor{red}{Эндоморфизм} --- гомоморфизм из пространства само в себя. Множество всех эндоморфизмов обозначается $\End_\scriptK(V)$ или $\End(V)$. Также эндоморфизм называют \undercolor{red}{линейным оператором}.
        \dfn \undercolor{red}{Автоморфизм} --- эндоморфизм и изоморфизм (то есть изоморфизм из пространства в себя). Множество всех автоморфизмов обозначается $\Aut_\scriptK(V)$ или $\Aut(V)$.
        \begin{Example}
            Рассмотренное нами $\mathrm{id}_V$ --- автоморфизм на $V$.\\
            Отображение дифференцирования не инъективно (его ядро состоит из многочленов, равных константе), а сюръективно будет только если рассмотреть его $P_n\to P_{n-1}$. Если $P_n\to P_n$ --- то не будет. Зато будет эндоморфизмом.\\
            Умножение на матрицу $A\in\scriptK^{m\times n}$ является эндоморфизмом только если $m=n$, инъекцией --- если $\rank A=m$, сюръекцией --- если $\rank A=n$.
        \end{Example}
        \dfn Пусть $\scriptA\in\Hom_\scriptK(W;V)$, а $\scriptB\in\Hom_\scriptK(U;W)$. Тогда \undercolor{red}{произведение линейных операторов} $\scriptA\cdot\scriptB$ --- это композиция $\scriptA\circ\scriptB$.
        \thm Очевидно, если $W=V$, то $\cdot$ имеет правый нейтральный элемент ($\id_V$), а если $W=U$, то левый ($\id_U$).
        \thm Очевидно, что для изоморфизмов $\scriptA$ и $\scriptB$ их произведение также изоморфизм.
        \thm Очевидно, $\scriptA(\scriptB_1+\scriptB_2)=\scriptA\scriptB_1+\scriptA\scriptB_2$. Аналогично очевидна правая дистрибутивность.
        \thm Очевидно, что умножение ассоциативно.
        \dfn Очевидно, если $\scriptA\in\Hom(U;V)$ --- изоморфизм, существует \undercolor{red}{обратное} ему \undercolor{red}{отображение} $\scriptA^{-1}$.
        \thm Очевидно, что $\scriptA^{-1}\in\Hom(V;U)$ и также изоморфизм.
        \thm Очевидно, что $\scriptA\scriptA^{-1}=\id_V$, $\scriptA^{-1}\scriptA=\id_U$.
        \dfn \undercolor{red}{Сужение линейного отображения} $\scriptA\in\Hom(U;V)$ на пространство $U_0\subset U$ --- линейное отображение (очевидно) $\scriptA\big|_{U_0}\colon\substack{U_0\to V\\u\mapsto\scriptA u}$.
        \thm Очевидно, если $\scriptA$ изоморфизм, то $\scriptA\big|_{U_0}$, рассмотренный как $U_0\to\Im\big|_{U_0}$, также изоморфизм.
        \thm \undercolor{darkgreen}{Теорема о ранге и дефекте}. Пусть $\scriptA\in\Hom(U;V)$, $U$ и $V$ --- конечномерны, тогда $\dim U=\rank\scriptA+\Def\scriptA$.
        \begin{Proof}
            Пусть $U_0=\Ker\scriptA$. Очевидно, $\Ker\scriptA\subset U$. У любого подпространства существует прямое дополнение. Обозначим дополнение к $U_0$ за $U_1$. Раз уж $U=U_0\oplus U_1$, $\dim U=\dim U_0+\dim U_1$. При этом $\dim U_0=\Def\scriptA$.\\
            Теперь докажем, что $U_1\cong\Im\scriptA$, что докажет нам теорему. Как докажем? Посмотрим на $u\in U$. Понятно, что мы можем единственным образом разложить его как $u_0+u_1$, где $u_1\in U_1$, $u_0\in U_0$. Несложно заметить, что $\scriptA u=\scriptA u_1$. А это значит, что $\Im\scriptA=\Im\scriptA\big|_{U_1}$. Теперь докажем, что $U_1\cong\Im\scriptA\big|_{U_1}$. Обозначим $\scriptA\big|_{U_1}$ за $\scriptA_1$, чтобы меньше писать. Как докажем, что $U_1\cong\Im\scriptA_1$? Предъявим изоморфизм явно. Это, барабанная дробь, $\scriptA_1$. Его сюръективность очевидна. Докажем инъективность. Для этого докажем тривиальность ядра. Рассмотрим $w\in\Ker\scriptA_1$. Как несложно понять, $w\in U_1$. В то же время, $\scriptA_1w=\mathbb0_V$, что значит что и $\scriptA w=\mathbb0_V$. Но это значит, что $w\in\Ker\scriptA$, то есть $w\in U_0$. Что значит, что $w\in U_0\cap U_1$, а пересечение дизъюнктных подпространств равно $\{\mathbb0_U\}$. Что и значит, что в $\Ker\scriptA_1$ есть только один элемент --- $\mathbb0_U$, значит ядро $\scriptA_1$ действительно тривиально.
        \end{Proof}
        \thm \undercolor{darkgreen}{Характеристики изоморфизма}. Если $\scriptA\in\Hom(U;V)$, то следующие утверждения равносильны:
        \begin{enumerate}
            \item $\scriptA$ --- изоморфизм.
            \item $\dim U=\dim V=\rank A$.
            \item $\dim U=\dim V\land\Def\scriptA=0$.
        \end{enumerate}
        \thm \undercolor{darkgreen}{Характеристики автоморфизма}. Если $\scriptA\in\End(V)$, то следующие утверждения равносильны:
        \begin{enumerate}
            \item $\scriptA\in\Aut(V)$.
            \item $\dim V=\rank A$.
            \item $\Def\scriptA=0$.
        \end{enumerate}
        \begin{Comment}
            Итак, давайте заметим следующее. Пространство $\End_\scriptK(V)$ является алгеброй с единицей (над $\scriptK$), а $\Aut_\scriptK(V)$ --- ещё и с делением.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Матрица линейного отображения}. \undercolorblack{orange}{Изоморфизм алгебр}.\\\undercolorblack{orange}{Преобразование матрицы отображения при смене базиса}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Пусть $\scriptA\in\Hom_\scriptK(U;V)$, $\xi_1;\ldots;\xi_n$ --- базис $U$, а $\eta_1;\ldots;\eta_m$ --- базис $V$. Тогда, как мы знаем, каждому вектору $U$ можно сопоставить координатный столбец из $\scriptK^n$, а каждому вектору $V$ --- столбец из $\scriptK^m$. Тогда $\scriptA u=\sum\limits_{i=1}^nu_1\scriptA\xi_i$. А это значит, что $\scriptA$ достаточно определить только на базисных векторах. К тому же, несложно заметить, что $\Im\scriptA=\Lin\{\scriptA\xi_1;\ldots;\scriptA\xi_n\}$. Также несложно заметить, что $\scriptA\xi_i\in V$. А это значит, что его можно разложить как $\sum\limits_{j=1}^ma_{ji}\eta_j$. Да это же матрица из $\scriptK^{m\times n}$!
        \end{Comment}
        \dfn Описанная выше матрица называется \undercolor{red}{матрицей линейного отображения} в базисах $(\xi;\eta)$.
        \begin{Example}
            Что будет, если рассмотреть $\id_V$ и $\eta=\xi$? Будет, что $\id_V\xi_i=\sum\limits_{j\neq i}^m0\xi_j+\xi_i$. То есть матрица отображения $\id_V$ в базисах $(\xi;\xi)$ --- это единичная матрица.\\
            А если $\eta\neq\xi$? Тогда, как несложно заметить, матрица оператора $\id_V$ будет просто матрицей перехода $T_{\eta\to\xi}$.
        \end{Example}
        \begin{Example}
            Давайте посмотрим на матрицу перехода дифференциального оператора. Для простоты рассмотрим его $P_2\to P_2$. Будем считать, что базисы одинаковые --- $\{1;t;t^2\}$. Что это даст нам? $\scriptA1=0\leftrightarrow\matr{0\\0\\0}$, $\scriptA x=1\leftrightarrow\matr{1\\0\\0}$, $\scriptA x^2=2x\leftrightarrow\matr{0\\2\\0}$. То есть матрица выглядит как $\matr{0 & 1 & 0\\0 & 0 & 2\\0 & 0 & 0}$. А если рассмотрим как $P_2\to P_1$, то матрица лишится последней строки: $\matr{0 & 1 & 0\\0 & 0 & 2}$.
        \end{Example}
        \begin{Comment}
            Пусть $v=\scriptA u$. Тогда $v=\sum\limits_{i=1}^nu_i\scriptA\xi_i=\sum\limits_{i=1}^n\sum\limits_{j=1}^mu_ia_{ji}\eta_j=\sum\limits_{j=1}^m\eta_j\sum\limits_{i=1}^nu_ia_{ji}$. Это разложение $v$ по $\eta$, а значит $v_i=\sum\limits_{i=1}^nu_ia_{ji}$, что значит $\mathrm v=A\mathrm u$, где $\mathrm u$ и $\mathrm v$ --- координатные столбцы $u$ и $v$ соответственно.
        \end{Comment}
        \begin{Example}
            Рассмотрим предыдущий комментарий с дифференциальным оператором. Рассмотрим $\frac{\mathrm d}{\mathrm dt}(at^2+bt+c)$. Это значит, что $\mathrm u=\matr{c\\b\\a}$. Комментарий утверждает, что $\mathrm v=\matr{0 & 1 & 0\\0 & 0 & 2}\matr{c\\b\\a}$. Ну, это правда, получается $\mathrm v=\matr{b\\2a}$, то есть $v=2at+b$.
        \end{Example}
        \begin{Example}
            Другой пример --- поворот плоскости на угол $\alpha$. Оба базиса берём $\{\vec i;\vec j\}$. Нам нужно всего лишь базисные вектора первого пространства повернуть на $\alpha$. Что получится? $\scriptA\vec i=\cos\alpha\vec i+\sin\alpha\vec j$, $\scriptA\vec j=-\sin\alpha\vec i+\cos\alpha\vec j$. Что даёт нам уже знакомую матрицу поворота: $\matr{\cos\alpha & -\sin\alpha\\\sin\alpha & \cos\alpha}$. Но тут у неё немного другой смысл. Тогда мы брали разные базисы и выражали один в другом. А тут у нас базис один и тот же, но мы преобразуем пространство, чего мы не делали тогда.
        \end{Example}
        \thm $\Hom_\scriptK(U;V)\cong\scriptK^{\dim V\times\dim U}$.
        \begin{Proof}
            Сначала докажем биективность. Линейному оператору мы уже сопоставили матрицу (в фиксированном базисе). Теперь наоборот. Ну, это понятно как сделать. Чтобы из матрицы получить линейное отображение, вектору сопоставим его координатный столбец, а столбцы свяжем умножением на матрицу. Получится та же самая формула, что в определении матрицы отображения.\\
            Осталось только проверить линейность. Проверим однородность. Рассмотрим отображение $\scriptA$, которому соответствует матрица $A$. Докажем, что отображению $\lambda\scriptA$ соответствует $\lambda A$. $(\lambda\scriptA)\xi_i=\lambda(\scriptA\xi_i)=\lambda\sum\limits_{j=1}^ma_{ji}\eta_j=\sum\limits_{j=1}^m(\lambda a_{ji})\eta_j$. А $\lambda a_{ji}$ --- как раз элементы матрицы $\lambda A$. Абсолютно аналогично проверяется аддитивность.
        \end{Proof}
        \begin{Comment}
            Помимо сложения и умножения на скаляр у матриц и у линейных отображений есть умножение друг на друга. Может, они тоже как-то согласованы?
        \end{Comment}
        \thm Пусть у нас есть пространства $U$, $W$ и $V$ с базисами $\xi$, $\zeta$ и $\eta$ соответственно. Также пусть $\scriptA\in\Hom(W;V)$ и $\scriptB\in\Hom(U;W)$. Тогда оператору $\scriptA\scriptB$ соответствует матрица $AB$.
        \begin{Proof}
            Матрица $B$ у нас такая, что $\scriptB\xi_i=\sum\limits_{k=1}^pb_{ki}\zeta_k$. Матрица $A$ --- такая, что $\scriptA\zeta_k=\sum\limits_{j=1}^ma_{jk}\eta_j$. А это значит, что $(\scriptA\scriptB)\xi_i=\scriptA\left(\sum\limits_{k=1}^pb_{ki}\zeta_k\right)=\sum\limits_{k=1}^pb_{ki}\scriptA\zeta_k=\sum\limits_{k=1}^pb_{ki}\sum\limits_{j=1}^ma_{jk}\eta_j=\sum\limits_{k=1}^p\sum\limits_{j=1}^ma_{jk}b_{ki}\eta_j=\sum\limits_{j=1}^m\eta_j\sum\limits_{k=1}^pa_{jk}b_{ki}=\sum\limits_{j=1}^m\eta_j(ab)_{ji}$.
        \end{Proof}
        \thm Пусть $\scriptA\in\Aut(V)$. При этом $V$ рассматривается с одинаковыми базисами. Тогда, если $\scriptA$ соответствует матрица $A$, то отображению $\scriptA^{-1}$ сопоставлена матрица $A^{-1}$.
        \begin{Proof}
            Пусть $\scriptA^{-1}$ соответствует матрице $B$. Тогда, поскольку $\scriptA\scriptA^{-1}=\id_V$, $AB=E$. Аналогично, $BA=E$.
        \end{Proof}
        \begin{Comment}
            Предыдущее доказательство верно исключительно, если на $V$ задан один базис, а не два разных. В противном случае получится, что $AB$ равна матрице перехода.
        \end{Comment}
        \begin{Comment}
            Таким образом, изоморфизм между отображениями является изоморфизмом ещё и в смысле алгебр. А если брать изоморфизм между автоморфизмами и матрицами, то сохранятся будет ещё и единица и обратных элементов.
        \end{Comment}
        \thm \undercolor{darkgreen}{Преобразование матрицы линейного отображения при смене базиса}. Пусть $\scriptA\in\Hom(U;V)$. Пусть в $U$ задано два базиса: $\xi$ и $\xi'$, а в $V$ --- $\eta$ и $\eta'$. Тогда матрица $A$ линейного отображения в базисах $(\xi;\eta)$ связана с матрицей $A'$ линейного отображения в базисах $(\xi';\eta')$ следующим равенством: $A'=T^{-1}_{\eta\to\eta'}AT_{\xi\to\xi'}$.
        \begin{Proof}
            Доказательство тривиально следует из композиции. Очевидно, $\scriptA=\id_V\scriptA\id_U$. Возьмём разные базисы на этом пути:
            \begin{center}
                \begin{tikzpicture}
                    \node (U) {$U$};
                    \node[below=of U] (U') {$U$};
                    \node[right=of U] (V) {$V$};
                    \node[below=of V] (V') {$V$};
                    \node[above left] at (U) {$\xi$};
                    \node[below left] at (U') {$\xi'$};
                    \node[above right] at (V) {$\eta$};
                    \node[below right] at (V') {$\eta'$};
                    \draw[->] (U') -- (U) node[midway, left] {$\id_U$};
                    \draw[->] (U) -- (V) node[midway, above] {$\scriptA$};
                    \draw[->] (V) -- (V') node[midway, right] {$\id_V$};
                    \draw[->] (U') -- (V') node[midway, below] {$\scriptA$};
                    \draw[->, red] (U'.north east) to[out=90, in=90, looseness=3.5] (V'.north west);
                \end{tikzpicture}
            \end{center}
            Получим, что $A'=T_{\eta'\to\eta}AT_{\xi\to\xi'}=T^{-1}_{\eta\to\eta'}AT_{\xi\to\xi'}$.
        \end{Proof}
        \thm А если $A\in\End(V)$ и на $V$ заданы два базиса: $E$ и $E'$ (первому соответствует $A$, второму --- $A'$), то $A'=T^{-1}_{E\to E'}AT_{E\to E'}$.
        \dfn Если $A,B\in\scriptK^{n\times n}$, то $A$ и $B$ называются \undercolor{red}{подобными}, если $\exists C\in\scriptK^{n\times n}~B=C^{-1}AC$. 
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Инварианты линейного отображения}.}
    \begin{itemize}
        \dfn \undercolor{red}{Инвариантом} (\undercolor{red}{инвариантностью}) называется величина/свойство, которые не меняются при определённом классе преобразований.
        \begin{Example}
            Например, параллельность прямых является инвариантом относительно поворотов, отражений и параллельных переносов.
        \end{Example}
        \begin{Example}
            Равенство $\mathrm v=A\mathrm u$ является инвариантом относительно смены базиса. Это мы доказали только что.
        \end{Example}
        \begin{Comment}
            И вообще, нас глобально интересует, не меняются ли изучаемые нами алгебраические свойства при замене базиса.
        \end{Comment}
        \dfn \undercolor{red}{Образ матрицы} ($\Im A$) --- линейная оболочка её столбцов.
        \begin{Comment}
            Это логично, ведь это то же самое, что и $\{Ax\mid x\in\scriptK^n\}$.
        \end{Comment}
        \thm $\rank A=\dim\Im A$.
        \dfn $\Ker A=\{x\in\scriptK^n\mid Ax=\mathbb0\}$ --- \undercolor{red}{ядро матрицы}.
        \dfn \undercolor{red}{Дефект матрицы} $\Def A$ --- размерность её ядра.
        \thm Как мы уже доказывали, $\Def A=n-\rank A$. Это аналог \undercolor{darkgreen}{теоремы о ранге и дефекте для матриц}.
        \thm $\scriptA\in\Hom(U;V)$ тогда в каком бы базисе мы ни взяли $A$ --- матрицу $\scriptA$, $\Def A$ и $\rank A$ окажутся одинаковыми.
        \begin{Proof}
            Как мы знаем, $\rank A=\rank\Im A=\dim\Lin\{A_1;\ldots;A_n\}$. Заметим, что между $\scriptK^n$ и $U$ есть координатный изоморфизм ($A_i$ мы сопоставляем $\scriptA\xi_i$). А при изоморфизме базис переходит в базис, что значит, что $\dim\Lin\{A_1;\ldots;A_n\}=\dim\Lin\{\scriptA\xi_1;\ldots;\scriptA\xi_n\}=\rank\scriptA$. Таким образом ранг матрицы $A$ всегда равен рангу отображения $\scriptA$.\\
            Чтобы доказать то же свойство для дефекта, достаточно применить теорему о ранге и дефекте для матриц и для отображений.
        \end{Proof}
        \thm $\scriptA\in\Hom(U;V)$ является изоморфизмом тогда и только тогда, когда $A$ невырожденная (опять же, какой базис мы бы ни взяли).
        \begin{Proof}
            Очевидное следствие из предыдущего.
        \end{Proof}
        \dfn Пусть $\scriptA\in\End(V)$, $(e_1;e_2;\ldots;e_n)$ --- базис $V$. Тогда \undercolor{red}{определителем оператора} $\scriptA$ называется определитель системы векторов $\det\scriptA=\det(\scriptA e_1;\ldots;\scriptA e_n)$ (то есть такая $n$-форма, которая на наборе базисных векторов равна 1).
        \thm Определитель оператора $\scriptA$ равен определителю его матрицы ($A$) в \textbf{любом} же базисе.
        \begin{Proof}
            Докажем сначала для того же базиса: $\det\scriptA=\det(\scriptA e_1;\ldots;\scriptA e_n)=\det\left(\sum\limits_{j=1}^na_{j1}e_j;\ldots;\sum\limits_{j=1}^na_{jn}e_j\right)=\sum\limits_{j_1=1}^na_{j_11}\cdots\sum\limits_{j_n=1}^na_{j_nn}\det(e_{j_1};\ldots;e_{j_n})$. Но мы знаем, что определитель системы столбцов --- $n$-форма, а значит мы можем взять это как $\sum\limits_{\sigma\in S_n}^na_{\sigma_11}\cdots a_{\sigma_nn}\det(e_{\sigma_1};\ldots;e_{\sigma_n})$.\\
            Теперь рассмотрим некоторый другой базис $E'$. Тогда $A'=T^{-1}AT$. Но тогда $\det A'=\det(T^{-1})\det A\det T$. При этом $\det(T^{-1})\det T=\det E=1$. А значит определитель $A'$ и $A$ равны.
        \end{Proof}
        \thm Пусть $f$ --- $n$-форма. Тогда $\forall\xi_1;\ldots;\xi_n\in V~f(\scriptA\xi_1;\ldots;\scriptA\xi_n)=(\det\scriptA)f(\xi_1;\ldots;\xi_n)$.
        \begin{Proof}
            Несложно по определению проверить, что $g(\xi_1;\ldots;\xi_n)=f(\scriptA\xi_1;\ldots;\scriptA\xi_n)$ также является $n$-формой. А любая $n$-форма, как мы знаем, выражается как $g(e_1;\ldots;e_n)\det(\xi_1;\ldots;\xi_n)$. Чему равно $g(e_1;\ldots;e_n)$? Оно равно $f(\scriptA e_1;\ldots;\scriptA e_n)$. Дальше мы знаем, что $f$ также является $n$-формой, а значит $f(\scriptA e_1;\ldots;\scriptA e_n)=\sum\limits_{\sigma\in S_n}^na_{\sigma_11}\cdots a_{\sigma_nn}f(e_{\sigma_1};\ldots;e_{\sigma_n})$ (это достигается рассуждением, аналогичным предыдущей теореме). В таком случае мы как раз и получаем, что нам надо.
        \end{Proof}
        \thm Из предыдущего $\det(\scriptA\xi_1;\ldots;\scriptA\xi_n)=(\det\scriptA)\det(\xi_1;\ldots;\xi_n)$.
        \begin{Comment}
            Это своего рода доказательство $\det(AB)=\det A\det B$, мы делали так:
            $$\det(AB)=\det(AB_1;\ldots;AB_n)$$
            А это равно $\det A\det B$.
        \end{Comment}
        \thm Если $\scriptA,\scriptB\in\End(V)$, то $\det(\scriptA\scriptB)=\det\scriptA\det\scriptB$.
        \begin{Proof}
            Рассмотрим базис, возьмём матрицы, получим нужное нам свойство. Или окажем напрямую как указано в комментарии выше.
        \end{Proof}
        \thm $\scriptA\in\Aut(V)$ тогда и только тогда, когда $\det\scriptA\neq0$. При это $\det\scriptA^{-1}=\frac1{\det\scriptA}$.
        \begin{Proof}
            Как мы знаем, $\scriptA\scriptA^{-1}=\id_V$, а значит $\det\scriptA\det\scriptA^{-1}=1$. В общем, как в матрицах доказательство.
        \end{Proof}
        \begin{Example}
            Рассмотрим геометрические вектора размерности 3. Пусть $f(\vec a;\vec b;\vec c)=\vec a\vec b\vec c$. Мы уже знаем, что эта функция линейна по всем аргументам, к тому же при смене любых двух местами, ответ умножится на $-1$. А значит это 3-форма. Давайте ещё рассмотрим произвольный линейное оператор $\scriptA$. Тогда, как следует из доказанного, $f(\scriptA\vec a;\scriptA\vec b;\scriptA\vec c)=\det\scriptA f(\vec a;\vec b;\vec c)$. То есть объём параллелепипеда при линейных отображениях меняется на константу. Мы знаем два таких отображения:
            \begin{enumerate}
                \item Можно растянуть вектор на константу $\mu\in\mathbb R$ (преобразование подобия). Что тогда происходит с объёмом? Надо посчитать определок $\det\scriptA$. Чему будет равна матрица оператора $\scriptA$? Очевидно, $\matr{\mu & 0 & 0\\0 & \mu & 0\\0 & 0 & \mu}$ (потому что $\scriptA\vec i=\mu i+0\vec j+0\vec k$ и аналогично). Итого $\det\scriptA=\mu^3$, то есть объём меняется в $|\mu^3|$ раз.
                \item Ещё мы знаем поворот. Была система $(\vec i;\vec j;\vec k)$, а стала --- $(\vec{e_1};\vec{e_2};\vec{e_3})$. У вектора $\vec{e_1}$ есть координаты, которые мы писали направляющими косинусами ($\vec{e_1}(\cos\alpha_1;\cos\beta_1;\cos\gamma_1)$). Аналогично, $\vec{e_2}(\cos\alpha_2;\cos\beta_2;\cos\gamma_2)$ и $\vec{e_3}(\cos\alpha_3;\cos\beta_3;\cos\gamma_3)$. Мы получили матрицу $A=\matr{\cos\alpha_1 & \cos\alpha_2 & \cos\alpha_3\\\cos\beta_1 & \cos\beta_2 & \cos\beta_3\\\cos\gamma_1 & \cos\gamma_2 & \cos\gamma_3}$. Чему равен её определок? Так-то вообще не понятно, но, очевидно, $\vec{e_1}\vec{e_2}\vec{e_3}$. Но тут мы знаем, что эти вектора единичные, попарно перпендикулярные и образуют правую тройку. А значит их смешное произведение равно 1, что не изменит объём параллелепипеда на векторах $(\vec i;\vec j;\vec k)$.
            \end{enumerate}
        \end{Example}
        \dfn \undercolor{red}{След} квадратной \undercolor{red}{матрицы} --- $\tr A=\sum\limits_{i=1}^na_{ii}$.
        \dfn \undercolor{red}{Символ Кронекера} --- $\delta_{ij}=\begin{cases}
            1 & i=j\\
            0 & i\neq j
        \end{cases}$.
        \thm Если $A$ и $B$ подобны, то $\tr A=\tr B$.
        \begin{Proof}
            Мы знаем, что $A=C^{-1}BC$. Пусть элементы $C^{-1}$ обозначены как $s_{ij}$. Тогда $\tr A=\sum\limits_{i=1}^na_{ii}=\sum\limits_{i=1}^n\sum\limits_{j=1}^ns_{ij}(BC)_{ji}=\sum\limits_{i=1}^n\sum\limits_{j=1}^n\sum\limits_{k=1}^ns_{ij}b_{jk}c_{ki}=\sum\limits_{j=1}^n\sum\limits_{k=1}^nb_{jk}\sum\limits_{i=1}^nc_{ki}s_{ij}=\sum\limits_{j=1}^n\sum\limits_{k=1}^nb_{jk}(CC^{-1})_{kj}=\sum\limits_{j=1}^n\sum\limits_{k=1}^nb_{jk}\delta_{kj}=\sum\limits_{j=1}^nb_{jj}=\tr B$. 
        \end{Proof}
        \dfn \undercolor{red}{Следом линейного оператора} называется след его матрицы (по предыдущей теореме в любом базисе получится одно и то же).
        \dfn Пусть $L\subset V$ --- некоторое множество (не обязательно подпространство). Тогда $L$ --- \undercolor{red}{инвариантное} \undercolor{red}{множество} относительно $\scriptA\in\End(V)$, если $\forall v\in L~\scriptA v\in L$.
        \begin{Example}
            Два таких множества нам знакомы: $\ker\scriptA$ и $\Im\scriptA$.
        \end{Example}
        \begin{Example}
            Если мы возьмём вращение вокруг прямой (очевидно, являющееся линейным), то, например, плоскость, перпендикулярная этой прямой, будет инвариантным множеством. Причём не обязательно брать плоскость, проходящую через 0, а значит вот пример инвариантного множества, не являющегося подпространством.
        \end{Example}
        \thm Пусть $L$ --- инвариантное линейное подпространство $V$, $\scriptA\in\End(V)$. Тогда существует такой базис $V$, что матрица оператора $\scriptA$ будет иметь в нём вид $A=\matr{\boxed{A^1} & \star\\0 & \boxed{A^2}}$, при этом $A^1\in\scriptK^{k\times k}$, где $k=\dim L$.
        \begin{Proof}
            Пусть $L=\Lin\{e_1;\ldots;e_k\}$. Тогда его можно дополнить до базиса $V$: $V=\Lin\{e_1;\ldots;e_k;e_{k+1};\ldots;e_n\}$. Тогда, $\scriptA e_1\in L$, а значит $\forall i\leqslant k~\scriptA e_i=\sum\limits_{j=1}^ka_{ji}e_j+\sum\limits_{j=k+1}^n0e_j$. Несложно заметить, что мы, по сути, и получили матрицу нужного вида.
        \end{Proof}
        \thm Пусть $L_1;\ldots;L_m$ --- такие инвариантные подпространства $V$, что $\bigoplus\limits_{i=1}^m L_i=V$, а $\scriptA\in\End(V)$. Тогда существует такой базис $V$, что что матрица оператора $\scriptA$ будет иметь в нём вид $A=\matr{\boxed{A^1} & 0 & \cdots & 0\\0 & \boxed{A^2} & \cdots & 0\\\vdots & \vdots & \ddots & \vdots\\0 & 0 & \cdots & \boxed{A^m}}$, при этом $A^i\in\scriptK^{\dim L_i\times\dim L_i}$.
        \begin{Proof}
            Ну, раз пространства дизъюнктны, базис $V$ --- объединение базисов каждый их них. Тогда рассмотрев любое $L_i$, получим, что базисные вектора, соотвествующие этому пространству, раскладываются по самим себе. Самое сложное в этом доказательстве, вспомнить достаточно букв, чтобы всё обозначить, но вообще оно тривиально.
        \end{Proof}
        \thm Пусть опять $L_1;\ldots;L_m$ --- такие инвариантные подпространства $V$, что $\bigoplus\limits_{i=1}^m L_i=V$, а $\scriptA\in\End(V)$. Тогда $\Im\scriptA=\bigoplus\limits_{i=1}^m\Im\left(\scriptA\big|_{L_i}\right)$.
        \begin{Proof}
            Рассмотрим $u\in V$ и $v=\scriptA u$. Тогда $u$ единственным образом раскладывается как $\sum\limits_{i=1}^mu_i$, где $u_i\in L_i$. А по инвариантности, $v_i=\scriptA u_i\in L_i$. А $v_i\in\Im\left(\scriptA\big|_{L_i}\right)$. Тогда по линейности $\scriptA$ ($v=\scriptA u=\sum\limits_{i=1}^m\scriptA u_i=\sum\limits_{i=1}^mv_i$) мы получаем, во-первых, дизъюнктность $\Im\left(\scriptA\big|_{L_i}\right)$, а во-вторых, нужное нам равенство.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Собственные векторы и собственные числа линейного оператора}.}
    \begin{itemize}
        \dfn Пусть $\scriptA\in\End_\scriptK(V)$. Тогда $\lambda$ называется \undercolor{red}{собственным числом} (либо \undercolor{red}{собственным значением}) оператора $\scriptA$, если $\exists v\neq\mathbb0~\scriptA v=\lambda v$. Тогда вектор $v$ называется \undercolor{red}{собственным вектором} оператора $\scriptA$, отвечающий собственному числу $\lambda$.
        \begin{Comment}
            Животрепещущие вопросы: существуют ли они, сколько их существует, и как их найти.
        \end{Comment}
        \begin{Example}
            Объективно, мы можем взять оператор подобия на $\lambda$, и тогда любой вектор будет его собственным (отвечать одному и тому же числу, $\lambda$).\\
            С другой стороны, можно взять оператор поворота. У него, как несложно заметить, не существует ни собственных векторов, ни собственных чисел (если, конечно, мы поворачиваем не на $\pi k$).
        \end{Example}
        \begin{Example}
            Пусть у нас есть оператор $\scriptA$, который в некотором базисе $e_1;\ldots;e_n$ имеет диагональный вид $\diag(\lambda_1;\ldots;\lambda_n)$. Тогда, как несложно заметить, вектора этого самого базиса будут собственными векторами (и вектор $e_i$ будет соответствовать собственному числу $\lambda_i$).
        \end{Example}
        \dfn Пусть у нас есть собственные вектор и число: $\scriptA v=\lambda v\Leftrightarrow(\scriptA-\lambda\id)v=\mathbb0\Leftarrow v\in\ker(\scriptA-\lambda\id)$. $\ker(\scriptA-\lambda v)$ --- \undercolor{red}{собственное подпространство}, соответствующее собственному числу $\lambda$. Обозначать мы его будем как $V_\lambda$.
        \thm Несложно заметить, что множество всех собственных векторов, соответствующих собственному числу $\lambda$ --- $V_\lambda\setminus\{\mathbb0\}$.
        \dfn \undercolor{red}{Геометрическая кратность собственного числа} $\lambda$ --- число $\gamma(\lambda)$, равное $\dim V_\lambda$.
        \thm Заметим, что кратность инвариантно относительно замены базиса, потому что по сути является дефектом некоторого оператора.
        \begin{Comment}
            Рассмотрим $\lambda$. Оно является собственным числом тогда и только тогда, когда $\Ker(\scriptA-\lambda\id)\text{ нетривиально}\Leftrightarrow\Def(\scriptA-\lambda\id)\neq0\Leftrightarrow\rank(\scriptA-\lambda\id)\neq n\Leftrightarrow\det(\scriptA-\lambda\id)=0$.
        \end{Comment}
        \dfn $\chi_\scriptA(t)=\det(\scriptA-t\id)$ называется \undercolor{red}{характеристическим многочленом} оператора $\scriptA$.
        \thm Характеристический многочлен является многочленом.
        \begin{Proof}
            Несложно заметить, что если матрица $A$ соответствует $\scriptA$, то оператору $\scriptA-t\id$ соответствует $A-tE$ (в том же базисе). А базис, как мы знаем, для определителя не важен, поскольку тот является инвариантом.\\
            В таком случае $\chi_\scriptA(t)=\left|\begin{matrix}
                a_{11}-t & a_{12} & \cdots & a_{1n}\\
                a_{21} & a_{22}-t & \cdots & a_{2n}\\
                \vdots & \vdots & \ddots & \vdots\\
                a_{n1} & a_{n2} & \cdots & a_{nn}-t
            \end{matrix}\right|$. Понятно, что (по свойству определителя) это выражение является суммой некоторых произведений. Каждое из них либо содержит $t$ в некоторой степени, либо нет. Не многочлен ли это.
        \end{Proof}
        \begin{Comment}
            Про этот хочется узнать некоторые члены. Ясно, что степень этого многочлена равна $n$, а коэффициент при $t^n$ равен $(-1)^n$. Чему равен коэффициент при $t^{n-1}$? Несложно заметить, что тогда нам надо взять $n-1$ элемент матрицы, содержащий $t$. но тогда мы автоматически возьмём и последний элемент, содержащий $t$. А значит мы просто возьмём $(a_{11}-1)(a_{22}-t)\cdots(a_{nn}-t)$, откуда и вытечет единственное слагаемое для $t^{n-1}$. Но тогда, несложно заметить, мы получим $\tr A(-1)^{n-1}$. Ещё хорошо было бы посмотреть на свободный член этого многочлена. Из матрицы самой по себе как-то непонятно, но мы можем подставить $t=0$ и получить свободный член --- $\det\scriptA$.
        \end{Comment}
        \thm Если $\lambda_1;\ldots;\lambda_n$ --- все корни $\chi_\scriptA(t)$ с учётом кратности, то $\prod\limits_{i=1}^n\lambda_i=\det A$ и $\sum\limits_{i=1}^n\lambda_i=\tr A$.
        \begin{Proof}
            Рассмотрим комментарий выше. Из него следует, что $\chi_\scriptA(t)=(-1)^nt^n+(-1)^{n-1}\tr At^{n-1}+\cdots+\det A$. А ещё мы знаем, что $\chi_\scriptA(t)=(\lambda_1-t)(\lambda_2-t)\cdots(\lambda_n-t)$. Тогда, как несложно заметить из второй формы, свободный член равен $\prod\limits_{i=1}^n\lambda_n$. И в то же время равен $\det A$.\\
            С другой же стороны, коэффициент при $t^{n-1}$ равен $(-1)^{n-1}\sum\limits_{i=1}^n\lambda_i$, потому что из каждой скобки кроме одной мы берём $-t$, а из одной --- $\lambda_i$. Получаем $n$ слагаемых вида $(-1)^{n-1}t^{n-1}\lambda_i$. И отсюда $(-1)^{n-1}\tr A=(-1)^{n-1}\sum\limits_{i=1}^n\lambda_i$.
        \end{Proof}
        \begin{Comment}
            Указанные два утверждения являются своего рода Теоремой Виета, что позволяет нам в некоторых случаях угадывать собственные числа.
        \end{Comment}
        \begin{Example}
            Рассмотрим поворот вокруг нуля. В нём $\chi_\scriptA(t)=\left|\begin{matrix}
                \cos\alpha-t & -\sin\alpha\\
                \sin\alpha & \cos\alpha-t
            \end{matrix}\right|=\cos^2\alpha-2t\cos\alpha+t^2+\sin^2\alpha=t^2-2t\cos\alpha+1$.Запишем дискриминант этой штуки. Он равен $4\cos^2\alpha-4=-4\sin^2\alpha$. Чтобы $\chi$ имело корень необходимо и достаточно $-4\sin^2\alpha\geqslant0$, что выполняется только при $\alpha=\pi k$. Ровно этот результат мы и получили ранее, что при $\alpha\neq\pi k$ собственных чисел и векторов нет.
        \end{Example}
        \dfn \undercolor{red}{Алгебраическая кратность} собственного числа $\lambda$ --- число $\alpha(\lambda)$, равное кратности его как корня многочлена $\chi_\scriptA(t)$
        \dfn \undercolor{red}{Спектр оператора} $\scriptA$ --- совокупность собственных чисел с учётом их кратности.
        \dfn Спектр называется \undercolor{red}{простым}, если все собственные числа имеют алгебраическую кратность 1 (то есть все они попарно различны).
        \thm Для любого собственного числа $\lambda$ $1\leqslant\gamma(\lambda)\leqslant\alpha(\lambda)$.
        \begin{Proof}
            $1\leqslant\gamma(\lambda)$ тривиально (иначе $\lambda$ не являлось бы собственным числом).\\
            Рассмотрим собственное подпространства для числа $\lambda$ --- $V_\lambda$. Обозначим его базисные вектора как $v_1;\ldots;v_\gamma$ (их $\gamma(\lambda)$ штук). Найдём $V'$ --- прямое дополнение $V_\lambda$. Это значит, что $\scriptA$ можно представить как блочную матрицу из двух блоков. Тогда несложно заметить, что $\chi_\scriptA(t)=\left|\begin{matrix}
                \boxed{A^1-tE} & 0\\
                0 & \boxed{A^2-tE}
            \end{matrix}\right|=\det(A^1-tE)\det(A^2-tE)$. Заметим, что $A^1$ --- это $\diag(\lambda;\ldots;\lambda)$. Почему? Ну, потому что $\lambda$ --- собственное число кратности $\gamma$, а значит $\det(A^1-tE)$ равно $(t-\lambda)^\gamma$. Ну, собственно, это нам и нужно, потому что $\lambda$ является корнем $\det(A^1-tE)\det(A^2-tE)$ как минимум кратности $\gamma$, а можем и больше.
        \end{Proof}
        \thm Пусть $\lambda_1;\ldots;\lambda_m$ --- попарно различные собственные числа $\scriptA$, а $v_1;\ldots;v_m$ --- соответствующие им собственные вектора. Тогда $v_1;\ldots;v_m$ линейно независимы.
        \begin{Proof}
            Докажем по индукции по $m$. База ($m=1$). Тогда вектор $v_1$ уж точно является линейно независимым, потому что не равен нулю.\\
            Переход: пусть индукционное предположение верно для $m-1$. То есть $v_1;\ldots;v_{m-1}$ линейно независимы. Докажем от противного, что можно добавить в систему ещё один вектор (не умаляя общности, он в конце, $v_m$). Пусть нельзя. Тогда $v_m=\sum\limits_{i=1}^{m-1}\alpha_iv_i$. Заметим, что $\scriptA v_m=\sum\limits_{i=1}^{m-1}\alpha_i\scriptA v_i=\sum\limits_{i=1}^{m-1}\alpha_i\lambda_iv_i$. С другой стороны, $\scriptA v_m=\lambda_mv_m=\sum\limits_{i=1}^{m-1}\alpha_i\lambda_mv_i$. Вычтем это из предыдущего, получим $\sum\limits_{i=1}^{m-1}\alpha_i(\lambda_m-\lambda_i)v_i=\mathbb0$. Это линейная комбинация независимых $v_i$, а значит $\forall i\in[1:m-1]~\alpha_i(\lambda_m-\lambda_i)=0$. При этом второй множитель ненулевой, а значит все $\alpha_i$ нулевые, а значит $v_m=\mathbb0$. Противоречие.
        \end{Proof}
        \thm Отсюда собственные подпространства, отвечающие различным собственным числам, дизъюнктны.
        \begin{Proof}
            Рассмотрим $v_1+\cdots+v_m=\mathbb0$, где $v_i\in V_{\lambda_i}$. Но тогда $v_i$ --- либо собственные вектора, отвечающие различным собственным числам, либо нули. А собственные вектора линейно независимы, значит если в указанной сумме есть не нули, то противоречие. А если все $v_i$ нули, то это именно то, что нам и нужно было для дизъюнктности.
        \end{Proof}
        \thm Пусть $L_1;L_2;\ldots;L_m$ --- инвариантные подпространства $V$ и $V=\bigoplus\limits_{i=1}^mL_i$, а $\scriptA\in\End(V)$. Тогда $\chi_\scriptA(t)=\prod\limits_{i=1}^m\chi_{\scriptA\big|_{L_i}}(t)$.
        \begin{Proof}
            См. теорему о матрице оператора блочной структуры. Тогда $\scriptA\big|_{L_i}$ соответствует $A^i$. А значит $\chi_\scriptA(t)=\left|\begin{matrix}
                \boxed{A^1-tE} & 0 & \cdots & 0\\
                0 & \boxed{A^2-tE} & \cdots & 0\\
                \vdots & \vdots & \ddots & \vdots\\
                0 & 0 & \cdots & \boxed{A^m-tE}\\
            \end{matrix}\right|$. А определитель каждого $A^i-tE$ --- это $\chi_{\scriptA\big|_{L_i}}(t)$
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Оператор простой структуры}. \undercolorblack{orange}{Проекторы}. \undercolorblack{orange}{Спектральное разложение}. \undercolorblack{orange}{Функция от матрицы}.}
    \begin{itemize}
        \dfn $\scriptA\in\End(V)$ называется \undercolor{red}{оператором простой структуры}, если в некотором базисе его матрица имеет диагональный вид (обычно обозначаемый $\Lambda$).
        \begin{Comment}
            Мы уже обсуждали, что в условиях этого определения числа на диагонали являются собственными числами, соответствующие соответственным базисным векторам \textit{того самого} базиса. А это значит, что ему можно дать альтернативное определение: Оператор обладает простой структурой, если его собственные вектора образуют базис.
        \end{Comment}
        \thm Если $\scriptA\in\End(V)$ и $\sum\limits_{\substack{\lambda\text{ --- собственное}\\\text{число }\scriptA}}\alpha(\lambda)=n$ (то есть все корни $\chi_\scriptA$ лежат в поле $\scriptK$). Тогда $\scriptA$ является оператором простой структуры тогда и только тогда, когда $\forall\lambda\text{ --- собственное число}~\alpha(\lambda)=\gamma(\lambda)$.
        \begin{Proof}
            Мы уже знаем, что $\gamma(\lambda)\leqslant\alpha(\lambda)$. Тогда если $\scriptA$ простой структуры, то $V$ является линейной оболочкой собственных векторов $\scriptA$, а значит $V=\bigoplus\limits_{\substack{\lambda\text{ --- собственное}\\\text{число }\scriptA}}V_\lambda$. Но подождите, тогда $\sum\limits_{\substack{\lambda\text{ --- собственное}\\\text{число }\scriptA}}\dim V_\lambda=\sum\limits_{\substack{\lambda\text{ --- собственное}\\\text{число }\scriptA}}\gamma(\lambda)=n$. А мы уже знаем, что каждое $\gamma$ не больше $\alpha$, а $\alpha$ в сумме дают $n$. Из этих трёх фактов как раз и следует, что $\alpha(\lambda)=\gamma(\lambda)$. Следствие в обратную сторону аналогично.
        \end{Proof}
        \thm Если спектр простой и количество собственных чисел равно $n$, то $\scriptA$ --- оператор простой структуры.
        \begin{Proof}
            Ну, лол, очевидно. Раз уж $\lambda_i$ попарно различны (из простоты спектра), то геометрическая кратность любого из них равна 1. А уж алгебраическая и подавно равна 1.
        \end{Proof}
        \begin{Comment}
            Все определения, теоремы и комментарии из текущего раздела и предыдущего без изменений распространяются на матрицы.
        \end{Comment}
        \dfn Матрица $A$ называется \undercolor{red}{диагонализируемой} (либо \undercolor{red}{подобной диагональной}), если существует невырожденная матрица $T$, что $T^{-1}AT=\Lambda$.
        \thm Матрица $A\in\scriptK^{n\times n}$ диагонализируема, если она является матрицей оператора простой структуры в некотором базисе.
        \begin{Proof}
            Если вам не очевидно доказательство, попробуйте поспать и перечитать несколько утверждений выше.
        \end{Proof}
        \dfn Пусть $V=\bigoplus\limits_{i=1}^n L_i$. Тогда, как мы знаем, любой вектор $v$ единственным образом раскладывается как сумма $\sum\limits_{i=1}^nv_i$, где $v_i\in L_i$. Тогда пусть $\scriptP_i\colon V\to V$ --- оператор, вектору $v$ сопоставляющий $v_i$. Очевидно, это линейный оператор. Он называется \undercolor{red}{оператором проектирования} либо \undercolor{red}{проектором}.
        \thm \undercolor{darkgreen}{Простейшие свойства проекторов}.
        \begin{enumerate}
            \item Если $i\neq j$, то $\scriptP_i\scriptP_j=\mathbb0$.
            \item $\forall k\in\mathbb N~\scriptP_i^k=\scriptP_i$.
            \item $\sum\limits_{i=1}^n\scriptP_i=\id$.
            \item $\Im\scriptP_i=L_i$, $\ker\scriptP_i=\bigoplus\limits_{\substack{j\in[1:n]\\j\neq i}}\scriptP_j$.
        \end{enumerate}
        \begin{Proof}
            Всё это проверяется тривиально.
        \end{Proof}
        \begin{Example}
            Если взять произвольный базис $(\vec{e_1};\vec{e_2};\vec{e_3})$, а мы хотим проекцию на плоскость на векторах $\vec{e_1}$ и $\vec{e_2}$, то матрица в этом же базисе будет $\matr{1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 0}$. Если мы хотим представить матрицу в другом базисе, то придётся писать матрицу перехода.
        \end{Example}
        \thm Если $\scriptP_i\in\End(V)$, выполнены условия 1 и 3, то $V=\bigoplus\limits_{i=1}^n\Im\scriptP_i$.
        \begin{Proof}
            Ну, действительно, $\scriptP_i=\scriptP_i\id=\scriptP_i\sum\limits_{j=1}^n\scriptP_j=\sum\limits_{j=1}^n\scriptP_i\scriptP_j=\scriptP_i^2$. По индукции имеем свойство 2.\\
            Теперь рассмотрим $v\in V$. $\id v=\left(\sum\limits_{i=1}^n\scriptP_i\right)v=\sum\limits_{i=1}^n\scriptP_iv$. Заметим, что $\scriptP_iv\in\Im\scriptP_i$, а это значит, что $V=\sum\limits_{i=1}^n\Im\scriptP_i$. Осталось доказать, что пространства дизъюнктны. Как мы это докажем? Тривиальностью разложения нуля. Пусть $\omega_i\in\Im\scriptP_i$ и рассмотрим $\sum\limits_{i=1}^n\omega_i=\mathbb0$. Раз уж $\omega_i\in\Im\scriptP_i$, $\exists v_i\in V~\omega_i=\scriptP_iv_i$. А это значит, что $\sum\limits_{i=1}^n\scriptP_iv_i=\mathbb0$. А это в свою очередь значит, что $\scriptP_i\mathbb0=\scriptP_i\sum\limits_{j=1}^n\scriptP_jv_i=\scriptP_i^2v_i=\omega_i$. То есть $\omega_i=\mathbb0$, что мы и хотели поучить.
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема о спектральном разложении оператора простой структуры}. Пусть $\scriptA\in\Hom(V)$ --- оператор простой структуры. Тогда, как мы знаем, $V=\sum\limits_{\substack{\lambda\text{ --- собственное}\\\text{число }\scriptA}}V_\lambda$. Тогда $\scriptA=\sum\limits_{\substack{\lambda\text{ --- собственное}\\\text{число }\scriptA}}\lambda \scriptP_\lambda$, где $\scriptP_\lambda$ --- проектор на $V_\lambda$.
        \begin{Proof}
            Известно, что $v$ единственным образом раскладывается как сумма $v_\lambda\in V_\lambda$. А это значит, что $\scriptA v=\sum\limits_{\substack{\lambda\text{ --- собственное}\\\text{число }\scriptA}}\scriptA v_\lambda=\sum\limits_{\substack{\lambda\text{ --- собственное}\\\text{число }\scriptA}}\lambda v_\lambda=\sum\limits_{\substack{\lambda\text{ --- собственное}\\\text{число }\scriptA}}\lambda\scriptP_\lambda v$. То есть это разложение ещё и единственно.
        \end{Proof}
        \dfn Разложение выше --- \undercolor{red}{спектральное разложение}.
        \thm Аналогично, если $A$ --- диагонализируемая матрица, а $P_\lambda$ --- матрицы проекции, то $A=\sum\limits_{\substack{\lambda\text{ --- собственное}\\\text{число }\scriptA}}\lambda P_\lambda$.
        \dfn Пусть $A^{(k)}$ --- последовательность матриц $\in\scriptK^{m\times n}$. Тогда матрица $A$ называется \undercolor{red}{пределом последовательности} \undercolor{red}{матриц} $A^{(k)}$, если $\forall i\in[1:m]~\forall j\in[1:n]~a_{ij}=\lim\limits_{k\to\infty}a_{ij}^{(k)}$.
        \dfn \undercolor{red}{Сумма ряда матриц} $\sum\limits_{i=1}^\infty A^{(i)}=\lim\limits_{k\to\infty}\sum\limits_{i=1}^kA^{(i)}$. Выражение $\sum\limits_{i=1}^kA^{(i)}$ называется \undercolor{red}{частичной суммой ряда}.
        \dfn \undercolor{red}{Степенной ряд} --- ряд вида $f(x)=\sum\limits_{i=0}^\infty c_i(x-a)^i$.
        \dfn \undercolor{red}{Радиус сходимости} ряда --- такое $R$, что при $|x-a|<R$ ряд абсолютно сходится, а при $|x-a|>R$ --- расходится.
        \begin{Example}
            Например, экспонента вообще для любого $x$ сходится, а значит её радиус --- $\infty$.
        \end{Example}
        \begin{Comment}
            Всё это более подробно будет на матане.
        \end{Comment}
        \dfn Пусть есть функция $f(x)$, которая разложена в степенной ряд $\sum\limits_{i=0}^\infty c_ix^i$ и имеет радиус сходимости $R$. Тогда мы можем определить \undercolor{red}{функцию от матрицы} $f(A)=\sum\limits_{i=0}^\infty c_iA^i$.
        \thm Пусть $A$ --- диагонализируемая матрица и все собственные числа $A$ по модулю меньше $R$. Тогда $f(A)=T\matr{f(\lambda_1) & 0 & \cdots & 0\\0 & f(\lambda_2) & \cdots & 0\\\vdots & \vdots & \ddots & \vdots\\0 & 0 & \cdots & f(\lambda_n)}T^{-1}$. И ещё тогда $f(A)=\sum\limits_{\lambda}f(\lambda)P_\lambda$.
        \begin{Proof}
            $f(A)=\sum\limits_{i=1}^\infty c_iA^i$. Как мы знаем, $A=T\Lambda T^{-1}$. Тогда $A^i=(T\Lambda T^{-1})^i=T\Lambda^iT^{-1}=T\diag(\lambda_1^i;\ldots;\lambda_n^i)T^{-1}$. Посчитаем частичную сумму: $\sum\limits_{i=1}^kc_iT\diag(\lambda_1^i;\ldots;\lambda_n^i)T^{-1}=T\diag(\sum\limits_{i=1}^kc_i\lambda_1^i;\ldots;\sum\limits_{i=1}^kc_i\lambda_n^i)T^{-1}$. Если взять предел этого, получим $T\diag(\sum\limits_{i=1}^\infty c_i\lambda_1^i;\ldots;\sum\limits_{i=1}^kc_i\lambda_n^i)T^{-1}$, а поскольку любое $\lambda_j$ в круге сходимости это равно тому, чему нам нужно.\\
            Теперь с проекторами. Нам нужно посчитать $A^i=\left(\sum\limits_\lambda\lambda P_\lambda\right)^i$. Если мы из скобок будем брать разные $P_\lambda$, будем получать ноль, а если одинаковые --- то $P_\lambda$. То есть $A^i=\sum\limits_\lambda\lambda^iP_\lambda$. А это значит частичная сумма равна $A^i=\sum\limits_\lambda\left(\sum\limits_{i=0}^kc_i\lambda^i\right)P_\lambda$. В пределе получится то, что требуется.
        \end{Proof}
        \begin{Comment}
            На самом деле можно считать функцию $f(At)$, где $t\in\scriptK$. Что тогда получится? Да то же самое, только вместо $f(\lambda_j)$ будет $f(\lambda_jt)$.
        \end{Comment}
        \dfn \undercolor{red}{Экспонента от матрицы} $e^{At}$ --- это функция, определяемая из комментария выше.
        \begin{Example}
            Рассмотрим, например, матрицу $\matr{
                7 & -12 & 6\\
                10 & -19 & 10\\
                12 & -21 & 13
            }$. В ней мы получим два собственных числа: 1 и $-1$. У первого собственное подпространство равно $\Lin\left\{\matr{2\\1\\0};\matr{1\\0\\-3}\right\}$, у второго --- $\Lin\left\{\matr{3\\5\\6}\right\}$. Тогда $T=\matr{
                2 & 1 & 3\\
                1 & 0 & 5\\
                0 & -1 & 5
            }$, а $\Lambda=\matr{
                1 & 0 & 0\\
                0 & 1 & 0\\
                0 & 0 & -1
            }$. А это значит $e^{At}=T\matr{
                e^t & 0 & 0\\
                0 & e^t & 0\\
                0 & 0 & e^{-t}
            }T^{-1}$. Если умножить, получим $\matr{
                4e^t-3e^{-t} & -6e^t+6e^{-t} & 3e^t-3e^{-t}\\
                5e^t-5e^{-t} & -9e^t+10e^{-t} & 5e^t-5e^{-t}\\
                6e^t-6e^{-t} & -12e^t+12e^{-t} & 7e^t-6e^{-t}
            }$. 
        \end{Example}
        \thm Также про функцию $e^{At}$ несложно заметить все те же свойства, что и про обычную экспоненту, $e^{A(t_1+t_2)}=e^{At_1}e^{At_2}$, например. Или $(e^{At})'=Ae^{At}$.
        \begin{Proof}
            $e^{A(t_1+t_2)}=e^{At_1}e^{At_2}$ очевидно, а производная легко проверяется в форме спектрального разложения.
        \end{Proof}
        \begin{Comment}
            Производная, кстати, может позволить нам решать систему линейных \textbf{дифференциальных} уравнений. Там тоже будет ФСР, всё будет как в алгебраических уравнениях, только там будут именно что функции от матрицы.
        \end{Comment}
        \begin{Comment}
            Кстати, через диагональную форму можно искать обратную матрицу. Как мы знаем, если $A$ диагонализируема и обратима, то $A^{-1}=T\Lambda^{-1}T^{-1}$. А $\Lambda$, очевидно, равна $\diag(\frac1{\lambda_1};\ldots;\frac1{\lambda_n})$. Поэтому, кстати, ни одно из собственных чисел не должно быть нулём, иначе $\det A=0$, и матрица не обратима (даже если и диагонализируема). Но то же самое можно делать и со спектральным разложением, тоже нужно лишь инвертировать все $\lambda_j$.
        \end{Comment}
        \dfn \undercolor{red}{Корень матрицы} $A$ --- $\sqrt[m]A$ --- это никогда не догадаетесь что. Определена эта функция, понятно, только тогда, когда $\forall\lambda\geqslant0$.
        \thm $(\sqrt[m]A)^m=A$.
        \begin{Comment}
            Одна проблема: матрица везде в этом параграфе должна быть диагонализируема. А хочется как-то с остальными разобраться.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Комплексификация вещественного линейного пространства}. \undercolorblack{orange}{Продолжение вещественного} \undercolorblack{orange}{линейного оператора}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Пусть $\scriptA\in\End(V)$, $\lambda$ --- корень $\chi_\scriptA(t)$. Либо все корни лежат в нашем поле, либо не все. Если все, то либо $\scriptA$ --- оператор простой структуры ($\gamma(\lambda)=\alpha(\lambda)$), либо нет. В первом случае мы уже разобрали всё что можно. Во втором случае мы разберёмся потом, там будет жорданова форма. А со случаем, когда не все собственные числа лежат в поле, мы разберёмся прямо сейчас.
        \end{Comment}
        \dfn Пусть $V$ --- линейное пространство над $\mathbb R$. Каждым $x,y\in V$ сопоставим $v=x+\im y\in V_\mathbb C$. При этом $V_\mathbb C$ работает по следующим правилам: $x+\im y=x'+\im y'\Leftrightarrow x=x'\land y=y'$, элементу $x\in V$ сопоставлен $x+\mathbb0\im$, $(x+\im y)+(x'+\im y')=(x+x')+\im(y+y')$, $(\alpha+\im\beta)(x+\im y)=(\alpha x-\beta y)+\im(\alpha y+\beta x)$. Несложно заметить, что тогда $V_\mathbb C$ с введёнными операциями действительно является линейным пространством над $\mathbb C$. Оно называется \undercolor{red}{комплексификацией} $V$.
        \thm Пусть $(e_1;\ldots;e_n)$ --- базис $V$, тогда $(e_1;\ldots;e_n)$ --- базис $V_\mathbb C$.
        \begin{Proof}
            Ну, рассмотрим любой комплексный вектор $x+\im y\in V_\mathbb C$. $x$ раскладывается по базису, и $y$ тоже. Тогда $x+\im y=\sum\limits_{k=1}^n(x_k+\im y_k)e_k$. Линейная независимость также очевидно следует из вещественного случая.
        \end{Proof}
        \dfn Если $x+\im y\in V_\mathbb C$, то никогда не догадаетесь, что такое \undercolor{red}{сопряжённый вектор}.
        \thm Если $(v_1;\ldots;v_m)$ --- линейно независимы, то $(\overline{v_1};\ldots;\overline{v_m})$ --- тоже.
        \begin{Proof}
            Рассмотрим $\sum\limits_{k=1}^m\overline{v_k}\lambda_k=\mathbb0$. Если применить сопряжение к обеим частям, получим $\sum\limits_{k=1}^mv_k\overline{\lambda_k}=\mathbb0$, а это уже значит, что все $\overline{\lambda_k}$ равны нулю.
        \end{Proof}
        \dfn Пусть $\scriptA\in\End(V)$. Тогда определим оператор $\scriptA_\mathbb C\colon V_\mathbb C\to V_\mathbb C$ так: $\scriptA_\mathbb C(x+\im y)=\scriptA x+\im\scriptA y$. Несложно проверить, что $\scriptA_\mathbb C\in\End(V_\mathbb C)$. $\scriptA_\mathbb C$ называется \undercolor{red}{продолжением} $\scriptA$ на $V_\mathbb C$.
        \thm Продолжению оператора (в том же базисе) соответствует та же матрица, что и оригинальному оператору.
        \begin{Proof}
            Ну, у нас есть базис $(e_1;\ldots;e_n)$ в $V$. Он же, как мы знаем, базис $V_\mathbb C$. Надо подействовать оператором $\scriptA_\mathbb C$ на базисные вектора. Но подождите, $e_k=e_k+\im\mathbb0$. А это значит, что $\scriptA_\mathbb Ce_k=\scriptA e_k+\mathbb0$. Вот та же матрица и соответствует.
        \end{Proof}
        \thm Предыдущий факт значит как минимум то, что характеристические многочлены $\chi_\scriptA(t)$ и $\chi_{\scriptA_\mathbb C}(t)$ совпадают.
        \begin{Comment}
            Предыдущий факт значит, что решения характеристического многочлена будут одинаковыми, а значит у $\scriptA_\mathbb C$ будет ровно $n$ собственных чисел.
        \end{Comment}
        \thm $\overline{\scriptA_\mathbb Cv}=\scriptA_\mathbb C\overline v$.
        \begin{Proof}
            Очевидно следует из линейности.
        \end{Proof}
        \begin{Comment}
            Что мы ещё знаем? Ещё мы знаем, что $\chi_{\scriptA_\mathbb C}(t)$ --- многочлен с вещественными коэффициентами. А это значит, что комплексные корни разбиваются на пары сопряжённых. Причём у них ещё и алгебраическая кратность одинаковая.\\
            Но при этом у собственного числа есть собственный вектор. Несложно из предыдущей теоремы увидеть, что у $\overline\lambda$ собственный вектор --- $\overline v$. А это значит, что у сопряжённых собственных чисел ещё и геометрическая кратность одинаковая.
        \end{Comment}
        \begin{Comment}
            То есть у случае, когда мы в $\chi_\scriptA$ нашли комплексные корни, мы просто рассматриваем $\scriptA_\mathbb C$ вместо $\scriptA$, матрица не меняется, характеристический многочлен не меняется, ничего не меняется, просто чиллим. и теперь можно уже разбираться, диагонализируема матрица или нет (только теперь всё будет комплексным).
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Минимальный многочлен оператора}. \undercolorblack{orange}{Теорема Кэли --- Гамильтона}.}
    \begin{itemize}
        \dfn Пусть $\scriptA\in\End(V)$, а $\psi(t)$ --- нормализованный многочлен (т.е. со старшим коэффициентом 1). Тогда $\psi(t)$ --- \undercolor{red}{аннулятор} элемента $x\in V$, если $\psi(\scriptA)x=\mathbb 0$.
        \dfn Аннулятор минимально возможной степени --- \undercolor{red}{минимальный аннулятор}.
        \thm \undercolor{darkgreen}{Теорема о минимальном аннуляторе}. У любого элемента существует единственный минимальный аннулятор. Все аннуляторы делятся на минимальный.
        \begin{Proof}
            Если $x=\mathbb0$, то очевидно, какой у него аннулятор ($\psi(\scriptA)=\id$). Также очевидно, что он минимальный и на него все делятся.\\
            Если $x\neq\mathbb0$, то сделаем так. Рассмотрим $x;\scriptA x;\scriptA^2 x;\ldots;\scriptA^m x$. Эту систему векторов мы будем строить до тех пор, пока она не станет линейно зависимой. Она определённо станет не позже, чем на $n$-ном шаге. А когда станет, получим, что $\scriptA^m x=\sum\limits_{k=0}^{m-1}a_k\scriptA^k x$. Тогда наш многочлен --- $\psi(\scriptA)=\scriptA^m-\sum\limits_{k=0}^{m-1}a_k\scriptA^k$. Почему не может быть многочлена меньшей степени? Ну, если есть, значит можно записать то же равенство $\scriptA^m x=\sum\limits_{k=0}^{m-1}a_k\scriptA^k x$, но для $m$ поменьше, а значит зависимость произошла раньше, чем на $m$-том шаге, противоречие.\\
            Теперь докажем, что делится. Пусть у нас есть $\psi_1(t)$ и только что построенное нами $\psi(t)$. Поделим $\psi_1$ на $\psi$ с остатком. Получим, что $\psi_1(t)=q(t)\psi(t)+r(t)$. Тогда $r(t)$ тоже аннулятор $x$ (подставьте, проверьте), но степень у него меньше, чем у $\psi$. А значит $\psi$ не было минимальным. Значит единственный вариант --- когда $r(t)=0$, что значит, что $\psi_1\divby\psi$.
        \end{Proof}
        \dfn Пусть $\scriptA\in\End(V)$. Тогда нормализованный многочлен $\varphi(t)$ является \undercolor{red}{аннулятором оператора} $\scriptA$, если $\varphi(\scriptA)=\mathbb0$.
        \dfn \undercolor{red}{Минимальный многочлен оператора} $\scriptA$ --- его аннулятор минимальной степени.
        \thm \undercolor{darkgreen}{Теорема о минимальном многочлене}. Минимальный многочлен существует и единственный. Также любой аннулятор оператора делится на минимальный многочлен.
        \begin{Proof}
            Рассмотрим $(e_1;\ldots;e_n)$ --- базис $V$. Существует минимальный аннулятор каждого из них --- $\psi_k$. Тогда рассмотрим Н.О.К. $\{\psi_1;\ldots;\psi_n\}=\varphi$. Докажем, что минимальный многочлен $\scriptA$ --- это $\varphi$. Ну, действительно, если $v=\sum\limits_{i=1}^nv_ie_i$, тогда $\varphi(\scriptA)v=\sum\limits_{i=1}^n\varphi(\scriptA)v_i$. Поскольку $\varphi(\scriptA)$ делится на $\psi_i(\scriptA)$, $\varphi(\scriptA)v_i=\mathbb0$, а значит это действительно аннулятор.\\
            Хорошо, а почему конкретно построенный нами многочлен --- минимальный многочлен? Ну, заметим, что он уж точно является аннулятором для всех элементов базиса. А аннулятор всех элементов базиса делится на все $\psi_i$. А значит вряд ли уж минимальный многочлен (делящийся на все $\psi_i$) будет иметь степень меньше, чем Н.О.К. всех $\psi_i$. Отсюда же, любой аннулятор делится на $\varphi$.\\
            Почему не может быть двух аннуляторов минимальной степени? Ну, если их два, то их разность также аннулятор, а степень имеет меньше.
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема Кэли --- Гамильтона}. Пусть $\scriptA\in\End(V)$. Тогда $\chi_\scriptA$ является аннулятором $\scriptA$.
        \begin{Proof}
            Если мы проверим это для конкретного базиса, то теорема по инвариантности будет доказана. Мы можем оперировать с матрицами (с матрицей $A$), поскольку $\chi_\scriptA(t)=\det(\scriptA-t\id)$, а определитель не зависит от базиса.\\
            Итак, пусть $\chi_\scriptA(t)=\sum\limits_{k=0}^n\alpha_k t^k$. Пусть $\mu$ --- не корень $\chi_\scriptA$, то есть $\det(A-\mu E)\neq0$. Это значит,что у матрицы $(A-\mu E)$ есть обратная. То есть $(A-\mu E)^{-1}=\frac B{\det(A-\mu E)}$, где $B$ --- матрица алгебраических дополнений $A-\mu E$. Как выглядит $B$? Как $b_{ij}=(-1)^{i+j}M_{ji}$, где $M_{ji}$ --- матрица, получаемая из $A-\mu E$ вычёркиванием $j$-того столбца и $i$-той строки. А это значит, что $b_{ij}$ --- многочлен от $\mu$ степени $n-1$. А это в свою очередь значит, что $B=\sum\limits_{k=0}^{n-1}B_k\mu^k$, где $B_k$, как несложно понять, --- матрица коэффициентов при $\mu^k$. Тогда получаем равенство $\det(A-\mu E)E=(A-\mu E)B$. Это то же самое, что и $\left(\sum\limits_{k=0}^n\alpha_k\mu^k\right)E=(A-\mu E)\sum\limits_{k=0}^{n-1}B_k\mu^k$. Давайте приравняем коэффициенты в многочленах слева и справа. В нулевой степени слева получаем $\alpha_0E=AB_0$, в первой --- $\alpha_1E=AB_1-B_0$, во второй --- $\alpha_2E=AB_2-B_1$, ..., в $n-1$-вой --- $\alpha_{n-1}E=AB_{n-1}-B_{n-2}$, в $n$-ной --- $\alpha_nE=-B_{n-1}$. Тогда $\chi_\scriptA(A)=\sum\limits_{k=0}^n\alpha_0A^k=AB_0+(A^2B_1-AB_0)+(A^3B_2-A^2B_1)+\cdots-A^nB_{n-1}$. Как видно, в этой сумме всё сокращается, что и даёт нам, что $\chi_\scriptA$ аннулятор.
        \end{Proof}
        \begin{Comment}
            Заметим, что доказывать это как $\chi_\scriptA(A)=\det(A-AE)=\mathbb0$ нельзя потому, что $\det(A-tE)$ --- это всё-таки многочлен от числа, а потом мы говорим, что берём тот же многочлен для матрицы. То есть определение было не вполне формально корректно, поэтому такое не прокатывает.
        \end{Comment}
        \thm Элементарное следствие --- $\chi_\scriptA\divby\varphi_\scriptA$.
        \thm также тривиально, что если $\deg\varphi_\scriptA=n=\dim V$, то $\varphi_\scriptA(t)=(-1)^n\chi_\scriptA(t)$.
        \thm \undercolor{darkgreen}{Теорема о корнях минимального многочлена}. Множество корней $\chi_\scriptA$ и $\varphi_\scriptA$ совпадают для любого $\scriptA\in\End(V)$.
        \begin{Proof}
            То что все корни $\varphi_\scriptA$ являются корнями $\chi_\scriptA$ --- тривиально следует из теоремы Кэли --- Гамильтона. В обратную сторону сложнее.\\
            Пусть все корни $\chi_\scriptA$ лежа в поле $\scriptK$. Пусть у нас $\lambda$ --- корень $\chi_\scriptA(t)$. Тогда существует такой вектор $v$, что $\lambda v=\scriptA v$. Но тогда $\psi(t)=t-\lambda$ --- минимальный аннулятор $v$. А поскольку $\varphi_\scriptA(t)$ --- аннулятор любого элемента $V$, он является аннулятором $\psi(t)$. А это значит, что $\varphi_\scriptA\divby\psi$, что значит, что $\lambda$ --- корень $\varphi_\scriptA$. То есть все корни $\chi_\scriptA$ являются корнями $\varphi_\scriptA$.\\
            А если не все корни лежат в $\scriptK$? Тогда придётся взять $\scriptA_\mathbb C$ и $V_\mathbb C$. Тогда $\chi_\scriptA$ и $\chi_{\scriptA_\mathbb C}$ имеют одинаковые корни. При этом все корни $\chi_{\scriptA_\mathbb C}$ являются корнями $\varphi_{\scriptA_\mathbb C}$. Но вот одинаковы ли корни $\varphi_\scriptA$ и $\varphi_{\scriptA_\mathbb C}$, не очевидно. Но на самом деле одинаковы. Как мы строим $\varphi$? Сначала строим $\psi$ $n$ раз, а потом берём Н.О.К. А $\psi$ мы строим при помощи $\scriptA^k e_i$. При этом что в $V$, что в $V_\mathbb C$ это вещественные вектора (одинаковые), а $e_i$ дают базис. А ещё в оригинальном пространстве и его комплексификации линейная зависимость и независимость равносильны, что значит, что все $\psi$ получатся одинаковыми, а значит одинаковыми получатся их Н.О.К. --- $\varphi_{\scriptA_\mathbb C}$ и $\varphi_\scriptA$.
        \end{Proof}
        \begin{Comment}
            Тут мы получили второй способ поиска собственных чисел, мы можем не искать $\chi$ (определитель), а искать $\varphi$ (через независимость и Н.О.К.).
        \end{Comment}
        \dfn Кратность собственного числа $\lambda$ в $\varphi_\scriptA$ обозначаем за \undercolor{red}{$m(\lambda)$}.
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Операторное разложение единицы}. \undercolorblack{orange}{Корневое подпространство}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Сейчас будем некий отрывок теории многочленов.
        \end{Comment}
        \dfn Пусть $\varphi(t)=\prod\limits_{\mu}(t-\mu)^{m(\mu)}$, все корни которого лежат в поле $\scriptK$. Выделим некоторый корень $\varphi$ и назовём его $\lambda$. Тогда $\varphi(t)=(t-\lambda)^{m(\lambda)}\prod\limits_{\mu\neq\lambda}(t-\mu)^{m(\mu)}$. Обозначим вторую часть как $\varphi_\lambda(t)$. Заметим, что она взаимно проста с первой. Обозначим $m=\deg\varphi=\sum\limits_\lambda m(\lambda)$.\\
        Пусть $P_{m-1}$ --- линейное пространство всех многочленов степени $\leqslant m-1$. $\dim P_{m-1}=m$. Тогда $I_\lambda=\{p\in P_{m-1}\mid p\divby\varphi_\lambda\}$ --- \undercolor{red}{главный идеал} в пространстве $P_{m-1}$, порождённый $\varphi_\lambda$.
        \begin{Comment}
            Из каких многочленов состоит главный идеал? Ну, мы взяли $\varphi_\lambda$ и дополнили его некоторыми корнями, но так, чтобы степень не стала больше либо равна $m$. При этом, несложно заметить, что если $p\in I_\lambda$, то $p=a_\lambda\varphi_\lambda$, где $\deg a_\lambda\leqslant m(\lambda)-1$. При этом заметим, что любой такой $a_\lambda$ нам подходит, а значит $I_\lambda$ можно поставить в соответствие $P_{m(\lambda)-1}$. При этом несложно заметить, что $I_\lambda$ является подпространством и что соответствие с $P_{m(\lambda)-1}$ --- изоморфизм. А значит $\dim I_\lambda=m(\lambda)$.
        \end{Comment}
        \thm $P_{m-1}=\bigoplus\limits_\lambda I_\lambda$.
        \begin{Proof}
            Сначала дизъюнктность. Рассмотрим $p_\lambda\in I_\lambda$ такие что $\sum\limits_\lambda p_\lambda=\mathbb0$. Хочется доказать, что тогда все $p_\lambda=\mathbb0$. Итак, $\sum\limits_\lambda p_\lambda=a_\mu\varphi_\mu+\sum\limits_{\lambda\neq\mu}a_\lambda\varphi_\lambda$. Заметим, что первое делится на $(t-\mu)^{m(\mu)}$, и $\varphi_\lambda$ --- тоже. При этом $\varphi_\mu(t)$ с $(t-\mu)^{m(\mu)}$ взаимно просто, а значит $a_\mu(t)\divby(t-\mu)^{m(\mu)}$. Но подождите-ка, $\deg a_\mu\leqslant m(\mu)-1$. А это значит, что он тождественен нулю, то есть и $p_\mu=\mathbb0$. Поскольку то же можно провести для любого $\mu$, подпространства дизъюнктны.\\
            А то, что их сумма равна $P_{m-1}$ тривиально, ведь $\dim I_\lambda=m(\lambda)$, а значит сумма размерностей равна $m$.
        \end{Proof}
        \dfn \undercolor{red}{Полиномиальным разложением единицы}, порождённым многочленом $\varphi(t)$ называется $\sum\limits_\lambda p_\lambda(t)=1$, где $p_\lambda\in I_\lambda$.
        \begin{Comment}
            Если $\lambda\neq\mu$, то $p_\lambda p_\mu\divby\varphi(t)$ так как $p_\lambda p_\mu=a_\lambda\varphi_\lambda a_\mu\varphi_\mu$ и произведение второго и четвёртого делится на $\varphi$.
        \end{Comment}
        \begin{Comment}
            Если $\forall\lambda~m(\lambda)=1$, то $a_\lambda=\mathrm{const}=\alpha_\lambda$, а значит $p(t)=\sum\limits_\lambda\alpha_\lambda\varphi_\lambda(t)$.
        \end{Comment}
        \thm \undercolor{darkgreen}{Теорема Лагранжа}. Пусть $\forall\lambda~m(\lambda)=1$. Тогда $p(t)=\sum\limits_\lambda\frac{p(\lambda)}{\varphi'(\lambda)}\varphi_\lambda(t)$. То есть в терминах предыдущего комментария $\alpha_\lambda=\frac{p(\lambda)}{\varphi'(\lambda)}$.
        \begin{Proof}
            $p(t)=\sum\limits_\lambda\alpha_\lambda\varphi_\lambda(t)$. А это в частности значит, что $p(\lambda)=\sum\limits_\mu\alpha_\mu\varphi_\mu(\lambda)$. Но давайте внимательно вглядимся в $\varphi_\mu(\lambda)$. Поскольку $\varphi_\mu$ --- это просто $\varphi$ из которого мы вычленили $(t-\mu)^{m(\mu)}$, все остальные корни $\varphi$ также являются корнями $\varphi_\mu$. То есть все $\lambda\neq\mu$ обнуляют $\varphi_\mu(\lambda)$. А это значит из суммы остаётся только $\alpha_\lambda\varphi_\lambda(\lambda)$. Отсюда $\alpha_\lambda=\frac{p(\lambda)}{\varphi_\lambda(\lambda)}$. Осталось только доказать, что $\varphi_\lambda(\lambda)=\varphi'(\lambda)$. Почему так? Если $\varphi(t)=\prod\limits_\mu(t-\mu)$, то $\varphi'(t)=\sum\limits_\mu\prod\limits_{\nu\neq\mu}(t-\nu)$. Что тогда такое $\varphi'(\lambda)$? Ну, тогда во все слагаемые, кроме одного ($\mu=\lambda$), войдут $(\lambda-\lambda)$. То есть $\varphi'(\lambda)=\prod\limits_{\nu\neq\lambda}(\lambda-\nu)$. А это и есть $\varphi_\lambda(\lambda)$.
        \end{Proof}
        \thm Если $\forall\lambda~m(\lambda)=1$, то $t=\sum\limits_\lambda\lambda p_\lambda(t)$, где $\sum\limits_\lambda p_\lambda(t)=1$.
        \begin{Proof}
            Как мы знаем, $\sum\limits_\lambda\frac{1}{\varphi'(\lambda)}\varphi_\lambda(t)=1$, потому что 1 --- многочлен такой что $1(\lambda)=1$. А $t(\lambda)=\lambda$. Поэтому $\sum\limits_\lambda\frac{\lambda}{\varphi'(\lambda)}\varphi_\lambda(t)=t$.
        \end{Proof}
        \dfn Если $\scriptA\in\End(V)$ и все корни $\chi$ и $\varphi$ лежат в $\scriptK$, то $\varphi(t)=\prod\limits_\lambda(t-\lambda)^{m(\lambda)}=(t-\lambda)^{m(\lambda)}\varphi_\lambda(t)$. Тогда $\id=\sum\limits_\lambda p_\lambda(\scriptA)=\sum\limits_\lambda \scriptP_\lambda$ --- \undercolor{red}{операторное разложение единицы}, $\scriptP_\lambda$ --- \undercolor{red}{спектральные проекторы}, $\Im\scriptP_\lambda$ --- \undercolor{red}{спектральные подпространства}.
        \thm $\scriptP_\lambda$ правда являются проекторами.
        \begin{Proof}
            Проверим, что $\scriptP_\lambda\scriptP_\mu=\mathbb0$ для $\mu\neq\lambda$. Известно, что $p_\lambda(t)=a_\lambda(t)\varphi_\lambda(t)$, а значит $\scriptP_\lambda\scriptP_\mu=a_\lambda(\scriptA)\varphi_\lambda(\scriptA)a_\mu(\scriptA)\varphi_\mu(\scriptA)$. Эта штука, как мы знаем, делится на $\varphi(\scriptA)$, а $\varphi(\scriptA)=\mathbb0$.
        \end{Proof} 
        \dfn \undercolor{red}{Корневое подпространство} $K_\lambda=\ker(\scriptA-\lambda\id)^{m(\lambda)}$.
        \thm $K_\lambda$ инвариантно относительно $\scriptA$.
        \begin{Proof}
            Рассмотрим $x\in K_\lambda$. Рассмотрим $(\scriptA-\lambda\id)^{m(\lambda)}\scriptA x=\scriptA(\scriptA-\lambda\id)^{m(\lambda)}x=\mathbb0$, то есть $\scriptA x\in K_\lambda$ --- инвариант.
        \end{Proof}
        \thm $\Im P_\lambda=K_\lambda$.
        \begin{Proof}
            Пусть $x_\lambda\in K_\lambda$. Тогда $(\scriptA-\lambda\id)x_\lambda=\mathbb0$. Теперь возьмём $x_\lambda\in K_\lambda$. Тогда $x_\lambda=\id x_\lambda=\sum\limits_{\mu}\scriptP_\mu x_\lambda=\scriptP_\lambda x_\lambda\in\Im\scriptP_\lambda$. Следовательно $K_\lambda\subseteq\Im\scriptP_\lambda$.\\
            Рассмотрим $x\in V$. Хочется доказать, что $\scriptP_\lambda x\in K_\lambda$. $(\scriptA-\lambda\id)^{m(\lambda)}\scriptP_\lambda x=(\scriptA-\lambda\id)^{m(\lambda)}a_\lambda(\scriptA)\varphi_\lambda(\scriptA)x$. Тут мы произведением первого и третьего множителей имеем $\varphi(\scriptA)$, то есть всё произведение --- $\mathbb0$. А это значит, что $\Im\scriptP\subseteq K_\lambda$.
        \end{Proof}
        \thm $(t-\lambda)^{m(\lambda)}$ является минимальным многочленом $\scriptA\big|_{K_\lambda}$.
        \begin{Proof}
            Обозначим сужение оператора за $\scriptB$. Очевидно, $\psi(t)=(t-\lambda)^{m(\lambda)}$ --- аннулятор $\scriptB$. Нужно лишь доказать его минимальность. Ну, пусть он не минимален. А минимален $\psi_1$. Тогда рассмотрим $\varphi_1=\psi_1\varphi_\lambda$. $\varphi_1(\scriptA)x=\varphi_1(\scriptA)\sum\limits_\mu x_\mu=\sum\limits_{\mu}\psi_1(\scriptA)\varphi_\lambda(\scriptA)x_\mu=\psi_1(\scriptA)\varphi_\lambda(\scriptA)x_\lambda=\mathbb0$, то есть $\varphi_1$ является аннулятором $\scriptA$, но у него степень меньше $\varphi$, что противоречит с минимальностью $\varphi$.
        \end{Proof}
        \thm Если $\scriptA\in\End(V)$, все корни $\chi$, $\varphi$ лежат в $\scriptK$. Тогда $\scriptA$ --- оператор простой структуры тогда и только тогда, когда $\forall\lambda~m(\lambda)=1$.
        \begin{Proof}
            \rightimp. Если о.п.с., то $V=\bigoplus\limits_\lambda V_\lambda$. Покажем, что $\varphi(t)=\prod\limits_{\lambda}(t-\lambda)$ --- минимальный многочлен. Его минимальность тривиальна, надо доказать лишь что он аннулятор. Мы знаем, что $x=\sum\limits_{\lambda}x_\lambda$, где $x_\lambda\in V_\lambda$. Тогда рассмотрим $\varphi(\scriptA)x=\sum\limits_{\mu}\varphi(\scriptA)x_\mu=\sum\limits_{\mu}q_\mu(\scriptA)(\scriptA-\mu\id)x_\mu=\mathbb0$.\\
            \leftimp. Известно, что $\prod\limits_\lambda(t-\lambda)$ --- минимальный многочлен. Построим $\sum\limits_{\lambda}\scriptP_\lambda=\id$. Как мы знаем, $\sum\limits_{\lambda}\Im\scriptP_\lambda=V$. Также мы знаем, что $\Im\scriptP_\lambda=K_\lambda=\ker(\scriptA-\lambda\id)=V_\lambda$. Это значит, что $\bigoplus\limits_{\lambda}V_\lambda=V$, что и значит, что $\scriptA$ --- о.п.с.
        \end{Proof}
        \thm По следствию теоремы Лагранжа $t=\sum\limits_{\lambda}\lambda p_\lambda(t)$, а это значит, что $\scriptA=\sum\limits_{\lambda}\lambda\scriptP_\lambda$. Мы снова получили спектральное разложение.
        \thm $\forall\lambda~m(\lambda)\leqslant\dim K_\lambda$.
        \begin{Proof}
            Степень минимального многочлена не может превосходить размерности пространства, в котором оно работает.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Нимпотентный оператор}. \undercolorblack{orange}{Разложение Жордана}.}
    \begin{itemize}
        \dfn Эндоморфизм $\scriptB$ называется \undercolor{red}{нильпотентным оператором}, если его минимальный многочлен имеет вид $t^\nu$, где $\nu>0$, а $\nu$ называется \undercolor{red}{индексом нильпотентности}.
        \thm \undercolor{darkgreen}{Теорема о разложении Жордана}. $\scriptA\in\End(V)$, все корни $\chi$, $\varphi$ лежат в $\scriptK$. Тогда $\scriptA=\scriptD+\scriptB$, где $\scriptD$ --- оператор простой структуры, а $\scriptB$ --- нильпотентный, причём $\scriptD\scriptB=\scriptB\scriptD$.
        \begin{Proof}
            Рассмотрим $\id=\sum\limits_{\lambda}\scriptP_\lambda$. Тогда $\scriptD=\sum\limits_{\lambda}\lambda\scriptP_\lambda$, $\scriptB=\scriptA-\scriptD$. Очень хочется верить, что $\scriptD$ --- о.п.с., то есть что $\scriptP_\lambda$ --- это и его спектральные проекторы тоже. А если это его собственные пространства, то получим спектральное разложение $\scriptD$. Рассмотрим $\scriptD\scriptP_\lambda x=\sum\limits_{\mu}\mu\scriptP_\mu\scriptP_\lambda x=\lambda\scriptP_\lambda x$. То есть мы взяли элемент $\scriptP_\lambda x\in\Im\scriptP_\lambda$, а мы получили его, но умноженный на $\lambda$. То есть весь $\Im\scriptP_\lambda$ является собственным подпространством $\scriptD$. Точнее, мы получили включение первого во второе, но поскольку сумма размерностей $\Im\scriptP_\lambda$ равна $n$, получается равенство.\\
            Теперь докажем нильпотентность $\scriptB$. $\nu=\max\limits_{\lambda}m(\lambda)$. Тогда рассмотрим $\scriptB^\nu=(\scriptA-\scriptD)^\nu=\left(\scriptA\id-\sum\limits_{\lambda}\lambda\scriptP_\lambda\right)^\nu=\left(\scriptA\sum\limits_{\lambda}\scriptP_\lambda-\sum\limits_{\lambda}\lambda\scriptP_\lambda\right)^\nu=
            \left(\sum\limits_{\lambda}(\scriptA-\lambda\id)\scriptP_\lambda\right)^\nu=\sum\limits_{\lambda}(\scriptA-\lambda\id)^\nu\scriptP_\lambda$. Смотрите, что получается. Возьмём $\scriptB^\nu x$. Получим $\sum\limits_{\lambda}(\scriptA-\lambda\id)^{\max\limits_{\mu}m(\mu)}\scriptP_\lambda x$.\\
            Теперь докажем, что $\scriptB$ и $\scriptD$ можно менять местами. Ну, лол, вы можете $\scriptP_\lambda$ и $A-\lambda\id$ менять местами, значит и $\scriptB$ и $\scriptD$ --- тоже.
        \end{Proof}
        \thm $\scriptB=\sum\limits_{\lambda}(\scriptA-\lambda\id)\scriptP_\lambda=\sum\limits_{\lambda}\scriptB_\lambda$, где $\scriptB_\lambda=(\scriptA-\lambda\id)\big|_{K_\lambda}$.
        \thm Все три оператора из теоремы ($\scriptA$, $\scriptB$ и $\scriptD$) перестановочны.
        \thm \undercolor{darkgreen}{Теорема о единственности разложения Жордана}. $\scriptA\in\End(V)$, все корни $\chi$, $\varphi$ лежат в $\scriptK$. Тогда разложение Жордана единственно.
        \begin{Proof}
            Докажем от противного. Пусть есть другое разложение $\scriptD'+\scriptC$. $\scriptD'=\sum\limits_{\mu}\mu\scriptQ_\mu$, где $\sum\limits_{\mu}\scriptQ_\mu=\id$ и $\scriptD'\scriptC=\scriptC\scriptD'$. Тогда нам надо доказать, что множество $\mu$ и множество $\lambda$ совпадают и что $\Im\scriptP_\mu=\Im\scriptQ_\mu$. Тогда мы получим, что $\scriptD=\scriptD'$.\\
            Без \sout{пол-литра} плана в этом доказательстве не разобраться, поэтому вот он:
            \begin{enumerate}
                \item Нашей целью является подбор таких многочленов $\psi_\mu(t)=(t-\mu)^{k(\mu)}$, что их произведение будет равно $\varphi$. Тогда мы докажем, что $\mu=\lambda$.
                \item Эти самые $\psi_\mu$ будут подбираться как аннуляторы всех элементов $\Im\scriptQ_\mu$, для чего нам придётся доказать, что $\Im\scriptQ_\mu\subset\ker\psi_\mu(\scriptA)=\ker(\scriptA-\mu\id)^{k(\mu)}$.
                \item Чтобы доказать этот факт, мы посмотрим, чему равно произведение $(\scriptA-\mu\id)^k\scriptQ_\mu$. И хотим мы получить, что оно равно $C^k\scriptQ_\mu$.
                \item Чтобы доказать это равенство первым делом докажем равенство $(\scriptA-\mu\id)^k\scriptQ_\mu=\scriptQ_\mu\scriptC^k$, а потом докажем, что $\scriptQ_\mu$ и $\scriptC$ можно переставлять местами.
                \begin{itemize}
                    \item Сначала мы докажем перестановочность $\scriptD'$ и $\scriptQ_\mu$.
                    \item Потом --- что $\scriptQ_\mu\scriptC\scriptQ_\xi=\scriptQ_\xi\scriptC\scriptQ_\mu$.
                    \item И в конце получим перестановочность $\scriptQ_\mu$ и $\scriptC$.
                \end{itemize}
            \end{enumerate}
            Для начала посмотрим, что будет, если применить $\scriptA-\mu\id$ к $\scriptQ_\mu$. Получится $\left(\sum\limits_\xi\xi\scriptQ_\xi+\scriptC-\mu\id\right)\scriptQ_\mu=\mu\scriptQ_\mu+\scriptC\scriptQ_\mu-\mu\scriptQ_\mu=\scriptC\scriptQ_\mu$.\\
            Теперь докажем, что $\scriptC\scriptQ_\mu=\scriptQ_\mu\scriptC$. Что будет, если мы возьмём $\scriptD'\scriptQ_\mu$. Получится $\sum\limits_\xi\xi\scriptQ_\xi\scriptQ_\mu=\mu\scriptQ_\mu$. И если возьмём $\scriptQ_\mu\scriptD'$ --- тоже. То есть $\scriptQ_\mu$ и $\scriptD'$ перестановочны. Теперь заметим следующий факт: $(\xi-\mu)\scriptQ_\xi\scriptC\scriptQ_\mu=\scriptQ_\xi\scriptD'\scriptC\scriptQ_\mu-\scriptQ_\xi\scriptC\scriptD'\scriptQ_\mu=\scriptQ_\xi(\scriptD'\scriptC-\scriptC\scriptD')\scriptQ_\mu=\mathbb0$. Тогда, если $\xi\neq\mu$, то $\scriptQ_\xi\scriptC\scriptQ_\mu=\mathbb0=\scriptQ_\mu\scriptC\scriptQ_\xi$, а если равно, тем более. А тогда $\scriptC\scriptQ_\mu=\sum\limits_{\xi}\scriptQ_\xi\scriptC\scriptQ_\mu=\sum\limits_{\xi}\scriptQ_\mu\scriptC\scriptQ_\xi=\scriptQ_\mu\scriptC\sum\limits_{\xi}\scriptQ_\xi=\scriptQ_\mu\scriptC$.\\
            А теперь рассмотрим $(\scriptA-\mu\id)^k\scriptQ_\mu$. Как мы уже доказали выше, $(\scriptA-\mu\id)\scriptQ_\mu=\scriptC\scriptQ_\mu=\scriptQ_\mu\scriptC$, а значит $(\scriptA-\mu\id)^k\scriptQ_\mu=\scriptQ_\mu\scriptC^k$. Поскольку $\scriptC$ --- нильпотентный оператор, $\scriptC^\nu$ обнуляет всё пространство, а значит и $\scriptQ_\mu\scriptC^\nu$ и подавно, но для $\scriptQ_\mu\scriptC^k$ может быть возможно, что можно взять $k$ и поменьше, чем $\nu$. Обозначим это самое минимальное $k$ за $k(\mu)$. Тогда $\scriptQ_\mu\scriptC^{k(\mu)}=\mathbb0$, то есть $(\scriptA-\mu\id)^{k(\mu)}\scriptQ_\mu=\mathbb0$, то есть $\Im\scriptQ_\mu\subseteq\ker(\scriptA-\mu\id)^{k(\mu)}$, что в свою очередь значит, что $(t-\mu)^{k(\mu)}$ --- аннулятор всех элементов $\Im\scriptQ_\mu$. Назовём его $\psi_\mu$.\\
            Известно, что $\varphi$ является минимальным многочленом $\scriptA$, а значит и аннулятором всех элементов $\Im\scriptQ_\mu$ тоже является, но тогда $\varphi\divby\psi_\mu$. Все $\psi_\mu$ взаимно просты, значит все $\varphi\divby\prod\limits_{\mu}\psi_\mu$. Тогда корни $\psi_\mu$ (то есть сами $\mu$) --- корни $\varphi$. Покажем, что $\prod\limits_{\mu}\psi_\mu(t)$ --- аннулятор $\scriptA$, тогда получится, что корни $\prod\limits_{\mu}\psi_\mu(t)$ и $\varphi(t)$ совпадают. Мы хотим рассмотреть $\prod\limits_{\mu}\psi_\mu(\scriptA)$ и доказать, что это нулевой оператор. Или что для любого $x\in V$ $\prod\limits_{\mu}\psi_\mu(\scriptA)x=\mathbb0$. Для этого разложим $x$ как $\sum\limits_\xi x_\xi$, где $x_\xi\in\Im\scriptQ_\xi$. Тогда $\prod\limits_{\mu}\psi_\mu(\scriptA)x=\prod\limits_{\mu}(\scriptA-\mu\id)^{k(\mu)}x=\sum\limits_\xi\prod\limits_{\mu}(\scriptA-\mu\id)^{k(\mu)}x_\xi$. Заметим, что из этого произведения никто не убирал $\mu=\xi$, а значит там встречается множитель $(\scriptA-\xi\id)^{k(\xi)}x_\xi$. Этот множитель равен нулю, поскольку $x_\xi\in\Im\scriptQ_\xi$, а $(\scriptA-\xi\id)^{k(\xi)}=\psi_\xi$ аннулирует все элементы $\Im\scriptQ_\xi$. Хорошо, значит у нас есть сумма некоторого количества нулей, значит она равна нулю. Значит $\prod\limits_{\mu}\psi_\mu$ действительно аннулятор, а значит делится на минимальный многочлен $\varphi$. Итого $\prod\limits_{\mu}\psi_\mu$ и $\varphi$ делятся друг на друга, а значит это один и тот же многочлен. А значит корни этих многочленов одинаковы. И тут мы доказали первый пункт, что хотели --- $\lambda$ совпадают с $\mu$. Более того, равны же не только корни, но и их кратности, а значит $k(\mu)=m(\mu)$.\\
            Осталось доказать, что $\Im\scriptP_\mu=\Im\scriptQ_\mu$. Как мы уже доказали выше, $\Im\scriptQ_\mu\subseteq\ker(\scriptA-\mu\id)^{k(\mu)}=\ker(\scriptA-\mu\id)^{m(\mu)}=K_\mu=\Im\scriptP_\mu$. Но подождите-ка, $\bigoplus\limits_\mu\Im\scriptQ_\mu=V=\bigoplus\limits_\mu\Im\scriptP_\mu$. Будет очень странно, если $\scriptQ_\mu$ не равны $\scriptP_\mu$.
        \end{Proof}
        \thm Пусть $\scriptA=\scriptD+\scriptB$ --- разложение Жордана. Тогда $\chi_\scriptA(t)=\chi_\scriptD(t)$.
        \begin{Proof}
            $\chi_\scriptA(t)=\det(\scriptA-t\id)$. Тогда $\chi^k_\scriptA=\det(\scriptA-t\id)^k$. Посмотрим, что такое $(\scriptA-t\id)^\nu$, где $\nu=\max\limits_\lambda\{m(\lambda)\}$ --- индекс нильпотентности $\scriptB$. Пусть $\mu$ --- не корень $\chi_\scriptA$. Рассмотрим $(\scriptA-\mu\id)^\nu-(t\scriptB)^\nu$. С одной стороны (по нильпотентности $\scriptB$) это равно $(\scriptA-\mu\id)^\nu$. С другой стороны мы знаем, что $a^k-b^k=(a-b)(a^{k-1}+a^{k-2}b+\cdots+b^{k-1})$. Это можно применять, потому что все матрицы у нас перестановочные (по теореме о разложении Жордана). Поэтому $(\scriptA-\mu\id)^\nu-(t\scriptB)^\nu=(\scriptA-\mu\id-t\scriptB)((\scriptA-\mu\id)^{\nu-1}+(\scriptA-\mu\id)^{\nu-2}t\scriptB+\cdots+(t\scriptB)^{\nu-1})$. Вернёмся к характеристическому многочлену $\chi_\scriptA(\mu)$ и возведём его в степень $\nu$. Получим $\det(\scriptA-\mu\id)^\nu=\det(\scriptA-\mu\id-t\scriptB)\det((\scriptA-\mu\id)^{\nu-1}+(\scriptA-\mu\id)^{\nu-2}t\scriptB+\cdots+t^{\nu-1}\scriptB^{\nu-1})$. Мы получаем произведение многочленов от $t$. Но слева у нас нет $t$, а значит в произведении всё, содержащее $t$ должно как-то сократиться. А это значит, что (по независимости обоих множителей от $t$) мы можем подставить в левый $t=1$, а в правый --- $t=0$. В первом получим $\det(\scriptA-\mu\id-\scriptB)=\det(\scriptD-\mu\id)$, во втором --- $\chi^{\nu-1}_\scriptA(\mu)$. То есть $\chi^\nu_\scriptA(\mu)=\chi_\scriptD(\mu)\cdot\chi_\scriptA^{\nu-1}(\mu)$. А поскольку $\mu$ --- не корень, на $\chi_\scriptA(\mu)$ можно поделить и получить $\chi_\scriptA(\mu)=\chi_\scriptD(\mu)$. Что это нам даёт? Ну, смотрите. Мы знаем, что те значения $t$, которые не корень $\chi_\scriptA$ также не корень $\chi_\scriptD$. Остаются ещё $n$ значений, про которые мы ничего не знаем (которые корни $\chi_\scriptA$). Но ведь $\chi_\scriptD$ --- многочлен степени $n$, значит $n$ корней у него есть. А значит те самые значения $t$, которые являются корнями $\chi_\scriptA$ также являются корнями $\chi_\scriptD$. То есть значения $\chi_\scriptA$ и $\chi_\scriptD$ совпадают не только в не-корнях, но и в корнях, а значит это один и тот же многочлен.
        \end{Proof}
        \thm $\det\scriptA=\det\scriptD$.
        \begin{Proof}
            $\det\scriptA=\chi_\scriptA(0)=\chi_\scriptD(0)=\det\scriptD$.
        \end{Proof}
        \thm $\forall\lambda\text{ --- собственное число}~\alpha(\lambda)=\dim\Im\scriptP_\lambda=\dim K_\lambda$.
        \begin{Proof}
            $\alpha(\lambda)$ --- это кратность корня $\lambda$ в многочлене $\chi_\scriptA$. А значит и в многочлене $\chi_\scriptD$. Но поскольку $\scriptD$ --- оператор простой структуры, его геометрическая кратность равна алгебраической. Но геометрическая кратность в $\scriptD$ --- это и есть $\dim V^\scriptD_\lambda=\dim\Im\scriptP_\lambda$.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Жорданова форма матрицы}. \undercolorblack{orange}{Жорданов базис}.}
    \begin{itemize}
        \item 
        \begin{Comment}
            Пусть $\scriptA\in\End(V)$, все корни $\chi_\scriptA$ лежат в $\scriptK$. Как мы знаем, $V=\bigoplus\limits_\lambda K_\lambda$. Мы хотим построить в $V$ какой-то <<хороший>> базис. А для этого его надо построить в каждом $K_\lambda$, чтобы в сумме получить блочную матрицу для каждого корневого подпространства. А раз мы будем всё делать для каждого $K_\lambda$ одинаково, зафиксируем $\lambda$ и будем опускать его во всех обозначениях: $\alpha$, $\gamma$, $m$ и $K$ (не путать с $\scriptK$). А ещё обозначим $\scriptB=\scriptA-\lambda\id\big|_{K_\lambda}$.
        \end{Comment}
        \pagebreak
        \begin{Comment}
            Алгоритм выглядит так: мы берём последовательность $V_\lambda\subsetneq K_1\subsetneq K_2\subsetneq\cdots\subsetneq K_m=K_\lambda=K$, где $K_r=\ker\scriptB^r$. Сначала почему $K_r\neq K_{r+1}$. Если равны, то равны их размерности, а по теореме о ранге и дефекте тогда равны $\Im\scriptB^r=\Im\scriptB^{r+1}$. А это значит, что $\Im\scriptB\cdot\scriptB^r=\Im\scriptB^r$, то есть $\ker\scriptB=\mathbb0$ или $\scriptB^k=\mathbb0$, оба варианта нам не подходят. Первый по нетривиальности $V_\lambda$, второй --- по $m=m(\lambda)$. Потом мы берём, например, $K_m$ и $K_{m-1}$ и говорим, что поскольку они не равны, в базисе $K_m$ есть несколько лишних базисных элементов. Вот возьмём их. Их линейную оболочку возьмём (пусть она $j_m$), и применим к ней $\scriptB$. Потом ещё раз. И ещё. Утверждается, что $\scriptB j_m\in K_{m-1}$. Что логично. Да и вообще, если $j_r\in K_r=\ker\scriptB^r$, то $\scriptB j_r\in\ker\scriptB^{r-1}=K_{r-1}$. И вот если мы возьмём все эти $j_1;j_2;j_3;\ldots;j_{m-1};j_m$ и скажем, что это циклический базис. И все вектора, кроме $j_m$ --- присоединённые к нему. И обозначим $\scriptT_m=\Lin\{j_1;j_2;\ldots;j_m\}$. После чего рассмотрим $\scriptA\big|_{\scriptT_m}$. Поскольку $j_1\in K_1=V_\lambda$, $\scriptA j_1=\lambda j_1$. Дальше $\scriptA j_2=\scriptA\scriptB j_1=\scriptA(\scriptA-\lambda\id)j_2=j_1+\lambda j_2$. А $\scriptA j_3=j_2+\lambda j_3$. И так далее, $\scriptA j_m=j_{m-1}+\lambda j_m$. Получается, что в этом базисе нашему оператору соответствует матрица $\matr{
                \lambda & 1 & 0 & 0 & 0\\
                0 & \lambda & 1 & 0 & 0\\
                0 & 0 & \lambda & 1 & 0\\
                0 & 0 & 0 & \lambda & 1\\
                0 & 0 & 0 & 0 & \lambda\\
            }$. Эта штука --- <<клетка Жордана>>, порождённая базисом $(j_1;\ldots;j_m)$ (в данном случае $(j_1;\ldots;j_5)$) размерности $m\times m$ (блок нижнего уровня).\\
            Ну хорошо, но вы думали, что это всё? Как бы не так. Для начала, что делать, если в $K_m$ есть несколько базисных векторов, которых нет в $K_{m-1}$? Ну, возьмём их все, и $\scriptT_m$ будет состоять из нескольких циклических базисов (получится объект, называемый <<башней>>). Но это вообще ещё не всё, потому что потом у нас могут остаться неиспользованные вектора из $K_{m-1}$. Так давайте на их основе также строить башню $\scriptT_{m-1}$. Так делаем, пока векторов не останется. Итого $\scriptT_r$ --- это прямая сумма подпространств, порождённых циклическими базисами длины $r$. Её высота --- это $r$, её ширина (количество базисов) --- $d_r$. Итого у нас получается своеобразный <<з\'{а}мок>> Жордана. Что нужно понимать про этот замок? Последний вектор каждой башни лежит в $K_1=V_\lambda$. А значит общее число <<клеток>> равно $\dim V_\lambda=\gamma$. Также $K_\lambda$ равно сумме всех циклических базисов, и это самое объединение базисов (причём ещё по всем корневым подпространствам, то есть по всем $\lambda$) --- жорданов базис, а матрица $\scriptA$ в этом базисе --- жорданова форма. При этом то, что соответствует <<клетке>> --- блок нижнего уровня, то, что соответствует <<башне>> --- блок среднего уровня, что соответствует $K_\lambda$ --- блок верхнего уровня. И блоки верхнего уровня обозначаются как $J$.
        \end{Comment}
        \begin{Example}
            Иногда можно не искать базис. Например, если $\alpha=4$, $\gamma=3$, то у матрицы $J$ вариантов немного, как выглядеть. Только один, собственно. Вот такой: $\matr{
                \lambda & 1 & 0 & 0\\
                0 & \lambda & 0 & 0\\
                0 & 0 & \lambda & 0\\
                0 & 0 & 0 & \lambda
            }$. А если вас просят базис, то проще уравнение $TJ=AT$ решить. Хотя, не всегда проще.\\
            Усложним задачу. Что делать, если $\alpha=4$, а $\gamma=2$. То непонятно, вариантов уже два. Но если добавить условие $m=3$, то вариант $\matr{
                \lambda & 1 & 0 & 0\\
                0 & \lambda & 0 & 0\\
                0 & 0 & \lambda & 1\\
                0 & 0 & 0 & \lambda
            }$ будет некорректным, потому что $m$ равно высоте максимальной башни, а на указанной матрице нет башни высоты 3.
        \end{Example}
        \begin{Example}
            Хорошо, как выглядит <<замок Жордана>>, если оператор имеет простую структуру? Ну, абсолютно очевидно, там деревенька получается.
        \end{Example}
        \begin{Comment}
            Теперь нам нужно это доказать. Давайте введём обозначение $\scriptB^kK_r=\Im\scriptB^k\big|_{K_r}$. И ещё введём $Z_0=\scriptB K$, $Z_r=\scriptB K+K_r$.\\
            Несложно заметить, что $Z_m=\scriptB K+K_m=K_m$. Также очевидно, что $Z_0\subseteq Z_1\subseteq\cdots\subseteq Z_m$. Нас нестрогие вложения не устраивают, мы хотим строгие. $Z_{r+1}=Z_r\oplus\overline{K_r}$, где $\overline{K_r}\subset K_r$. То есть $K=Z_m=\scriptB K\oplus\overline{K_1}\oplus\overline{K_2}\oplus\cdots\oplus\overline{K_m}$. $\overline{K_r}$ --- опорные пространства.
        \end{Comment}
        \thm $\scriptB^rK=\scriptB^rK_{r+1}\oplus\scriptB^rK_{r+2}\oplus\cdots\oplus\scriptB^rK_m\oplus\scriptB^{r+1}K$.
        \begin{Proof}
            Возьмём наше равенство $K=\scriptB K\oplus\overline{K_1}\oplus\overline{K_2}\oplus\cdots\oplus\overline{K_m}$ и применим к нему $\scriptB^r$. Несложно понять, что по дизъюнктности $\overline{K_r}$ и $\scriptB K$ верно, что $\scriptB^r$ просто применяется к подпространствам. Заметим также, что для $j\leqslant r$ $\scriptB^r\overline{K_j}=\mathbb0$. Почему? Ну, потому что $\overline{K_j}\subset K_j=\ker\scriptB^j$. То есть если умножить $\scriptB^j$ к $K_j$, получится ноль. А если мы применим $\scriptB$ в большей степени, то и подавно. Итого получаем $\scriptB^rK=\scriptB^{r+1}K\oplus\scriptB^r\overline{K_{r+1}}\oplus\scriptB^r\overline{K_{r+2}}\oplus\cdots\oplus\scriptB^r\overline{K_m}$. Ну, почти это. Мы точно получили $\scriptB^rK=\scriptB^{r+1}K+\scriptB^r\overline{K_{r+1}}+\scriptB^r\overline{K_{r+2}}+\cdots+\scriptB^r\overline{K_m}$, а то что сумма прямая, надо доказать. Ну, докажем. Рассмотрим $x_r\in\overline{K_r}$, $y\in K$. Надо доказать, что $\scriptB^{r+1}y+\scriptB^rx_{r+1}+\scriptB^rx_{r+2}+\cdots+\scriptB^rx_m=\mathbb0$ тогда и только тогда, когда все $x_r$ равны нулю, и $y$ --- тоже. Давайте вынесем отсюда $\scriptB^r$. Получится $\scriptB^r(\scriptB y+x_{r+1}+x_{r+2}+\cdots+x_m)=\mathbb0$. Такое бывает, когда $\scriptB y+x_{r+1}+x_{r+2}+\cdots+x_m\in\ker\scriptB^r=K_r\subset Z_r=\scriptB K\oplus\overline{K_1}\oplus\overline{K_2}\oplus\cdots\oplus\overline{K_r}$. Но тогда мы получили разложение по базису. Но вектора $x_{r+1}$, $x_{r+2}$, ..., $x_m$ тут лишние, а значит равны нулю. То есть $\scriptB^r x_j=\mathbb0$ для всех $j\in[r+1:m]$. А это значит, что $B^{r+1}y=\mathbb0$, раз $\sum\limits_{j\in[r+1:m]}\scriptB^r x_j+B^{r+1}y=\mathbb0$. А значит все вектора как раз нули.
        \end{Proof}
        \thm $K=\overline{K_1}\oplus\overline{K_2}\oplus\cdots\oplus\overline{K_m}\oplus\scriptB\overline{K_2}\oplus\scriptB\overline{K_3}\oplus\cdots\oplus\scriptB\overline{K_m}\oplus\cdots\oplus\scriptB^{m-2}\overline{K_{m-1}}\oplus\scriptB^{m-1}\overline{K_m}$.
        \begin{Proof}
            Возьмём $K=\scriptB K\oplus\overline{K_1}\oplus\overline{K_2}\oplus\cdots\oplus\overline{K_m}$. Имеем в начале $\scriptB K$, подставим туда вместо $K$ это равенство из предыдущей теоремы для $r=1$. Получим то же самое, что и для $K$, но со слагаемыми $\scriptB\overline{K_2}$, $\scriptB\overline{K_3}$ и т.д. И ещё получим $\scriptB^2 K$. И туда тоже подставим и раскроем. Будем делать так до посинения, пока не получим условие теоремы (т.е. пока в конце не вылезет $\scriptB^{m-1}\overline{K_m}$). Как раз подставлять и раскрывать мы можем по предыдущей теореме, они по ней дизъюнктны и получатся.
        \end{Proof}
        \thm Теперь мы получили доказательство алгоритма Жордана.
        \begin{Proof}
            \begin{center}
                \begin{tabular}{|c|c|c|c|c|c|c|}
                    \cline{7-7}
                    \multicolumn{6}{c|}{} & \bigg.$\overline{K_m}$\\
                    \cline{6-7}
                    \multicolumn{5}{c|}{} & \bigg.$\overline{K_{m-1}}$ & $\scriptB\overline{K_m}$\\
                    \cline{5-7}
                    \multicolumn{4}{c|}{} & \bigg.$\overline{K_{m-2}}$ & $\scriptB\overline{K_{m-1}}$ & $\scriptB^2\overline{K_m}$\\
                    \cline{4-7}
                    \multicolumn{3}{c|}{} & \Big.\reflectbox{$\ddots$} & $\vdots$ & $\vdots$ & $\vdots$\\
                    \cline{3-7}
                    \multicolumn{2}{c|}{} & \bigg.$\overline{K_3}$ & $\cdots$ & $\scriptB^{m-5}\overline{K_{m-2}}$ & $\scriptB^{m-4}\overline{K_{m-1}}$ & $\scriptB^{m-3}\overline{K_m}$\\
                    \cline{2-7}
                    \multicolumn{1}{c|}{} & \bigg.$\overline{K_2}$ & $\scriptB\overline{K_3}$ & $\cdots$ & $\scriptB^{m-4}\overline{K_{m-2}}$ & $\scriptB^{m-3}\overline{K_{m-1}}$ & $\scriptB^{m-2}\overline{K_m}$\\
                    \hline
                    \bigg.$\overline{K_1}$ & $\scriptB\overline{K_2}$ & $\scriptB^2\overline{K_3}$ & $\cdots$ & $\scriptB^{m-3}\overline{K_{m-2}}$ & $\scriptB^{m-2}\overline{K_{m-1}}$ & $\scriptB^{m-1}\overline{K_m}$\\
                    \hline
                \end{tabular}
            \end{center}
            Если у нас $\overline{K_r}$ не тривиально, то $\scriptT_r=\overline{K_r}\oplus\scriptB\overline{K_r}\oplus\cdots\oplus\scriptB^{r-1}\overline{K_r}$, а иначе такой башни просто не существует.\\
            При этом хочется сказать, что прямая сумма всех этажей не выше $r$-того --- это $K_r$.\\
            Сначала заметим, что $\overline{K_1}\oplus\scriptB\overline{K_2}\oplus\cdots\oplus\scriptB^{m-1}\overline{K_m}\subset K_1$. Чтобы проверить правдивость этого, надо проверить, что все элементы суммы лежат в $\ker\scriptB$. Ну, они правда лежат, ведь $\scriptB^i\overline{K_i}=\mathbb0$, а значит при применении $\scriptB$ к этой сумме получится только ноль. Теперь заметим, что $\overline{K_1}\oplus\scriptB\overline{K_2}\oplus\cdots\oplus\scriptB^{m-1}\overline{K_m}+\overline{K_2}\oplus\scriptB\overline{K_3}\oplus\cdots\oplus\scriptB^{m-2}\overline{K_m}\subset K_2$ (по той же причине). А значит мы получили, что сумма первого этажа является подмножеством $K_1$, а сумма первого и второго --- $K_2$. Аналогично мы получаем, что сумма всех элементов, кроме верхнего этажа --- подмножество $K_{m-1}$, а сумма вообще всех --- $K_m$. Но сумма всех равна $K_m$, потому что $K_m=K$. А значит посмотрим на последние два набора этажей. В предпоследних --- некоторое подмножество $K_{m-1}$, во всех --- ровно $K_m$. Но тогда это подмножество обязано быть равно $K_{m-1}$, потому что мы добавляем в него $\overline{K_m}$, чтобы получилось $K_m$. А значит и сумма всех этажей, кроме последних двух, не подмножество $K_{m-2}$, а равна ему. И так далее. Итого получаем
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема о размерности <<башни>>}. $\scriptT_r=\overline{K_r}\oplus\scriptB\overline{K^r}\oplus\cdots\oplus\scriptB^{r-1}\overline{K_r}$. Все подпространства $\scriptB\overline{K_r}$ имеют одну и ту же размерность (которую мы обозначаем $d_r$).
        \begin{Proof}
            Покажем, что $\scriptB^r\colon\overline{K_r}\to\scriptB^j\overline{K_r}$ --- изоморфизм для $j\in[1:r-1]$. Для этого докажем тривиальность ядра. Рассмотрим $x\in\ker\scriptB^j\cap\overline{K_r}$. Тогда $x\in\ker\scriptB^j=K_j\subset Z_j=\overline{K_1}\oplus\overline{K_2}\oplus\cdots\oplus\overline{K_j}\oplus\scriptB K$. Но $\overline{K_r}$ дизъюнктно со всеми этими пространствами, а значит $\ker\scriptB^j\cap\overline{K_r}=\{\mathbb0\}$, то есть ядро тривиально, значит это правда изоморфизм.
        \end{Proof}
        \thm $\sum\limits_{r=1}^md_r=\dim V_\lambda=\gamma$. $\sum\limits_{r=1}^mrd_r=\dim K=\alpha$.
        \thm \undercolor{darkgreen}{Теорема Фробениуса}. $\forall r\in[1:m-1]~d_r=\rank\scriptB^{r-1}-2\rank\scriptB^r+\rank\scriptB^{r+1}$. А $d_m=\rank\scriptB^m-\rank\scriptB^{m-1}$.
        \begin{Proof}
            Как мы уже доказывали, $\scriptB^rK=\scriptB^rK_{r+1}\oplus\scriptB^rK_{r+2}\oplus\cdots\oplus\scriptB^rK_m\oplus\scriptB^{r+1}K$. Если взять размерность от обеих частей, получим $\rank\scriptB^r=d_{r+1}+d_{r+2}+\cdots+d_m+\rank\scriptB^{r+1}$. Это равенство можно выписать для любого $r$. Например, для нуля. $\rank\scriptB^0-\rank\scriptB^1=d_1+d_2+\cdots+d_m$. И для единицы: $\rank\scriptB^1-\rank\scriptB^2=d_2+\cdots+d_m$. Из этих двух равенств можно найти $d_1$, он получится как надо в формуле. Потом запишем для $r=2$, получим для него формулу тоже.
        \end{Proof}
        \thm Теперь докажем, что матрица выглядит так, как мы хотим.
        \begin{Proof}
            Сначала докажем, что она блочно-диагональная по башням. Ну, для заданной башни имеем на верхнем уровне набор $g_1;g_2;\ldots;g_d$, потом $\scriptB g_1;\scriptB g_2;\ldots;\scriptB g_d$ (тоже базис, поскольку $\scriptB$ --- изоморфизм). И так далее, по увеличению степени $\scriptB$. При этом эти базисы порождают пространства $\scriptB^0\overline{K_r}$, $\scriptB^1\overline{K_r}$ и так далее, которые дизъюнктны, а значит базисы можно просто объединить, чтобы получить базис $\scriptT_r$. А значит матрица будет блочно-диагональная.\\
            Осталось посмотреть, как выглядят блоки этой матрицы (то есть <<клетки>>). Для этого возьмём один циклический базис $g_i;\scriptB g_i;\ldots;\scriptB^{r-1}g_i$. Ну, чтобы узнать, что будет в <<клетке>>, применим $\scriptA\scriptB^k g_i=(\scriptB+\lambda\id)\scriptB^kg_i=\scriptB^{k+1}g_i+\lambda\scriptB^kg_i$. То есть на главной диагонали будет $\lambda$, а чуть выше --- единица. При этом, при $k+1=r$, <<чуть выше>> не будет существовать, ведь $\scriptB^rg_i=\mathbb0$ из-за того, что $g_i\in\overline{K_r}\subset K_r=\ker\scriptB^r$.\\
            И вот мы уже правда получили блоки верхнего, среднего и нижнего уровней.
        \end{Proof}
        \begin{Comment}
            Хотим ещё один алгоритм построения жордановой формы. У нас есть наши корневые пространства $K_\lambda=\ker(\scriptA-\lambda\id)^{m(\lambda)}$. И мы выяснили, что $\Im\scriptB=\Im(\scriptA-\lambda\id)\big|_{K_\lambda}$ --- это всё кроме верхних блоков каждой башни. При этом эти самые блоки нам и нужны для построения базиса. Также мы знаем, что $K_1$ --- нижняя строка, $K_2$ --- нижние две строки и так далее. А значит мы можем взять $\scriptB K;K_1;K_2;\ldots;K_m$. Мы можем взять базисы всего этого и <<прополоть>>. Так мы выбросим базис $\scriptB K$. Останется базис $\overline{K_1};\overline{K_2};\ldots;\overline{K_m}$. А тогда мы берём базисный вектор $v\in\overline{K_j}$, берём $\scriptB v$, $\scriptB^2v$, ..., $\scriptB^{j-1}v$, $\scriptB^jv$. И рано или поздно, на каком-то $j$-том шаге это обнулится. Тогда $j$ --- это высота <<башни Жордана>>.
        \end{Comment}
        \begin{Example}
            $$
            \matr{
                5 & 1 & 3 & 1\\
                6 & 1 & 4 & 0\\
                -8 & -3 & -4 & -3\\
                -10 & -1 & -7 & 0
            }
            $$
            Тогда $\varphi(t)=(t+1)(t-1)^2$. Для $\lambda=1$ $K_1=V=\Lin\left\{\matr{0\\-1\\0\\1}\right\}$, $K_2=\Lin\left\{\matr{-1\\1\\1\\0};\matr{0\\-1\\0\\1}\right\}$, $K_3=K=\Lin\left\{\matr{0\\0\\-1\\0};\matr{-1\\1\\1\\0};\matr{0\\-1\\0\\1}\right\}$, $\scriptB K=\Lin\left\{\matr{-1\\-2\\1\\9};\matr{-1\\-1\\1\\2};\matr{0\\-1\\0\\1}\right\}$. А значит прополов это, получим $\Lin\left\{\matr{-1\\-2\\1\\9};\matr{-1\\-1\\1\\2};\matr{0\\-1\\0\\1};\matr{0\\0\\-1\\1}\right\}$. Первые три вектора --- из $\scriptB K$, а значит наше $v=\matr{0\\0\\-1\\1}$. В таком случае $\scriptB v=\matr{-2\\-4\\2\\6}$, $\scriptB^2v=\matr{0\\-4\\0\\4}$, $\scriptB^3v=\matr{0\\0\\0\\0}$, а значит наш циклический базис --- $v;\scriptB v;\scriptB^2v$.
        \end{Example}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Функция от матрицы в жордановой форме}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Напоминание: если $f(x)=\sum\limits_{i=0}^\infty c_ix^i$ c радиусом сходимости $|x|<R$, то $f(At)=\sum\limits_{i=0}^\infty c_iA^it^i$.\\
            Подставим туда жорданову форму $J$. $f(At)=\sum\limits_{i=0}^\infty c_i(TJT^{-1})^it^i=\sum\limits_{i=0}^\infty c_iTJ^iT^{-1}t^i=T\left(\sum\limits_{i=0}^\infty c_iJ^it^i\right)T^{-1}=Tf(Jt)T^{-1}$. То есть нам надо научится применять функцию к жордановой форме матриц. Также понятно, что блочно-диагональная матрица возводится в степень очень легко: $\matr{J_1 & \mathbb0\\\mathbb0 & J_2}^i=\matr{J_1^i & \mathbb0\\\mathbb0 & J_2^i}$. То есть по сути нам нужно научиться только одну клетку в степень возводить. Назовём клетку буквой $\tau$. Заметим что $\tau=\lambda E+I_k$, где $I_k=\matr{
                0 & 1 & 0 & 0\\
                0 & 0 & 1 & 0\\
                0 & 0 & 0 & 1\\
                0 & 0 & 0 & 0\
            }$. Что будет, если мы возведём $I_k$ в степень $k$? А всё просто, диагональ из единиц с каждым умножением на себя уезжает вверх. А значит $I_k^k=\mathbb0$. Итого $\tau^m=(I_k+\lambda E)^m=\sum\limits_{i=0}^m\Cnk miI_k^i\lambda^{m-i}$. Несложно заметить, что мы получим треугольную матрицу такого вида:
            $$
            \matr{
                \Cnk m0\lambda^m & \Cnk m1\lambda^{m-1} & \Cnk m2\lambda^{m-1} & \cdots & \Cnk m{k-1}\lambda^{m-k+1}\\
                0 & \Cnk m0\lambda^m & \Cnk m1\lambda^{m-1} & \cdots & \Cnk m{k-2}\lambda^{m-k+2}\\
                0 & 0 & \Cnk m0\lambda^m & \cdots & \Cnk m{k-3}\lambda^{m-k+3}\\
                \vdots & \vdots & \vdots & \ddots & \vdots\\
                0 & 0 & 0 & \cdots & \Cnk m0\lambda^m
            }
            $$
            Как мы знаем, $\Cnk mi=\frac{m!}{i!(m-i)!}=\frac{m(m-1)\cdots(m-i+1)}{i!}$. Тогда
            $$
            f(t\tau)=\sum\limits_{m=0}^\infty c_mt^m\tau^m=
            $$
        \end{Comment}
    \end{itemize}
            \hfsetfillcolor{black}
            \hfsetbordercolor{black}
            \[
            \begin{split}
                \color{white}
                \tikzmarkin{b}(0,-2)(0,2.5)
                \matr{
                    \sum\limits_{m=0}^\infty\!\!\! c_m(t\tau)^m & \frac t{1!}\!\!\!\sum\limits_{m=1}^\infty\!\!\! c_mm(t\tau)^{m-1} & \frac{t^2}{2!}\!\!\!\sum\limits_{m=2}^\infty\!\!\! c_mm(m-1)(t\tau)^{m-2} & \cdots & \frac{t^{k-1}}{(k-1)!}\!\!\!\sum\limits_{m=k}^\infty\!\!\! c_mm(m-1)\cdots(m-k+1)(t\tau)^{m-k+1}\\
                    0 & \sum\limits_{m=0}^\infty\!\!\! c_m(t\tau)^m & \frac t{1!}\!\!\!\sum\limits_{m=1}^\infty\!\!\! c_mm(t\tau)^{m-1} & \cdots & \frac{t^{k-2}}{(k-2)!}\!\!\!\!\!\sum\limits_{m=k-1}^\infty\!\!\!\!\! c_mm(m-1)\cdots(m-k+2)(t\tau)^{m-k+2}\\
                    0 & 0 & \sum\limits_{m=0}^\infty\!\!\! c_m(t\tau)^m & \cdots & \frac{t^{k-3}}{(k-3)!}\!\!\!\!\!\sum\limits_{m=k-2}^\infty\!\!\!\!\! c_mm(m-1)\cdots(m-k+3)(t\tau)^{m-k+3}\\
                    \vdots & \vdots & \vdots & \ddots & \vdots\\
                    0 & 0 & 0 & \cdots & \sum\limits_{m=0}^\infty\!\!\! c_m(t\tau)^m
                }
                \tikzmarkend{b}
            \end{split}
            \]
    \begin{itemize}
        \item[]\begin{Comment}
            А у степенного ряда есть свойство, что в радиусе сходимости можно любое количество раз дифференцировать и интегрировать. А мы, собственно, производные тут и получили. Посмотрите просто: $f'(x)=\sum\limits c_mmx^{m-1}$, $f''(x)=\sum\limits c_mm(m-1)x^{m-2}$ и так далее. То есть
            $$
            f(t\tau)=\matr{
                f(t\lambda) & \frac t{1!}f'(t\lambda) & \frac{t^2}{2!}f''(t\lambda) & \cdots & \frac{t^{k-1}}{(k-1)!}f^{(k-1)}(t\lambda)\\
                0 & f(t\lambda) & \frac t{1!}f'(t\lambda) & \cdots & \frac{t^{k-2}}{(k-2)!}f^{(k-2)}(t\lambda)\\
                0 & 0 & f(t\lambda) & \cdots & \frac{t^{k-3}}{(k-3)!}f^{(k-3)}(t\lambda)\\
                \vdots & \vdots & \vdots & \ddots & \vdots\\
                0 & 0 & 0 & \cdots & f(t\lambda)
            }
            $$
        \end{Comment}
        \begin{Example}
            Если $\tau=\matr{
                \lambda & 1 & 0 & 0\\
                0 & \lambda & 1 & 0\\
                0 & 0 & \lambda & 1\\
                0 & 0 & 0 & \lambda
            }$, а $f(x)=\sin x$, тогда $f'(x)=\cos x$, $f''(x)=-\sin x$, $f'''(x)=-\cos x$. Итого
            $$
            f(\tau t)=\matr{
                \sin(t\lambda) & t\cos(t\lambda) & -\frac{t^2}2\sin(t\lambda) & -\frac{t^3}6\cos(t\lambda)\\
                0 & \sin(t\lambda) & t\cos(t\lambda) & -\frac{t^2}2\sin(t\lambda)\\
                0 & 0 & \sin(t\lambda) & t\cos(t\lambda)\\
                0 & 0 & 0 & \sin(t\lambda)
            }
            $$
        \end{Example}
    \end{itemize}
    \section{Тензоры.}
    \paragraph{\undercolorblack{orange}{Линейные формы (функционалы)}. \undercolorblack{orange}{Сопряжённое (дуальное) пространство}.\\\undercolorblack{orange}{Ковариантные и контрвариантные преобразования}.}
    \begin{itemize}
        \dfn \undercolor{red}{Линейной формой} (\undercolor{red}{функционалом}) на пространстве $V$ над $\scriptK$ называется линейная по функция $f\colon V\to\scriptK$. То есть $\forall\lambda\in\scriptK~\forall v_1;v_2\in V~f(\lambda v_1+v_2)=\lambda f(v_1)+f(v_2)$.
        \begin{Example}
            Например, $V=C(\mathbb R)$, $\delta(f)=f(0)$. Довольно известная функция в теории обобщённых функций, которая называется дельта-функцией Дирака. Понятно, что она линейна и действует из $C(\mathbb R)$ в $\mathbb R$, а значит она правда функционал.
        \end{Example}
        \begin{Example}
            Зафиксируем вектор $\vec b$ в трёхмерном пространстве. Тогда $f(\vec a)=\proj_{\vec b}\vec a$. Это, как мы знаем, равно $\frac{\dotprod{\vec a}{\vec b}}{|\vec b|}$. И эта форма линейна по свойствам скалярного произведения.
        \end{Example}
        \begin{Example}
            На $\scriptK^{n\times n}$ есть $\tr$ --- след матрицы, что, очевидно, также линейный функционал.
        \end{Example}
        \begin{Example}
            На $\scriptP_n$ (пространстве многочленов степени не выше $n$) можно зафиксировать точку $t_0$ и целое число $m$, и тогда функция $f(p)=\frac{p^{(m)}}{m!}(t_0)$. То есть это производная $m$-того порядка в точке $t_0$, что, очевидно, также линейный оператор по свойству дифференцирования.
        \end{Example}
        \dfn Множество всех функционалов над $V$ обозначают $V^*$. Определим \undercolor{red}{сложение функционалов} $+\colon V^*\times V^*\to V^*$ и \undercolor{red}{умножение функционала на константу} $\cdot\colon\scriptK\times V^*\to V^*$ так что $(\lambda f+g)(x)=\lambda f(x)+g(x)$.
        \thm Очевидно, $(V^*;+;\cdot)$ --- линейное подпространство над $\scriptK$.
        \dfn $V^*$ называют \undercolor{red}{сопряжённым} или \undercolor{red}{дуальным} к $V$.
        \begin{Comment}
            Дальше у нас везде пространство будет конечномерное, и $\dim V=n$. А ещё вспоминаем правило Эйнштейна.
        \end{Comment}
        \dfn Пусть $e_1\cdots e_n$ --- базис $V$. Тогда, как мы знаем $x=\mathrm x^je_j$, где $\matr{\mathrm x^1\\\mathrm x^2\\\vdots\\\mathrm x^n}$ --- разложение $x$ по базису. Пусть $f\in V^*$. Как несложно заметить, $f(x)=f(\mathrm x^je_j)=\mathrm x^jf(e_j)=\mathrm x^ja_j$. Тогда $a_j$ называют \undercolor{red}{коэффициентами линейной формы} $f$ \undercolor{red}{относительно базиса} $e$.
        \thm Форма и её коэффициенты $a=\matr{a_1 & a_2 & \cdots & a_n}\in\scriptK_n$ сопоставлены взаимооднозначно.
        \begin{Comment}
            Почему тут написано $\scriptK_n$. А потому что в этой главе мы будем считать $\scriptK^n$ --- столбцы, $\scriptK_n$ --- строки.
        \end{Comment}
        \thm Соответствие линейной формы и её коэффициентов линейно.
        \begin{Proof}
            Тривиально.
        \end{Proof}
        \thm $V^*\cong\scriptK_n$.
        \begin{Proof}
            Из предыдущего.
        \end{Proof}
        \begin{Comment}
            Ст\'{о}ит заметить, что это не координатный изоморфизм. Пока что. Потому что мы пока не доказали, что $a_j$ --- координаты $f$.
        \end{Comment}
        \begin{Comment}
            Также этот изоморфизм зависит от выбора базиса.
        \end{Comment}
        \dfn Пусть в $V$ зафиксирован базис $e$. Тогда \undercolor{red}{координатными функциями относительно базиса} $e$ называются функции $\omega^i\colon V\to\scriptK$, которые вектору $v$ сопоставляют $i$-тую координату вектора $v$ в базисе $e$.
        \thm $\omega^i(e_j)=\delta^i_j$ (символ Кронекера).
        \begin{Proof}
            Единственное что нужно вспомнить, что такое $\delta_j^i$. А это $\begin{cases}
                1 & i=j\\
                0 & i\neq j
            \end{cases}$. А так тривиально.
        \end{Proof}
        \thm $\omega^i$ являются линейными формами.
        \begin{Proof}
            Очевидно следует из свойств координат вектора.
        \end{Proof}
        \thm $\omega^i$ --- базис в $V^*$.
        \begin{Proof}
            Всё что нам нужно --- проверить независимость, ведь их уже $n$ штук, а мы как раз уже знаем, что $V^*\cong\scriptK_n$, а значит равны размерности.\\
            Итак, рассмотрим $\alpha_i\omega^i=\mathbb0$. А значит для любого вектора $x\in V$ выполнено $\alpha_i\omega^i(x)=\mathbb0$. Например, это верно для $e_j$. $\alpha_i\omega^i(e_j)=\alpha_i\delta_j^i=\alpha_j$. А это значит только то, что $\alpha_j=0$. Поскольку $j$ было произвольным, нулевая линейная комбинация только тривиальна.
        \end{Proof}
        \thm Тривиальное следствие --- $a_j=f(e_j)$ являются координатами формы $f$ в $V^*$ относительно базиса $\omega$, то есть $f=a_j\omega^j$.
        \begin{Proof}
            Пусть $f\in V^*$. Как мы знаем, $\forall x\in V~f(x)=\mathrm x^ia_j$. При этом $a_j=f(e_j)$. С другой же стороны $\mathrm x^j=\omega^j(x)$. А это как раз и значит, что $f(x)=a_j\omega^j(x)\Rightarrow f=a_j\omega^j$.
        \end{Proof}
        \begin{Comment}
            Координаты линейных форм принято писать в строку ($a=\matr{a_1 & \cdots & a_n}$), потому что это удобнее, когда мы рассматриваем взаимосвязи $V$ и $V^*$.
        \end{Comment}
        \dfn Если $e$ --- базис $V$, то $\omega$ --- \undercolor{red}{дуальный} или \undercolor{red}{сопряжённый} к $e$ \undercolor{red}{базис} $V^*$
        \begin{Comment}
            Встаёт вопрос --- всякий ли базис $V^*$ является сопряжённым для некоторого базиса $V$? Оказывается, что да.
        \end{Comment}
        \thm Если $({\omega'}^1;{\omega'}^2;\ldots;{\omega'}^n)$ --- базис $V^*$, то в $V$ существует базис $(e_1';\ldots;e_n')$, что $\omega'$ сопряжённый $e'$.
        \begin{Proof}
            Рассмотрим некоторый базис $e$ в $V$ и его сопряжённый $\omega$ в $V^*$. Значит $\omega$ и $\omega'$ --- два базиса одного пространства. А значит они связаны матрицей перехода $T_{\omega\to\omega'}$, что $({\omega'}^1;{\omega'}^2;\ldots;{\omega'}^n)=({\omega}^1;{\omega}^2;\ldots;{\omega}^n)T_{\omega\to\omega'}$. То есть столбцы $T_{\omega\to\omega'}$ --- координатные столбцы $\omega'$ в $\omega$. Есть один нюанс --- у нас же координаты $V^*$ в строках. Ну, похуй, транспонируем всё:
            $\matr{{\omega'}^1\\{\omega'}^2\\\vdots\\{\omega'}^n}=T_{\omega\to\omega'}^T\matr{{\omega}^1\\{\omega}^2\\\vdots\\{\omega}^n}$. Матрица $T_{\omega\to\omega'}^T$ называется $S$. Несложно заметить, что $S$ невырожденная, а значит у неё существует обратная. Тогда мы говорим, что $({e'}_1;{e'}_2;\ldots;{e'}_n)=(e_1;e_2;\ldots;e_n)S^{-1}$. То есть $e_j'=(S^{-1})_j^ie_i$. То есть мы берём $j$-тый столбец и он --- координаты $e_j'$ в $e$. То есть $S^{-1}=T_{e\to e'}$. Остаётся только проверить, что $\omega'$ --- координатные функции $e$.\\
            Ну, что ж. Поскольку $\matr{{\omega'}^1\\{\omega'}^2\\\vdots\\{\omega'}^n}=S\matr{{\omega}^1\\{\omega}^2\\\vdots\\{\omega}^n}$, верно что $\mathrm x'_j={\omega'}^j(x)=S_i^j\omega^i(x)=S_i^j\mathrm x^i$. А $\mathrm x=T\mathrm x'$, что значит что $\mathrm x'=S\mathrm x$. Если раскрыть это как умножение матрицы на вектор, получится именно что $\mathrm x'_j=S_i^j\mathrm x^i$.
        \end{Proof}
        \thm Итак, имеем $e$ и $e'$ --- соответственно, старый и новый базисы $V$ с матрицей перехода $T=T_{e\to e'}$ между ними. И имеем $\omega$ и $\omega'$ --- соответственно, старый и новый базисы $V^*$ с матрицей перехода $S=T^{-1}=T_{\omega\to\omega'}^T$ между ними. Тогда $\mathrm x'=T^{-1}x=Sx$, $a'=aT$.
        \begin{Proof}
            Всё это --- подытоживание уже известного, кроме последнего. $f=a_j\omega^j$ и $f=a'_j{\omega'}^j$. Если мы запишем $a$ и $a'$ в привычной нам нотации вектор-столбец (т.е. $a^T$ и ${a'}^T$), то мы получим по сути $a^T=T_{\omega\to\omega'}{a'}^T$. А если транспонировать обе части равенства, получим как раз $a=a'S$ или $a'=aS^{-1}=aT$.
        \end{Proof}
        \begin{Comment}
            Давайте возьмём $f(x)=\mathrm x^ia_i$, подставим сюда $\mathrm x$ и $a$ и ожидаем получить ${\mathrm x'}^ia'_i$.\\
            $$
            \mathrm x^ia_i=t_j^i{\mathrm x'}^ja'_kS_i^k=\underbrace{t_j^is_i^k}_{(ST)_j^k}{\mathrm x'}^ja'_k=\delta_j^k{\mathrm x'}^ja'_k={\mathrm x'}^ja'_j
            $$
        \end{Comment}
        \begin{Comment}
            Как мы знаем, $(e'_1;\ldots;e'_n)=(e_1;\ldots;e_n)T$, а у $\mathrm x$ --- наоборот, $\mathrm x'=T^{-1}x$. А вот координаты при $\omega$ и $\omega'$ (т.е. $a$ и $a'$) зависят друг от друга с матрицей $T$, а не $T^{-1}$.
        \end{Comment}
        \dfn Векторы, координаты которых при смене базиса меняются по закону, \textbf{согласованному} с формулой замены базиса $e\to e'$ (то есть с той же матрицей) называются \undercolor{red}{ковариантными векторами} или \undercolor{red}{ковекторами}.
        \dfn Векторы, координаты которых при смене базиса меняются по закону, \textbf{противоположному} с формулой замены базиса $e\to e'$ (то есть с обратной матрицей) называются \undercolor{red}{контрвариантными векторами} или просто \undercolor{red}{векторами}.
        \dfn $V^{**}$ --- \undercolor{red}{дважды сопряжённое пространство} --- это $({V^*})^*$.
        \thm $V^{**}\cong V$.
        \begin{Comment}
            Тут мы хотим привести изоморфизм между $V^{**}$ и $V$, чтобы потом сказать, что не только $V^*$ сопряжено $V$, но и наоборот.
        \end{Comment}
        \begin{Comment}
            Давайте зафиксируем $x\in V$. И посмотрим на отображение $"x"\in V^{**}$ такое что $"x"(f)=f(x)$. Очевидно, оно таковым является. И тут мы сопоставили $x\in V$ и $"x"\in V^{**}$. Но нам хочется сопоставить и обратно, что уже не так очевидно. А ещё нужно доказать линейность.
        \end{Comment}
        \thm $"\cdot"$ --- изоморфизм.
        \begin{Proof}
            Начнём с линейности. Рассмотрим $\lambda x_1+x_2$. Тогда что такое $"\lambda x_1+x_2"$? Это такая функция что $"\lambda x_1+x_2"(f)=f(\lambda x_1+x_2)=\lambda f(x_1)+f(x_2)$. А это уже $\lambda"x_1"+"x_2"$.\\
            Теперь хочется посмотреть на то, что соответствие взаимооднозначное. Достаточно (в силу линейности) проверить для базисов. Пусть $e_1;\ldots;e_n$ --- базис $V$. Тогда хочется доказать, что $"e_1";\ldots;"e_n"$ --- базис $V^{**}$. Что такое $"e_j"$? Это $"e_j"(f)=f(e_j)=a_j$ то есть координата $f$ в $V^*$ относительно $\omega^1;\ldots;\omega^n$. А это в свою очередь значит, что $"e_j"$ --- координатная функция относительно базиса $\omega$. А значит мы однозначно (мы доказали уже взаимооднозначность) сопоставляем базис $e$ и $\omega$, а потом поняли, что точно также сопоставлены $\omega$ и $"e"$ (по той же самой теореме). А значит базисы $e$ и $"e"$ сопоставлены однозначно, а значит $"\cdot"$ --- изоморфизм между $V$ и $V^{**}$.
        \end{Proof}
        \begin{Comment}
            Это объясняет, почему пространства $V$ и $V^*$ (и базисы $e$ и $\omega$) называют дуальными. У нас получилось, что не только $V$ сопряжено $V^*$, но и наоборот.
        \end{Comment}
        \begin{Comment}
            Также $"\cdot"$ --- это не просто изоморфизм, а \textbf{естественный}, то есть для него не нужно фиксировать базис (как нужно было для $\omega$).
        \end{Comment}
        \begin{Comment}
            $V$ и $V^{**}$ отождествляются. Отождествляются до такой степени, что в обозначении $"x"$ не пишут кавычки, а пишут просто $x(f)$. И теперь у нас по сути $V$ и $V^*$ равноправны.
        \end{Comment}
        \begin{Comment}
            Хорошо, как найти сопряжённый базис? $\omega^i(e_j)=\delta_i^j$, а значит $\matr{\omega^1\\\vdots\\\omega^n}\matr{e_1 & \cdots & e_n}=\matr{
                \omega^1(e_1) & \cdots & \omega^1(e_n)\\
                \vdots & \ddots & \vdots\\
                \omega^n(e_1) & \cdots & \omega^n(e_n)
            }=E$. То есть всё что нам нужно --- найти $\matr{e_1 & \cdots & e_n}^{-1}$.
        \end{Comment}
        \begin{Comment}
            Также можно записать, что $f(x)=\mathrm x^ja_j=\matr{a_1 & \cdots & a_n}\matr{\mathrm x^1\\\vdots\\\mathrm x^n}$. Вау, зумеры изобрели скалярное произведение.
        \end{Comment}
        \begin{Example}
            Пусть $V=\bigoplus_\lambda V_\lambda$ (не обязательно это собственные подпространства какого-то оператора, но могут быть и они). И нам нужно построить проекторы $\scriptP_\lambda\colon V\to V_\lambda$. Пусть $V=\Lin\{v_1;\ldots;v_n\}$ --- состоит из базисов $V_\lambda$. Тогда единственным образом любое $x$ раскладывается как $\sum\limits_\lambda x_\lambda=\mathrm x^iv_i=\omega^i(x)v_i$. А $\scriptP_\lambda x=x_\lambda=\sum\limits_{v_k\in V_\lambda}\mathrm x^kv_k=\sum\limits_{v_k\in V_\lambda}\omega^k(x)v_k=\left(\sum\limits_{v_k\in V_\lambda}v_ka^k\right)x$. А вот и всё, собственно, $v_ka^k=\scriptP_\lambda$.
        \end{Example}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Два определения тензора}. \undercolorblack{orange}{Многомерная матрица}. \undercolorblack{orange}{Линейное пространство тензоров}.}
    \begin{itemize}
        \dfn Пусть $V$ и $V^*$ --- сопряжённые пространства над $\scriptK$. Тогда тензором типа $(p;q)$ (т.е. $p$ раз ковариантный и $q$ раз контрвариантный) называется полилинейная функция $\colon V^p\times(V^*)^q\to\scriptK$. При этом $p$ и $q$ называют \undercolor{red}{валентностями} тензора, $p+q$ --- \undercolor{red}{рангом} тензора.
        \begin{Comment}
            В литературе могут обозначать $(q;p)$. Это не меняет ковариантность и контрвариантность, но меняет порядок аргументов.
        \end{Comment}
        \dfn Тензор типа $(p;0)$ называют \undercolor{red}{ковариантным тензором валентности $p$}, а тензор типа $(0;q)$ ---\\\undercolor{red}{контрвариантным тензором валентности $q$}. В противном случае тензор называют \undercolor{red}{тензором}\\\undercolor{red}{смешанного типа}. Тензор типа $(0;0)$ --- это не что-то непонятное, а скаляр из $\scriptK$.
        \dfn \undercolor{red}{Сумма тензоров} одного и того же типа --- это сумма их как функций, \undercolor{red}{умножение тензора на скаляр} --- умножение его на скаляр как функции.
        \dfn Нулевой тензор --- $\mathbb0\colon(\xi_1;\ldots;\xi_p;\eta_1;\ldots;\eta_q)\mapsto0$.
        \thm Множество тензоров одинакового типа является линейным пространством. Обозначается оно как $T_{pq}$.
        \begin{Proof}
            Тривиально.
        \end{Proof}
        \dfn Зафиксируем $e$ --- базис $V$ и сопряжённый ему $\omega$. Как мы знаем, $\xi=\xi^ie_i$, $\eta=\eta_j\omega^j$. Тогда что будет, если мы подставим $\xi$ и $\eta$ в тензор $f\in T_{pq}$? $f(\xi_1;\ldots;\xi_p;\eta^1;\ldots;\eta^q)$? Сначала мы вынесем все скаляры, потом --- по всем $\eta$, а потом будет базис. Итого:
        $$
        f(\xi_1;\ldots;\xi_p;\eta^1;\ldots;\eta^q)=\xi_1^{i_1}\xi_2^{i_2}\cdots\xi_p^{i_p}\eta^1_{j_1}\eta^2_{j_2}\cdots\eta^q_{j_q}f(e_{i_1};e_{i_2};\ldots;e_{i_p};\omega^{j_1};\omega^{j_2};\omega^{j_q})
        $$
        То есть нам нужны все наборы базисных элементов. Это самое значение на наборе обозначается $\alpha_{i_1\cdots i_p}^{j_1\cdots j_q}$. Все такие $\alpha$ (для всех индексов) называются \undercolor{red}{компонентами тензора относительно базисов} $e$ и $\omega$.
        \dfn \undercolor{red}{$s=p+q$-мерной матрицей} порядка $n$ называется множество элементов, занумерованных двумя типами индексов: верхних $j_1;\ldots;j_q$ и нижних $i_1;\ldots;i_p$. При этом все индексы пробегают все значения от 1 до $n$.
        \begin{Comment}
            Несложно заметить, что тут $n^s$ элементов.
        \end{Comment}
        \begin{Example}
            Матрица $A\in\scriptK^{n\times n}$ удовлетворяет этому определению, причём её можно интерпретировать как $A_{ij}$, как $A^j_i$ и как $A^{ij}$.
        \end{Example}
        \begin{Example}
            Что такое трёхмерная матрица? Ну, возьмите кубик со стороной $n$ и поставьте в каждую его ячейку чиселко.
        \end{Example}
        \begin{Comment}
            Несложно заметить, что тензор типа $(p;q)$ и матрица размерности $p+q$ --- это одно и то же. Но в линейной алгебре не любят слова <<одно и то же>>, потому что всё же природа этих объектов немного разная, поэтому говорят <<изоморфны>>. Но до этого более детально ещё дойдём.
        \end{Comment}
        \begin{Comment}
            Соглашение о записи элементов многомерной матрицы либо тензора.\\
            Нужно понять, какой индекс считать первым, какой --- вторым и т.д. Наверху у нас индексы $i_1$, $i_2$, ..., $i_q$, а снизу --- $j_1$, $j_2$, ..., $j_p$. При этом мы считаем, что $i$ --- это с первого до $q$-того, а $j$ --- с $q+1$-го до $q+p$-го. То есть сначала идут верхние, потом --- нижние.\\
            В случае $p+q=2$, мы всегда считаем, что первый индекс всегда является строкой, а второй --- столбцом, вне зависимости от того, как записана двумерная матрица: как $\alpha_{ij}$, как $\alpha^i_j$ или как $\alpha^{ij}$. Во всех трёх записях $i$ --- строка, $j$ --- столбец.\\
            В случае $p+q=3$ добавляется новая координата --- <<слой>>. Тут мы считаем, что это всегда первый слой, поэтому трёхмерная матрица может быть записана как $\alpha_{ijk}$, $\alpha^i_{jk}$, $\alpha^{ij}_k$ или $\alpha^{ijk}$. Слои при этом пишутся в строку вот так:
            $$
            \arr{ccc|ccc|ccc}{
                \alpha^1_{11} & \alpha^1_{21} & \alpha^1_{31} & \alpha^1_{12} & \alpha^1_{22} & \alpha^1_{32} & \alpha^1_{13} & \alpha^1_{23} & \alpha^1_{33}\\
                \alpha^2_{11} & \alpha^2_{21} & \alpha^2_{31} & \alpha^2_{12} & \alpha^2_{22} & \alpha^2_{32} & \alpha^2_{13} & \alpha^2_{23} & \alpha^2_{33}\\
                \alpha^3_{11} & \alpha^3_{21} & \alpha^3_{31} & \alpha^3_{12} & \alpha^3_{22} & \alpha^3_{32} & \alpha^3_{13} & \alpha^3_{23} & \alpha^3_{33}\\
            }
            $$
            В случае $p+q=4$ правила на строку, столбец и слой такие же, а четвёртый индекс --- всегда <<сечение>>. Четвёртый индекс пишут как $m$. Тут идея в том же самом, и обозначение похожее. Но есть один нюанс. Теперь слой --- это не горизонталь, а вертикаль:
            $$
            \arr{cc|cc}{
                \alpha^{11}_{11} & \alpha^{12}_{11} & \alpha^{11}_{12} & \alpha^{12}_{12}\\
                \alpha^{21}_{11} & \alpha^{22}_{11} & \alpha^{21}_{12} & \alpha^{22}_{12}\\
                \hline
                \alpha^{11}_{21} & \alpha^{12}_{21} & \alpha^{11}_{22} & \alpha^{12}_{22}\\
                \alpha^{21}_{21} & \alpha^{22}_{21} & \alpha^{21}_{22} & \alpha^{22}_{22}
            }
            $$
            Для б\'{о}льших $p+q$ жить очень сложно, и даже стандартного способа записи не существует.
        \end{Comment}
        \thm \undercolor{darkgreen}{Смена базиса}. Мы имели $$
        f(\xi_1;\ldots;\xi_p;\eta^1;\ldots;\eta^q)=\xi_1^{j_1}\xi_2^{j_2}\cdots\xi_p^{j_p}\eta^1_{i_1}\eta^2_{i_2}\cdots\eta^q_{i_q}\alpha_{j_1\cdots j_p}^{i_1\cdots i_q}
        $$
        После этого мы имеем новый базис $e_1';e_2';\ldots;e_n'$ и сразу получаем ${\omega^1}';{\omega^2}';\ldots;{\omega^n}'$. Пусть $T=T_{e\to e'}$, $S=T^{-1}=T_{\omega\to\omega'}^T$. Тогда что имеем? Тогда имеем $\mathrm x=T\mathrm x'$ или в записи Эйнштейна $\xi^i=t_k^i{\xi^k}'$. Также имеем $a=a'S$ или в записи Эйнштейна $\eta_j=\eta'_ms_j^m$. Что будет, когда мы подставим это в тензор? А вот что
        \[
        f(\xi_1;\ldots;\xi_p;\eta^1;\ldots;\eta^q)=t_{k_1}^{j_1}{\xi^{k_1}_1}'t_{k_2}^{j_2}{\xi^{k_2}_2}'\cdots t_{k_p}^{j_p}{\xi^{k_p}_p}'s_{i_1}^{m_1}{\eta^1_{m_1}}'s_{i_2}^{m_2}{\eta^2_{m_2}}'\cdots s_{i_q}^{m_q}{\eta^q_{m_q}}'\alpha_{j_1\cdots j_p}^{i_1\cdots i_q}
        \]
        Но тут можно переставить множители и вынести все очень много суммирований вовне и после этого переставлять множители как хотим.
        \[
        f(\xi_1;\ldots;\xi_p;\eta^1;\ldots;\eta^q)={\xi^{k_1}_1}'{\xi^{k_2}_2}'\cdots {\xi^{k_p}_p}'{\eta^1_{m_1}}'{\eta^2_{m_2}}'\cdots {\eta^q_{m_q}}'\underbrace{t_{k_1}^{j_1}t_{k_2}^{j_2}\cdots t_{k_p}^{j_p}s_{i_1}^{m_1}s_{i_2}^{m_2}\cdots s_{i_q}^{m_q}\alpha_{j_1\cdots j_p}^{i_1\cdots i_q}}_{f(e_{k_1}';e_{k_2}';\ldots;e_{k_p}';{\omega^{m_1}}';{\omega^{m_2}}';\cdots;{\omega^{m_q}}')}
        \]
        Что мы в итоге получили? Ну, на самом деле всё законно получили, что у нас при смене базиса формула выполняется также --- выносим координаты, оставляем значения на базисах. Но тут мы получаем формулу:
        $$
        {\alpha'}^{m_1\cdots m_q}_{k_1\cdots k_p}=\alpha^{i_1\cdots i_q}_{j_1\cdots j_p}t_{k_1}^{j_1}t_{k_2}^{j_2}\cdots t_{k_p}^{j_p}s_{i_1}^{m_1}s_{i_2}^{m_2}\cdots s_{i_q}^{m_q}
        $$
        То есть у нас верхние индексы контрвариантны, а нижние --- ковариантны.
        \begin{Example}
            Что такое линейная форма в данном случае? Это $\alpha_j$ по сути, ведь в преобразовании базиса форма преобразуется ковариантно.
        \end{Example}
        \begin{Comment}
            Это всё было про первое определение тензора. А есть второе, которое выросло из физики. Физикам удобно забывать о том, что это полилинейная функция, а оперировать с тензорами как с многомерными матрицами, которые нужным образом контр- или ковариантны.
        \end{Comment}
        \dfn \undercolor{red}{Геометрический объект} --- это \textit{нечто}, что от базиса никак не зависело, но согласовано с его заменой (то есть при смене базиса получается объект того же типа).
        \begin{Example}
            Например, берём плоскость. Её индифферентно на базис изначально. Но в базисе плоскость можно записать, преобразовать один базис в другой, и получится другое уравнение плоскости.
        \end{Example}
        \dfn \undercolor{red}{Тензором} типа $(p;q)$ называется геометрический объект на линейном пространстве $V$, описываемый $p+q$-мерной матрицей размерности $n=\dim V$, такой что при смене базиса $e\to e'$, $\omega\to\omega'$, элементы этой матрицы пересчитываются по формуле
        $$
        {\alpha'}^{m_1\cdots m_q}_{k_1\cdots k_p}=\alpha^{i_1\cdots i_q}_{j_1\cdots j_p}t_{k_1}^{j_1}t_{k_2}^{j_2}\cdots t_{k_p}^{j_p}s_{i_1}^{m_1}s_{i_2}^{m_2}\cdots s_{i_q}^{m_q}
        $$
        \dfn \undercolor{red}{Суммой тензором} и \undercolor{red}{умножением тензора на скаляр} для тензора называется понятно что.
        \thm И сумма, и умножение тензора на скаляр также дают нам тензор (того же типа) в смысле второго определения.
        \begin{Proof}
            Тривиально. Пример подобного доказательства можно посмотреть чуть ниже. Тут всё то же самое --- берём сумму в новом базисе, расписываем её как слагаемые в новом базисе, записываем матрицы перехода (они будут одинаковы), выносим их за скобки. Аналогично с умножением на скаляр.
        \end{Proof}
        \thm Определения тензоров равносильны.
        \begin{Comment}
            Так-то тут разве что надо пояснить, что имеется ввиду под <<равносильностью>>. Ведь по сути мы получили объекты разного типа: одно --- функции, другое --- матрицы. Как-то это не совсем одно и то же.\\
            Так вот речь тут идёт об изоморфизме. То есть о том, что с многомерными матрицами и с тензорами мы можем делать одни и те же действия и получать одинаковые результаты. Если это ходит как \sout{утка} тензор и \sout{крякает} преобразуется как \sout{утка} тензор, то это \sout{утка} тензор.\\
            А если мы поняли, что речь идёт об изоморфизме, дальше всё ясно. Берём тензор как функцию, у него есть $\alpha^{i_1\cdots i_q}_{j_1\cdots j_p}$, это и есть многомерная матрица. Преобразуется он как надо. Берём тензор как матрицу, эта матрица --- $\alpha^{i_1\cdots i_q}_{j_1\cdots j_p}$ некоего (очевидно, строго одного) тензора, значит можно построить функцию. То что это изоморфизм --- очевидно.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Тензорное произведение}. \undercolorblack{orange}{Базис пространства тензоров}. \undercolorblack{orange}{Свёртка}.}
    \begin{itemize}
        \dfn Пусть $\alpha\in T_{(p_1;q_1)}$, $\beta\in T_{(p_2;q_2)}$. Тогда \undercolor{red}{тензорное произведение} --- это многомерная матрица $\gamma=\alpha\otimes\beta$ такая что $\gamma^{i_1\cdots i_{q_1}m_1\cdots m_{q_2}}_{j_1\cdots j_{p_1}k_1\cdots k_{p_2}}=\alpha^{i_1\cdots i_{q_1}}_{j_1\cdots j_{p_1}}\beta^{m_1\cdots m_{q_2}}_{k_1\cdots k_{p_2}}$.
        \thm Тензорное произведение является тензором типа $(p_1+p_2;q_1+q_2)$ (надо проверить ко- и контрвариантность).
        \begin{Proof}
            Ну, давайте проверять.
            \[
            \begin{split}
                {\gamma'}^{\widetilde{i_1}\cdots\widetilde{i_{q_1}}\widetilde{m_1}\cdots\widetilde{m_{q_2}}}_{\widetilde{j_1}\cdots\widetilde{j_{p_1}}\widetilde{k_1}\cdots\widetilde{k_{p_2}}}={\alpha'}^{\widetilde{i_1}\cdots\widetilde{i_{q_1}}}_{\widetilde{j_1}\cdots\widetilde{j_{p_1}}}{\beta'}^{\widetilde{m_1}\cdots\widetilde{m_{q_2}}}_{\widetilde{k_1}\cdots\widetilde{k_{p_2}}}=\\
                \alpha^{i_1\cdots i_q}_{j_1\cdots j_p}t_{\widetilde{j_1}}^{j_1}t_{\widetilde{j_2}}^{j_2}\cdots t_{\widetilde{j_p}}^{j_p}s_{i_1}^{\widetilde{i_1}}s_{i_2}^{\widetilde{i_2}}\cdots s_{i_q}^{\widetilde{i_q}}\beta^{m_1\cdots m_q}_{k_1\cdots k_p}t_{\widetilde{k_1}}^{k_1}t_{\widetilde{k_2}}^{k_2}\cdots t_{\widetilde{k_p}}^{k_p}s_{m_1}^{\widetilde{m_1}}s_{m_2}^{\widetilde{m_2}}\cdots s_{m_q}^{\widetilde{m_q}}=\\
                t_{\widetilde{j_1}}^{j_1}t_{\widetilde{j_2}}^{j_2}\cdots t_{\widetilde{j_p}}^{j_p}t_{\widetilde{k_1}}^{k_1}t_{\widetilde{k_2}}^{k_2}\cdots t_{\widetilde{k_p}}^{k_p}s_{i_1}^{\widetilde{i_1}}s_{i_2}^{\widetilde{i_2}}\cdots s_{i_q}^{\widetilde{i_q}}s_{m_1}^{\widetilde{m_1}}s_{m_2}^{\widetilde{m_2}}\cdots s_{m_q}^{\widetilde{m_q}}\underbrace{\alpha^{i_1\cdots i_q}_{j_1\cdots j_p}\beta^{m_1\cdots m_q}_{k_1\cdots k_p}}_{\gamma^{i_1\cdots i_{q_1}m_1\cdots m_{q_2}}_{j_1\cdots j_{p_1}k_1\cdots k_{p_2}}}
            \end{split}
            \]
            Это ли не то что нам надо.
        \end{Proof}
        \thm Если $\alpha$ --- тензор типа $(0;0)$ (то есть скаляр), то тензорное произведение его с чем-то --- это умножение того самого чего-то на этот самый скаляр.
        \thm Тензорное произведение ассоциативно, но не коммутативно.
        \begin{Proof}
            Боже упаси вас руками проверять ассоциативность. Но она выполняется, это на самом деле хорошо явно видно.
        \end{Proof}
        \begin{Example}
            А с коммутативностью есть пример. Пусть $\alpha,\beta\in T_{(1;0)}$, $\alpha=\matr{-1 & 2 & 3}$, $\beta=\matr{0 & 1 & -2}$. Тогда $\gamma=\alpha\otimes\beta=\matr{0 & -1 & 2\\0 & 2 & -4\\0 & 3 & -6}$. А $\delta=\beta\otimes\alpha=\matr{0 & 0 & 0\\-1 & 2 & 3\\2 & -4 & -6}$. Тут мы получили транспонированную матрицу (у нас просто индексы поменялись местами), но в многомерном случае транспонирование бывает намного более сложным, там непонятно, как менять индексы.
        \end{Example}
        \thm Если $\alpha\in T_{(p;0)}$, $\beta\in T_{(0;q)}$, то $\alpha\otimes\beta=\beta\otimes\alpha$.
        \begin{Proof}
            Домашнее задание читателю.
        \end{Proof}
        \begin{Comment}
            Теперь давайте поймём, как умножение выглядит для линейных форм. Как мы знаем, $\alpha\longleftrightarrow f\colon V^{p_1}\times(V^*)^{q_1}\to\scriptK$
            $\beta\longleftrightarrow g\colon V^{p_2}\times(V^*)^{q_2}\to\scriptK$. Тогда
            $\gamma\longleftrightarrow h\colon V^{p_1+p_2}\times(V^*)^{q_1+q_2}\to\scriptK$.
            \[
            \begin{split}
                h(\xi_1;\ldots;\xi_{p_1};\zeta_1;\ldots;\zeta_{p_2};\eta^1;\ldots;\eta^{q_1};\theta^1;\ldots;\theta^{q_2})=\\
                \gamma^{i_1\cdots i_{q_1}m_1\cdots m_{q_2}}_{j_1\cdots j_{p_1}k_1\cdots k_{p_2}} \xi_1^{j_1};\ldots;\xi_{p_1}^{j_{p_1}};\zeta_1^{k_1};\ldots;\zeta_{p_2}^{k_{p_2}};\eta_{i_1}^1;\ldots;\eta_{i_{q_1}}^{q_1}\theta_{m_1}^1;\ldots;\theta_{m_{q_2}}^{q_2}=\\
                \alpha^{i_1\cdots i_{q_1}}_{j_1\cdots j_{p_1}}\beta^{m_1\cdots m_{q_2}}_{k_1\cdots k_{p_2}} \xi_1^{j_1};\ldots;\xi_{p_1}^{j_{p_1}};\zeta_1^{k_1};\ldots;\zeta_{p_2}^{k_{p_2}};\eta_{i_1}^1;\ldots;\eta_{i_{q_1}}^{q_1}\theta_{m_1}^1;\ldots;\theta_{m_{q_2}}^{q_2}=\\
                f(\xi_1;\ldots;\xi_{p_1};\eta^1;\ldots;\eta^{q_1})g(\zeta_1;\ldots;\zeta_{p_2};\theta^1;\ldots;\theta^{q_2})
            \end{split}
            \]
        \end{Comment}
        \begin{Comment}
            Давайте теперь вот на что посмотрим. Что будет, если мы рассмотрим $f_1;f_2;\ldots;f_p\in T_{(1;0)}$ и возьмём произведение их всех? Будет
            $$
            f(\xi_1;\ldots;\xi_p)=f^1(\xi_1)f^2(\xi_2)\cdots f^p(\xi_p)
            $$
            Аналогично можно взять несколько тензоров $g_1;\ldots;g_q\in T_{(0;1)}$. И более того, можно взять и те, и другие. Тогда получится 
            $$
            f^1(\xi_1)f^2(\xi_2)\cdots f^p(\xi_p)g_1(\eta^1)g_2(\eta^2)\cdots g_q(\eta^q)
            $$
            При этом $p\in V^*$, а $q\in V^{**}\cong V$. А это значит, что совершенно легально мы можем взять конкретные $f$ и $g$:
            $$
            \left(\omega^{i_1}\otimes\omega^{i_2}\otimes\cdots\otimes\omega^{i_p}\otimes e_{j_1}\otimes e_{j_2}\otimes\cdots\otimes e_{j_q}\right)(\xi_1;\ldots;\xi_p;\eta^1;\ldots;\eta^q)=\\
            \xi_1^{i_1}\ldots\xi_p^{i_p}\eta^1_{j_1}\ldots\eta^q_{j_q}
            $$
        \end{Comment}
        \thm Указанные $n^{p+q}$ тензоров являются базисом пространства $T_{(p;q)}$.
        \begin{Proof}
            Докажем, что эта система порождающая. Ну, это тривиально, вообще говоря. Мы знаем, что
            $$
            f(\xi_1;\ldots;\xi_p;\eta^1;\ldots;\eta^q)=\xi_1^{i_1}\xi_2^{i_2}\cdots\xi_p^{i_p}\eta^1_{j_1}\eta^2_{j_2}\cdots\eta^q_{j_q}\alpha_{i_1\cdots i_p}^{j_1\cdots j_q}
            $$
            Есть некоторая корреляция с формулой выше, не находите? Более того, получается, что $\alpha_{i_1\cdots i_p}^{j_1\cdots j_q}$ --- это коэффициенты тензора в нашем базисе.\\
            Вообще, можно не доказывать, что система линейно независима, потому что их уже по количеству столько, сколько надо (мы же уже знаем размерность $T_{(p;q)}$).
        \end{Proof}
        \begin{Comment}
            Давайте теперь посмотрим, как выглядит матрица тензора $\omega^{i_1}\otimes\omega^{i_2}\otimes\cdots\otimes\omega^{i_p}\otimes e_{j_1}\otimes e_{j_2}\otimes\cdots\otimes e_{j_q}$. Ну, давайте подействуем этим тензором на базисный набор $(e_{\widetilde{i_1}};e_{\widetilde{i_2}};\ldots;e_{\widetilde{i_p}};\omega^{\widetilde{j_1}};\omega^{\widetilde{j_2}};\ldots;\omega^{\widetilde{j_q}})$. Ну, мы получим произведение символов Кронекера $\delta_{\widetilde{i_1}}^{i_1}\cdots\delta_{\widetilde{i_p}}^{i_p}\delta^{\widetilde{j_1}}_{j_1}\cdots\delta^{\widetilde{j_q}}_{j_q}$. Оно равно нулю почти всегда, а единице, когда индексы с волной соответственно равны индексам без волны. То есть у матрицы $\omega^{i_1}\otimes\omega^{i_2}\otimes\cdots\otimes\omega^{i_p}\otimes e_{j_1}\otimes e_{j_2}\otimes\cdots\otimes e_{j_q}$ есть только одна единичка и куча нулей. Как и было с матрицами, на самом деле, тоже в базисе матриц единица бегала по всем элементом.\\
            Через то же самое можно было честно доказать линейную независимость в предыдущем доказательстве, если надо.
        \end{Comment}
        \dfn Пусть $p,q\geqslant1$, $\alpha\in T_{(p;q)}$. Тогда  $$
        \beta^{i_1;\ldots;i_{k-1};i_{k+1};\ldots;i_q}_{j_1;\ldots;j_{m-1};j_{m+1};\ldots;j_p}=
        \alpha^{i_1;\ldots;i_{k-1};\kappa;i_{k+1};\ldots;i_q}_{j_1;\ldots;j_{m-1};\kappa;j_{m+1};\ldots;j_p}
        $$ --- тензор, полученный из $\alpha$ \undercolor{red}{свёрткой} по $k$-тому верхнему и $m$-тому нижнему индексам.
        \begin{Example}
            Например, рассмотрим двумерную матрицу $A^i_j$. Тогда свернув его по единственным верхнему и нижнему индексам, получим $\alpha_i^i=\tr A$.
        \end{Example}
        \thm Свёртка является тензором.
        \begin{Proof}
            \[
            \begin{split}
                {\beta'}^{\widetilde{i_1};\ldots;\widetilde{i_{k-1}};\widetilde{i_{k+1}};\ldots;\widetilde{i_q}}_{\widetilde{j_1};\ldots;\widetilde{j_{m-1}};\widetilde{j_{m+1}};\ldots;\widetilde{j_p}}=\\
                {\alpha'}^{\widetilde{i_1};\ldots;\widetilde\kappa;\ldots;\widetilde{i_q}}_{\widetilde{j_1};\ldots;\widetilde\kappa;\ldots;\widetilde{j_p}}=\\
                \alpha^{i_1;\ldots;\kappa_1;\ldots;i_q}_{j_1;\ldots;\kappa_2;\ldots;j_p}t_{\widetilde{j_1}}^{j_1}\cdots t_{\widetilde\kappa}^{\kappa_2}\cdots t_{\widetilde{j_p}}^{j_p}s_{i_1}^{\widetilde{i_1}}\cdots s_{\kappa_1}^{\widetilde\kappa}\cdots s_{i_q}^{\widetilde{i_q}}
            \end{split}
            \]
            Тут можно заметить $t^{\kappa_2}_{\widetilde\kappa}s^{\widetilde\kappa}_{\kappa_1}$. Это $E^{\kappa_2}_{\kappa_1}$, то есть $\delta^{\kappa_2}_{\kappa_1}$. А значит у нас пропадёт всё, где $\kappa_1\neq\kappa_2$. А где $\kappa_1=\kappa_2=\kappa$, мы получаем
            \[
            \begin{split}
                &\alpha^{i_1;\ldots;\kappa;\ldots;i_q}_{j_1;\ldots;\kappa;\ldots;j_p}t_{\widetilde{j_1}}^{j_1}\cdots t_{\widetilde{j_{m-1}}}^{j_{m-1}}t_{\widetilde{j_{m+1}}}^{j_{m+1}}\cdots t_{\widetilde{j_p}}^{j_p}s_{i_1}^{\widetilde{i_1}}\cdots s_{i_{k-1}}^{\widetilde{i_{k-1}}}s_{i_{k+1}}^{\widetilde{i_{k+1}}}\cdots s_{i_q}^{\widetilde{i_q}}=\\
                &\beta^{i_1;\ldots;i_{k-1};i_{k+1};\ldots;i_q}_{j_1;\ldots;j_{m-1};j_{m+1};\ldots;j_p}t_{\widetilde{j_1}}^{j_1}\cdots t_{\widetilde{j_{m-1}}}^{j_{m-1}}t_{\widetilde{j_{m+1}}}^{j_{m+1}}\cdots t_{\widetilde{j_p}}^{j_p}s_{i_1}^{\widetilde{i_1}}\cdots s_{i_{k-1}}^{\widetilde{i_{k-1}}}s_{i_{k+1}}^{\widetilde{i_{k+1}}}\cdots s_{i_q}^{\widetilde{i_q}}
            \end{split}
            \]
        \end{Proof}
        \dfn Свёртка может проводиться сразу по нескольким парам индексов. Если в результате проведения подобной свёртки получается скаляр, получается \undercolor{red}{полная свёртка}.
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Транспонирование тензора}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Хочется как-то обобщить транспонирование матриц.
        \end{Comment}
        \dfn Пусть $\alpha\in T_{(p;q)}$ и $p\geqslant2$. Пусть $\sigma\in S_p$. Тогда \undercolor{red}{транспонированием тензора} $\alpha$ перестановкой $\sigma$ по множеству внешних индексов называется тензор $\beta=\sigma(\alpha)$ такой что $\beta^{i_1\cdots i_q}_{j_1\cdots j_p}=\alpha^{i_1\cdots i_q}_{j_{\sigma_1}\cdots j_{\sigma_p}}$.
        \begin{Comment}
            Транспонирование по верхним индексам определяется аналогично. Аналогично и доказываются все свойства.
        \end{Comment}
        \begin{Comment}
            Транспонирование тензора проводится только по одному типу индексов (либо по верхним, либо по нижним), в отличие от многомерной матрицы.\\
            В частности, транспонирование двумерной матрицы, если она записана как $\alpha^i_j$, не определено.
        \end{Comment}
        \begin{Comment}
            Любая перестановка, как мы знаем, --- конченое число транспозиций. А значит любое транспонирование тензора можно представить как конечное число транспонирований транспозициями.
        \end{Comment}
        \begin{Comment}
            Опять возникает вопрос, является ли эта штука тензором. Нам надо показать преобразование координат. Но учитывая комментарий выше, достаточно проверить корректность только для транспозиции, а не для произвольной перестановки.
        \end{Comment}
        \thm Определение корректно.
        \begin{Proof}
            Пусть мы меняем местами $j_k$ и $j_m$. То есть
            $$
            \beta^{i_1\cdots i_q}_{\cdots j_m\cdots j_k\cdots}=\alpha^{i_1\cdots i_q}_{\cdots j_k\cdots j_m\cdots}
            $$
            Проверим преобразование координат.
            \[
            {\beta'}^{\widetilde{i_1}\cdots\widetilde{i_q}}_{\cdots\widetilde{j_m}\cdots\widetilde{j_k}\cdots}={\alpha'}^{\widetilde{i_1}\cdots\widetilde{i_q}}_{\cdots\widetilde{j_k}\cdots\widetilde{j_m}\cdots}=
            {\alpha}^{i_1\cdots i_q}_{\cdots j_k\cdots j_m\cdots}t^{j_1}_{\widetilde{j_1}}\cdots t^{j_p}_{\widetilde{j_p}}s_{i_1}^{\widetilde{i_1}}\cdots s_{i_q}^{\widetilde{i_q}}={\beta}^{i_1\cdots i_q}_{\cdots j_m\cdots j_k\cdots}t^{j_1}_{\widetilde{j_1}}\cdots t^{j_p}_{\widetilde{j_p}}s_{i_1}^{\widetilde{i_1}}\cdots s_{i_q}^{\widetilde{i_q}}
            \]
        \end{Proof}
        \begin{Proof}
            Ну, хорошо, а что происходит с тензором как с функцией?\\
            \[
            \begin{split}
                \beta(\xi_1;\ldots;\xi_p;\eta^1;\ldots;\eta^q)=\beta^{i_1\cdots i_q}_{j_1\cdots j_p}&\xi_1^{j_1}\cdots\xi_p^{j_p}\eta^1_{i_1}\cdots\eta^q_{i_q}=\\
                =\alpha^{i_1\cdots i_q}_{j_{\sigma_1}\cdots j_{\sigma_p}}&\xi_{\sigma_1}^{j_{\sigma_1}}\cdots\xi_{\sigma_p}^{j_{\sigma_p}}\eta^1_{i_1}\cdots\eta^q_{i_q}=\alpha(\xi_{\sigma_1};\ldots;\xi_{\sigma_q};\eta^1;\ldots;\eta^q)
            \end{split}
            \]
            То есть мы меняем аргументы функции местами.
        \end{Proof}
        \thm Свойства транспонирования.
        \begin{enumerate}
            \item Транспонирование --- линейная операция.
            \item Транспонирование заданной перестановкой --- взаимооднозначное соответствие.
            \item Транспонирование --- изоморфизм.
            \item Транспонирование тензора ассоциативно, но не коммутативно.
        \end{enumerate}
        \begin{Proof}
            Очевидно.
        \end{Proof}
        \thm Упражнение читателю: существует такая перестановка $\sigma$ что $\alpha\otimes\beta=\sigma(\beta\otimes\alpha)$ (если слева и справа тензоры одного типа).
        \dfn Тензор $\alpha\in T_{(p;q)}$ называется \undercolor{red}{симметричным}, если $\forall\sigma\in S_p~\sigma(\alpha)=\alpha$.
        \dfn Тензор $\alpha\in T_{(p;q)}$ называется \undercolor{red}{кососимметричным}, если $\forall\sigma\in S_p~\sigma(\alpha)=(-1)^{\varepsilon(\sigma)}\alpha$.
        \begin{Comment}
            Посмотрим на тензор. Он симметричный тогда и только тогда, когда не меняется при любой перестановке. А это в свою очередь верно тогда и только тогда, когда он не меняется при транспозициях. Или, если посмотреть на функцию, при смене любых двух $\xi_k$ и $\xi_m$ значение функции не меняется.\\
            Если же рассмотреть то же самое с кососимметричностью, то получится, что при транспозиции функция меняет знак.
        \end{Comment}
        \thm Тензор является кососимметричным тогда и только тогда, когда он равен нулю при повторяющихся аргументах.
        \begin{Proof}
            См. доказательство для определителя.
        \end{Proof}
        \thm Сумма симметричных тензоров симметрична, сумма кососимметричных кососимметрична.
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Операции симметрирования и альтерирования}.}
    \begin{itemize}
        \dfn \undercolor{red}{Альтернированием} (или \undercolor{red}{антисимметризацией}) тензора $\alpha\in T_{(p;q)}$ называется тензор $\Alt\alpha=\frac1{p!}\sum\limits_{\sigma\in S_p}(-1)^{\varepsilon(\sigma)}\sigma(\alpha)$.
        \dfn \undercolor{red}{Симметрированием} тензора $\alpha\in T_{(p;q)}$ называется тензор $\Sim\alpha=\frac1{p!}\sum\limits_{\sigma\in S_p}\sigma(\alpha)$.
        \begin{Comment}
            В результате мы получаем тензор из $T_{(p;q)}$ по той простой причине, что $\sigma(\alpha)\in T_{(p;q)}$. Во-вторых, понятно, что это линейная операция, так как $\sigma(\alpha)$ линейна. 
        \end{Comment}
        \begin{Comment}
            Тут мы альтернировали и симметрировали по нижним индексам, а можно аналогично для верхних.
        \end{Comment}
        \begin{Comment}
            Если $\alpha$ кососимметричен, то $\Alt\alpha=\alpha$, потому что в таком случае $\sigma(\alpha)=(-1)^{\varepsilon(\sigma)}\alpha$. Аналогично если $\alpha$ симметричен, то $\Sim\alpha=\alpha$.
        \end{Comment}
        \begin{Comment}
            Альтернирование и симметрирование может производиться не по всем индексам, а по какому-то подмножеству. В этих случаях индексы, по которым эта операция происходит, берут с скобки. В таком случае индексы, по которым производится операция, берут в скобки (для альтернирования в квадратные, для симметрирования --- в круглые). Если внутри скобок есть индексы, по которым операция не производится, их ограничивают вертикальными чертами.
        \end{Comment}
        \begin{Example}
            \begin{itemize}
                \item $\alpha_{[j_1;j_2;j_3;j_4;j_5]}$ --- альтернирование по всем.
                \item $\alpha_{[j_1;|j_2|;j_3;|j_4|;j_5]}$ --- альтернирование по $j_1$, $j_3$, $j_5$.
                \item $\alpha_{(j_1;|j_2;j_3;j_4|;j_5)}$ --- симметрирование по $j_1$ и $j_5$.
            \end{itemize}
        \end{Example}
        \begin{Example}
            Пусть $\alpha\in T_{2;q}$. Тогда $\Alt\alpha=\frac12\left(\alpha^{i_1\cdots i_q}_{ij}-\alpha^{i_1\cdots i_q}_{ji}\right)$, а $\Sim\alpha=\frac12\left(\alpha^{i_1\cdots i_q}_{ij}+\alpha^{i_1\cdots i_q}_{ji}\right)$.\\
            Кстати, несложно заметить, что в терминах матриц $\Alt A=\frac12(A-A^T)$, $\Sim A=\frac12(A+A^T)$. И действительно $(\Alt A)^T=-\Alt A$, $(\Sim A)^T=\Sim A$. То есть матрицы действительно кососимметричная и симметричная соответственно.
        \end{Example}
        \begin{Example}
            Теперь рассмотрим $\alpha\in T_{(3;q)}$.
        \end{Example}
        \thm $$\forall\sigma\in S_p~\Alt\sigma(\alpha)=\sigma(\Alt\alpha)=(-1)^{\varepsilon(\sigma)}\Alt\alpha$$
        $$\forall\sigma\in S_p~\Sim\sigma(\alpha)=\sigma(\Sim\alpha)=\Sim\alpha$$
        \begin{Proof}
            Докажем для альтернирования, для симметрирования аналогично.\\
            \[
            \Alt\sigma(\alpha)=\frac1{p!}\sum\limits_{\tau\in S_p}(-1)^{\varepsilon(\tau)}\tau(\sigma(\alpha))
            \]
            Рассмотрим $\tau\sigma$. Тут $\tau$ пробегает все перестановки, и его мы умножаем на конкретную. Поскольку перестановки образуют группу, $\tau\sigma$ также пробегает все перестановки. Так что запишем $\tau\sigma$ как $\rho$.\\
            Ещё было бы неплохо определить, как связаны $\varepsilon(\rho)$ и $\varepsilon(\tau)$. Да ну тривиально, $(-1)^{\varepsilon(\rho)}=(-1)^{\varepsilon(\tau)}(-1)^{\varepsilon(\sigma)}$. А это значит, что
            \[
            \Alt\sigma(\alpha)=\frac1{p!}\sum\limits_{\tau\in S_p}(-1)^{\varepsilon(\tau)}\tau(\sigma(\alpha))=(-1)^{\varepsilon(\sigma)}\frac1{p!}\sum\limits_{\rho\in S_p}(-1)^{\varepsilon(\rho)}\rho(\alpha)=(-1)^{\varepsilon(\sigma)}\Alt\alpha
            \]
            Теперь давайте запишем, чему равно $\sigma(\Alt\alpha)$. Ну,
            \[
            \sigma(\Alt\alpha)=\sigma\left(\frac1{p!}\sum\limits_{\tau\in S_p}(-1)^{\varepsilon(\tau)}\tau(\alpha)\right)\overset{\sigma\text{ линейно}}=\frac1{p!}\sum\limits_{\tau\in S_p}(-1)^{\varepsilon(\tau)}\sigma(\tau(\alpha))
            \]
            Тут также можно перейти к $\rho$, только тут $\rho=\sigma\tau$.\\
        С симметрированием будет то же самое, но без всяких $(-1)^{\varepsilon(\tau)}$.
        \end{Proof}
        \thm Отсюда по определению $\Alt\alpha$ --- кососимметричный тензор, а $\Sim\alpha$ --- симметричный.
        \thm $\alpha$ является кососимметричным тогда и только тогда, когда $\alpha=\Alt\alpha$.\\
        $\alpha$ является симметричным тогда и только тогда, когда $\alpha=\Sim\alpha$.
        \thm $\Alt\Alt\alpha=\Alt\alpha$, $\Sim\Sim\alpha=\Sim\alpha$, $\Alt\Sim\alpha=\Sim\Alt\alpha=\mathbb0$.
        \begin{Proof}
            Первые два следуют из предыдущего. Докажем последнее. Например, для $\Alt\Sim\alpha$.\\
            $$
            \Alt\Sim\alpha=\sum\limits_{\sigma\in S_p}(-1)^{\varepsilon(\sigma)}\sigma(\Sim\alpha)=\sum\limits_{\sigma\in S_p}(-1)^{\varepsilon(\sigma)}\Sim\alpha=\Sim\alpha\sum\limits_{\sigma\in S_p}(-1)^{\varepsilon(\sigma)}
            $$
            Что такое эта сумма? Это $\left|\begin{matrix}
                1 & 1 & \cdots & 1\\
                1 & 1 & \cdots & 1\\
                \vdots & \vdots & \ddots & \vdots\\
                1 & 1 & \cdots & 1
            \end{matrix}\right|=0$
        \end{Proof}
        \thm Рассмотрим $T_{(p;q)}^{\text{симм.}}$ и $T_{(p;q)}^{\text{кососимм.}}$ --- множества тензоров типа $(p;q)$ симметричных и кососимметричных по \textbf{заданным двум индексам} соотвественно. Очевидно, что они оба подпространства. Также $T_{(p;q)}^{\text{симм.}}\oplus T_{(p;q)}^{\text{кососимм.}}=T_{(p;q)}$.
        \begin{Proof}
            Ну, тривиально, эти пространства не пересекаются, а чтобы доказать, что их сумма равна $T_{(p;q)}$ возьмём $\alpha\in T_{(p;q)}$, и тогда $\Sim\alpha\in T_{(p;q)}^{\text{симм.}}$, $\Alt\alpha\in T_{(p;q)}^{\text{кососимм.}}$ и $\Sim\alpha+\Alt\alpha=\alpha$.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{$p$-форма}. \undercolorblack{orange}{Внешнее произведение $p$-форм}.}
    \begin{itemize}
        \dfn Если $f\in T_{(p;0)}$ кососимметричен (т.е. $f$ антисимметричная полилинейная функция $p$ аргументов), то его называют \undercolor{red}{$p$-формой} либо \undercolor{red}{внешней $p$-формой}. Множество $p$-форм обозначают $\Lambda^pV^*$. $\Lambda^1V^*$ считают равным $V^*$.
        \begin{Proof}
            Точно такое же понятие для нижних индексов называют не полилинейной формой, а поливектором. А точнее, $q$-поливектором. Множество всех $q$-поливекторов обозначают $V^qV$.
        \end{Proof}
        \dfn Пусть $f\in\Lambda^{p_1}V^*$, $g\in\Lambda^{p_2}V^*$. \undercolor{red}{Внешнее произведение форм} $f$ и $g$ --- это $f\land g=\frac{(p_1+p_2)!}{p_1!p_2!}\Alt(f\otimes g)$.
        \begin{Comment}
            Очевидно, $f\land g\in\Lambda^{p_1+p_2}V^*$.
        \end{Comment}
        \thm Свойства внешнего произведения:
        \begin{enumerate}
            \item $f\land g=(-1)^{p_1p_2}g\land f$.
            \item $(f+g)\land h=f\land h+g\land h$ и $f\land(g+h)=f\land g+f\land h$.
            \item $\lambda\cdot(f\land g)=(\lambda f)\land g=f\land(\lambda g)$.
            \item $\mathbb0_{\Lambda^{p_1}V^*}\land g=f\land\mathbb0_{\Lambda^{p_2}V^*}=\mathbb0_{\Lambda^{p_1+p_2}V^*}$.
            \item $(f\land g)\land h=f\land(g\land h)=\frac{(p_1+p_2+p_3)!}{p_1!p_2!p_3!}\Alt(f\otimes g\otimes h)$.
        \end{enumerate}
        \begin{Proof}
            Свойства 2, 3 и 4 тривиально следуют из определения и свойств альтернирования.\\
            Остальные докажем. $f\land g=\frac{(p_1+p_2)!}{p_1!p_2!}\Alt(f\otimes g)$. Пусть $f=\alpha_{i_1;\ldots;i_{p_1}}$, $g=\beta_{j_1;\ldots;j_{p_2}}$. Тогда $f\otimes g=\gamma_{i_1;\ldots;i_{p_1};j_1;\ldots;j_{p_2}}=\gamma_{k_1;\ldots;k_{p_1+p_2}}$. Тогда что такое $\Alt\gamma$? Это $\frac1{(p_1+p_2)!}\sum\limits_{\sigma\in S_{p_1+p_2}}(-1)^{\varepsilon(\sigma)}\gamma_{k_{\sigma_1};\ldots;k_{\sigma_{p_1+p_2}}}$. При этом если взять $g\land f$, получим $\frac1{(p_1+p_2)!}\sum\limits_{\sigma\in S_{p_1+p_2}}(-1)^{\varepsilon(\sigma)}h_{l_{\sigma_1};\ldots;l_{\sigma_{p_1+p_2}}}$. Несложно заметить, что слагаемые этих сумм можно разбить на пары $\gamma_{i_1;\ldots;i_{p_1};j_1;\ldots;j_{p_2}}$ и $h_{j_1;\ldots;j_{p_2};i_1;\ldots;i_{p_1}}$. Они равны. Если мы докажем, что коэффициенты при этих слагаемых правильные, то суммы будут равны. Ну, посчитаем чётность перестановок. Давайте возьмём $h_{j_1;\ldots;j_{p_2};i_1;\ldots;i_{p_1}}$ и поместим там индекс $i_1$ на первое место. Это $p_2$ смен чётности. Потом $i_2$ ---  это тоже $p_2$. Итого $p_1$ раз, то есть мы сменим чётность $p_1p_2$ раз, что нам и нужно, чтобы получить $(-1)^{p_1p_2}$.\\
            Теперь пятое. Пусть $f$ --- $p_1$-форма, $g$ --- $p_2$-форма, $g$ --- $p_3$-форма.
            \[
            \begin{split}
                (f\land g)\land h&=\left(\frac{(p_1+p_2)!}{p_1!p_2!}\Alt(f\otimes g)\right)\land h=\\
                &=\frac{(p_1+p_2+p_3)!}{p_1!p_2!p_3!}\Alt(\Alt(f\otimes g)\otimes h)
            \end{split}
            \]
            Нам лишь надо доказать, что $\Alt(\Alt(f\otimes g)\otimes h)=\Alt(f\otimes\Alt(g\otimes h))$.
            \[
            \begin{split}
                \Alt(\Alt(f\otimes g)\otimes h)\overset{\text{линейность }\otimes}=\Alt\left(\frac1{(p_1+p_2)!}\sum\limits_{\sigma\in S_{p_1+p_2}}(-1)^{\varepsilon(\sigma)}\sigma(f\otimes g)\otimes h\right)
            \end{split}
            \]
            Пусть $\tau$ переставляет в $f\otimes g\otimes h$ первые $p_1+p_2$ индексов так, как делаем $\sigma$, а $p_3$ последних не трогает. Тогда несложно заметить, что $\varepsilon(\sigma)=\varepsilon(\tau)$.\\
            По линейности альтернирования выражение выше равно
            \[
            \begin{split}
                &\frac1{(p_1+p_2)!}\sum\limits_{\sigma\in S_{p_1+p_2}}(-1)^{\varepsilon(\sigma)}\Alt(\tau(f\otimes g\otimes h))=\\
                =&\frac1{(p_1+p_2)!}\sum\limits_{\sigma\in S_{p_1+p_2}}(-1)^{\varepsilon(\sigma)}(-1)^{\varepsilon(\tau)}\Alt(f\otimes g\otimes h)=\\
                =&\Alt(f\otimes g\otimes h)\frac1{(p_1+p_2)!}\sum\limits_{\sigma\in S_{p_1+p_2}}1=\Alt(f\otimes g\otimes h)
            \end{split}
            \]
            Несложно заметить, что проведя те же операции с $f\land(g\land h)$, получим то же самое.
        \end{Proof}
        \thm Методом математической индукции тривиально можно доказать, что
        \[
        f^1\land f^2\land\cdots\land f^m=\frac{(p_1+p_2+\cdots+p_m)!}{p_1!p_2!\cdots p_m!}\Alt(f^1\otimes f^2\otimes\cdots f^m)
        \]
        \begin{Example}
            В частности, $\omega^{j_1}\land\cdots\land\omega^{j_p}=p!\Alt(\omega^{j_1}\otimes\cdots\otimes\omega^{j_p})$. При этом если мы возьмём два одинаковых индекса, будет $\mathbb0$.
        \end{Example}
        \thm \undercolor{darkgreen}{Теорема о базисе $\Lambda^pV^*$}. Система из векторов вида $\omega^{j_1}\land\cdots\land\omega^{j_p}$, где $j_1<j_2<\cdots<j_p$ является базисом $\Lambda^pV^*$.
        \begin{Proof}
            Докажем, что система порождающая. Рассмотрим $f\in T_{(p;0)}$, что $\Alt f=f$. По первому, $f$ можно разложить по базису $T_{(p;0)}$, который является тензорными произведениями $\omega$. То есть $f=\alpha_{j_1\ldots j_p}\omega^{j_1}\otimes\cdots\otimes\omega^{j_p}$. Теперь применим второе свойство: $f=\Alt f=\Alt(\alpha_{j_1\ldots j_p}\omega^{j_1}\otimes\cdots\otimes\omega^{j_p})=\frac1{p!}\alpha_{j_1\ldots j_p}\omega^{j_1}\land\cdots\land\omega^{j_p}$. Заметим, что все $\omega^{j_1}\land\cdots\land\omega^{j_p}$ (если индексы не упорядочены) представимы через такой же набор с упорядоченными индексами. То есть система порождающая. Но мы хотим координаты узнать.
            \[\begin{split}
                &\frac1{p!}\alpha_{j_1\ldots j_p}\omega^{j_1}\land\cdots\land\omega^{j_p}=\\
                =&\sum\limits_{j_1<\cdots<j_p}\beta_{j_1\ldots j_p}\omega^{j_1}\land\cdots\land\omega^{j_p}
            \end{split}\]
            Чему равно $\beta_{j_1\ldots j_p}$? Ну,
            $$
            \beta_{j_1\ldots j_p}=\frac1{p!}\sum\limits_{\sigma\in S_p}\alpha_{j_{\sigma_1}\ldots j_{\sigma_p}}(-1)^{\varepsilon(\sigma)}
            $$
            Вопрос: что это? Да это же $\alpha_{[j_1\ldots j_p]}$. Ну, а это, объективно, само $\alpha_{j_1\ldots j_p}$.\\
            Теперь давайте покажем линейную независимость. Возьмём $0=\sum\limits_{j_1<\ldots<j_p}\beta_{j_1\ldots j_p}\omega^{j_1}\land\cdots\land\omega^{j_p}=\alpha_{j_1\ldots j_p}\omega^{j_1}\otimes\cdots\otimes\omega^{j_p}$. А уж извините, $\omega^{j_1}\otimes\cdots\otimes\omega^{j_p}$ --- базисные вектора в $T_{(p;0)}$, если их линейная комбинация --- 0, то все $\alpha_{j_1\ldots j_p}=0$. А значит и все $\beta$ --- тоже.
        \end{Proof}
        \dfn Коэффициенты разложения в базисе $\omega^{j_1}\land\cdots\land\omega^{j_p}$ ($\beta_{j_1\ldots j_p}$ из доказательства выше) называются \undercolor{red}{существенными координатами $p$-формы}.
        \thm $\dim\Lambda^pV^*=\Cnk np$.
        \thm Рассмотрим набор линейных форм $f^1;\ldots;f^p\in\Lambda^1V^*$.\\
        Тогда $\left(f^1\land\ldots\land f^p\right)\left(\xi_1;\ldots;\xi_p\right)=\matrd{f^1(\xi_1)&\cdots&f^1(\xi_p)\\\vdots&\ddots&\vdots\\f^p(\xi_1)&\cdots&f^p(\xi_p)}=\det(f^i(\xi_j))$.
        \begin{Proof}
            \[
            \begin{split}
                \left(f^1\land\ldots\land f^p\right)\left(\xi_1;\ldots;\xi_p\right)&=p!\Alt(f^1\otimes\ldots\otimes f^p)\left(\xi_1;\ldots;\xi_p\right)=\\
                &=\sum\limits_{\sigma\in S_p}(-1)^{\varepsilon(\sigma)}\left(f^{\sigma_1}\otimes\ldots\otimes f^{\sigma_p}\right)\left(\xi_1;\ldots;\xi_p\right)=\\
                &=\sum\limits_{\sigma\in S_p}(-1)^{\varepsilon(\sigma)}f^{\sigma_1}(\xi_1)\cdots f^{\sigma_p}(\xi_p)
            \end{split}
            \]
            Нечто знакомое, не находите?
        \end{Proof}
        \begin{Example}
            В частности, $\left(\omega^1\land\cdots\land\omega^p\right)\left(\xi_1;\ldots;\xi_p\right)=\matrd{\xi^1_1&\cdots&\xi^1_p\\\vdots&\ddots&\vdots\\\xi^p_1&\cdots&\xi^p_p}$
        \end{Example}
        \thm Пусть $f^1;\ldots;f^p\in\Lambda^1V^*$ и $f^j$ имеет координаты $\matr{a^j_1&a^j_2&\cdots&a^j_n}=a^j$.\\
        Тогда $f^1\land\cdots\land f^p=\sum\limits_{j_1<\cdots<j_p}\matrd{a^1_{j_1}&\cdots&a^1_{j_p}\\\vdots&\ddots&\vdots\\a^p_{j_1}&\cdots&a^p_{j_p}}\omega^1\land\cdots\land\omega^p$.
        \begin{Proof}
            По сути надо доказать, что $\beta_{j_1\ldots j_p}=\matrd{a^1_{j_1}&\cdots&a^1_{j_p}\\\vdots&\ddots&\vdots\\a^p_{j_1}&\cdots&a^p_{j_p}}$. Ну, $\beta_{j_1\ldots j_p}=\alpha_{i_1\cdots i_p}$. А $\alpha$ по определению равно $f^1\land\cdots\land f^p(e_1;\ldots;e_n)$. Это, как мы знаем, равно
            $$\matrd{f^1(e_1)&\cdots&f^1(e_p)\\\vdots&\ddots&\vdots\\f^p(e_1)&\cdots&f^p(e_p)}$$
            А $f^j(e_i)=a^j_i$.
        \end{Proof}
        \thm Из двух утверждений выше
        $$f^1\land\cdots\land f^p(\xi_1;\ldots;\xi_p)=\sum\limits_{j_1<\cdots<j_p}\matrd{a^1_{j_1}&\cdots&a^1_{j_p}\\\vdots&\ddots&\vdots\\a^p_{j_1}&\cdots&a^p_{j_p}}\matrd{\xi_1^{j_1}&\cdots&\xi_1^{j_p}\\\vdots&\ddots&\vdots\\\xi_p^{j_1}&\cdots&\xi_p^{j_p}}$$
        \begin{Comment}
            Хорошо, а что с поливекторами? Которые $f\in T_{(0;q)}$ и антисимметричны. И пространство $V^qV$ образуют. Между ними также вводится внешнее произведение $f\lor g=\frac{(q_1+q_2)!}{q_1!q_2!}\Alt(f\otimes g)$. Поскольку от обсуждённого нами это отличается только тем, что альтернируем мы по нижним индексам, никакие свойства не меняются, только $\land$ меняется на $\lor$, $\omega$ --- на $e$ и так далее. Более того, давайте внимательно посмотрим на равенство $$f^1\land\cdots\land f^p(\xi_1;\ldots;\xi_p)=\sum\limits_{j_1<\cdots<j_p}\matrd{a^1_{j_1}&\cdots&a^1_{j_p}\\\vdots&\ddots&\vdots\\a^p_{j_1}&\cdots&a^p_{j_p}}\matrd{\xi_1^{j_1}&\cdots&\xi_1^{j_p}\\\vdots&\ddots&\vdots\\\xi_p^{j_1}&\cdots&\xi_p^{j_p}}$$
            Что такое $a_j^i$? Это $f^i(e_j)$. Но это же и $e_j(f^i)=f^i_j$. А что такое $\xi_i^j$? Это $\omega^i(\xi_j)$. Но это в то же время и $\xi_j(\omega^i)=\mathrm x_j^i$. То есть простой сменой букв (и поменяв определки местами) получаем
            $$f^1\land\cdots\land f^p(\xi_1;\ldots;\xi_p)=\sum\limits_{j_1<\cdots<j_p}\matrd{\mathrm x_1^{j_1}&\cdots&\mathrm x_1^{j_p}\\\vdots&\ddots&\vdots\\\mathrm x_p^{j_1}&\cdots&\mathrm x_p^{j_p}}\matrd{f^1_{j_1}&\cdots&f^1_{j_p}\\\vdots&\ddots&\vdots\\f^p_{j_1}&\cdots&f^p_{j_p}}$$
            Но это же $\xi_1\lor\cdots\lor\xi_p(f^1;\ldots;f^p)$! Просто буквально оно.
        \end{Comment}
    \end{itemize}
    \section{Евклидовы и унитарные пространства.}
    \undef{\Im}
    \let\Im\complexIm
    \paragraph{\undercolorblack{orange}{Скалярное и псевдоскалярное произведения}. \undercolorblack{orange}{Евклидово и унитарное пространства}.\\\undercolorblack{orange}{Норма в евклидовых и унитарных пространствах}.}
    \begin{itemize}
        \dfn Пусть $V$ --- линейное пространство над $\mathbb R$. Тогда операция $\dotprod{\cdot}{\cdot}\colon V^2\to\mathbb R$ называется \undercolor{red}{скалярным произведением}, если оно удовлетворяет аксиомам скалярного произведения:
        \begin{enumerate}
            \item Симметричность: $\dotprod{x}{y}=\dotprod{y}{x}$.
            \item Аддитивность по первому аргументу: $\dotprod{x_1+x_2}{y}=\dotprod{x_1}{y}+\dotprod{x_2}{y}$.
            \item Однородность по первому аргументу: $\dotprod{\lambda x}{y}=\lambda\dotprod{x}{y}$.
            \item Положительная определённость: $\forall x\neq\mathbb0~\dotprod{x}{x}>0$.
        \end{enumerate}
        \thm Из первых трёх аксиом скалярное произведение линейно по обоим аргументам.
        \dfn Вещественное линейное пространство со скалярным произведением называется\\
        \undercolor{red}{евклидовым линейным пространством}.
        \dfn Пусть $V$ --- линейное пространство над $\mathbb C$. Тогда операция $\dotprod{\cdot}{\cdot}\colon V^2\to\mathbb C$ называется \undercolor{red}{псевдоскалярным произведением}, если оно удовлетворяет аксиомам псевдоскалярного произведения:
        \begin{enumerate}
            \item Псевдосимметричность: $\dotprod{x}{y}=\overline{\dotprod{y}{x}}$.
            \item Аддитивность по первому аргументу: $\dotprod{x_1+x_2}{y}=\dotprod{x_1}{y}+\dotprod{x_2}{y}$.
            \item Однородность по первому аргументу: $\dotprod{\lambda x}{y}=\lambda\dotprod{x}{y}$.
            \item Положительная определённость: $\forall x\neq\mathbb0~\dotprod{x}{x}>0$.
        \end{enumerate}
        \begin{Comment}
            Чтобы в четвёртой аксиоме говорить о том, что $\dotprod{x}{x}>0$, нужно понять, что это вещественное число. Ну, оно действительно таковое по первой аксиоме.
        \end{Comment}
        \thm Псевдоскалярное произведение аддитивно по обоим аргументам, но не является линейным по второму, потому что $\dotprod{x}{\lambda y}=\overline\lambda\dotprod{x}{y}$.
        \dfn Комплексное линейное пространство с псевдоскалярным произведением называется\\
        \undercolor{red}{унитарным линейным пространством}.
        \dfn \undercolor{red}{Евклидова норма} --- $\|x\|=\sqrt{\dotprod{x}{x}}$.
        \thm \undercolor{darkgreen}{Неравенство Коши-Буняковского-Шварца}. $\forall x,y\in V~|\dotprod xy|\leqslant\|x\|\|y\|$. Причём равенство достигается только тогда, когда $x$ и $y$ линейно зависимы.
        \begin{Proof}
            Заметим, что
            $$\forall\alpha;\beta\in\mathbb C~\dotprod{\alpha x+\beta y}{\alpha x+\beta y}=\alpha^2\dotprod{x}{x}+\alpha\overline\beta\dotprod{x}{y}+\overline\alpha\beta\dotprod{y}{x}+\beta\overline\beta\dotprod{y}{y}$$
            Теперь возьмём конкретные $\alpha$ и $\beta$: $\alpha=\dotprod{y}{y}$, $\beta=-\dotprod{x}{y}$. Тогда несложно заметить, что $\overline\beta=-\dotprod{y}{x}$. Для данных $\alpha$ и $\beta$ имеем:
            $$\dotprod{y}{y}(\dotprod{y}{y}\dotprod{x}{x}-\dotprod{y}{x}\dotprod{x}{y}-\dotprod{x}{y}\dotprod{y}{x}+\dotprod{x}{y}\dotprod{y}{x}=\|y\|^2(\|x\|^2\|y\|^2-2|\dotprod{x}{y}|^2+|\dotprod{x}{y}|^2)$$
            Теперь заметим, что наше изначальное выражение $\dotprod{\alpha x+\beta y}{\alpha x+\beta y}$ не меньше нуля и $\|y\|^2$ не меньше нуля. А значит $\|x\|^2\|y\|^2-|\dotprod{x}{y}|^2\geqslant0$.\\
            Теперь докажем, что равенство достигается при линейно зависимых векторах. Сначала что из равенства следует зависимость. Равенство достигается при $\|\alpha x+\beta y\|=0\Leftrightarrow\alpha x+\beta y=\mathbb0$. Это как раз и есть линейная зависимость.\\
            Теперь докажем, что если $x$ и $y$ зависимы, то достигается равенство. Если они зависимы, то $\alpha x+\beta y=\mathbb0$, где $\alpha$ и $\beta$ не могут быть равны нулю одновременно. Ну, в таком случае $\dotprod{\alpha x+\beta y}{x}=0$ и $\dotprod{\alpha x+\beta y}{y}=0$. А это значит, что
            \[
            \left\{\begin{aligned}
                \alpha\dotprod{x}{x}+\beta\dotprod{y}{x}=0\\
                \alpha\dotprod{x}{y}+\beta\dotprod{y}{y}=0
            \end{aligned}\right.\Leftrightarrow
            \left\{\begin{aligned}
                \alpha\|x\|^2=-\beta\dotprod{y}{x}\\
                \beta\|y\|^2=-\alpha\dotprod{x}{y}
            \end{aligned}\right.\Leftrightarrow
            \alpha\beta\|x\|^2\|y\|^2=\alpha\beta|\dotprod{x}{y}|^2\]
        \end{Proof}
        \thm Евклидова норма действительно является нормой.
        \begin{Proof}
            \begin{itemize}
                \item Невырожденность: $\|x\|=0\Leftrightarrow x=\mathbb0$. Тривиально.
                \item Однородность: $\|\lambda x\|=\sqrt{\lambda\overline\lambda\dotprod xx}=|\lambda|\sqrt{\dotprod xx}=|\lambda|\|x\|$.
                \item Неравенство треугольника. Следует из К---Б---Ш следующим образом:
                \[
                \begin{split}
                    \|x+y\|^2&=\dotprod{x+y}{x+y}=\|x\|^2+\underbrace{\overline{\dotprod{x}{y}}+\dotprod{x}{y}}_{2\Re\dotprod{x}{y}}\leqslant\|x\|^2+2|\dotprod{x}{y}|+\|y\|^2=\\
                    &=\overset{\text{К---Б---Ш}}\leqslant\|x\|^2+2\|x\|\|y\|+\|y\|^2=(\|x\|+\|y\|)^2
                \end{split}
                \]
            \end{itemize}
        \end{Proof}
        \dfn \undercolor{red}{Длиной вектора} $x$ называется его евклидова норма.
        \dfn \undercolor{red}{Косинусом угла между векторами} $x$ и $y$ называется $\frac{\dotprod{x}{y}}{\|x\|\|y\|}$.
        \begin{Example}
            Самый простой пример --- геометрические вектора со скалярным произведением, которые мы изучали много времени очень давно.
        \end{Example}
        \begin{Example}
            На $\mathbb R^n$ можно ввести $\dotprod{x}{y}=\sum\limits_{i=1}^nx_iy_i$. Аксиомы были проверены на матане.
        \end{Example}
        \begin{Example}
            На пространстве функций $R[a;b]$ можно взять $\dotprod{f}{g}=\int_a^bf(x)g(x)~\mathrm dx$. Тут нет никаких проблем с аксиомами 1--3, но 4 просто не верна. потому что мы приводили пример функции, которая \textbf{почти} везде равна нулю, имеет нулевой интеграл. Поэтому такую функцию хочется отождествить нулевой, как и вообще две функции, разность которых равна нулю почти везде.
        \end{Example}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Процесс ортогонализации Грама --- Шмидта}. \undercolorblack{orange}{Ортонормированный базис}. \undercolorblack{orange}{Ортогональные системы}.}
    \begin{itemize}
        \dfn Вектора $x,y\in V$ называются \undercolor{red}{ортогональными}, если $\dotprod{x}{y}=0$.
        \thm Единственный вектор, ортогональный всем векторам из $V$ --- это $\mathbb0$.
        \begin{Proof}
            То что ноль ортогонален всем --- тривиально, для любого другого это не верно, потому что он должен быть ортогонален сам себе, а $\dotprod{x}{x}=0$ только если $x=\mathbb0$.
        \end{Proof}
        \dfn \undercolor{red}{Ортогональная система векторов} --- система \textbf{ненулевых} векторов, которые попарно ортогональны.
        \thm Ортогональная система векторов линейно независима.
        \begin{Proof}
            Посмотрим на нулевую линейную комбинацию $\sum\limits_{i=1}^m\alpha_i v_i=\mathbb0$. Возьмём скалярное произведение этой суммы с некоторым $v_j$, получим $\sum\limits_{i=1}^m\alpha_i\dotprod{v_i}{v_j}=\mathbb0$. По попарной ортогональности это равно $\alpha_j\dotprod{v_j}{v_j}=\mathbb0$, то есть (вектора ненулевые) $\alpha_j=0$.
        \end{Proof}
        \dfn \undercolor{red}{Ортом} ненулевого вектора $x\in V$ называется вектор $\frac x{\|x\|}$.
        \dfn Вектор, имеющий единичную длину, \undercolor{red}{называется нормированным}.
        \dfn \undercolor{red}{Ортонормированная система векторов} --- ортогональная система нормированных векторов.
        \dfn \undercolor{red}{Ортонормированный базис} --- базис, являющийся ортонормированной системой векторов.
        \thm \undercolor{darkgreen}{Процесс ортогонализации Грама --- Шмидта}. Любая система векторов $a_1;\ldots;a_m$ может быть заменена на систему ортогональных векторов $b_1;\ldots;b_p$ (где $p\leqslant m$) с той же линейной оболочкой.
        \pagebreak
        \begin{Comment}
            Почему $p\leqslant m$? Потому что $b_1;\ldots;b_p$ линейно независимы, а про независимость $a_1;\ldots;a_m$ мы ничего не знаем, значит векторов поменьше получится.
        \end{Comment}
        \begin{Proof}
            Будем строить $b_1;\ldots;b_p$ итеративно. И доказывать методом математической индукции. Пусть у нас есть сначала 2 вектора: $a_1$ и $a_2$. Мы хотим взять $b_1=a_1$, а в качестве $b_2$ взять $a_2-c_1b_1$, где $c_1$ --- некоторое число. При этом хочется взять такое число, что $\dotprod{b_2}{b_1}=0$. Тогда $0=\dotprod{a_2-c_1b_1}{b_1}=\dotprod{a_2}{b_1}-c_1\dotprod{b_1}{b_1}\Rightarrow c_1=\frac{\dotprod{a_2}{b_1}}{\dotprod{b_1}{b_1}}$.\\
            Как мы это будем делать для трёх векторов? Ну, мы знаем $b_1$ и $b_2$ и хотим найти $b_3=a_3-d_1b_1-d_2b_2$. Аналогичным рассуждением получим $d_1=\frac{\dotprod{a_3}{b_1}}{\dotprod{b_1}{b_1}}$, $d_2=\frac{\dotprod{a_3}{b_2}}{\dotprod{b_2}{b_2}}$. Теперь только грамотно распишем индукционный переход, и закончим с доказательством.\\
            Итак, мы уже взяли $b_1;\ldots;b_k$ --- ортогональные вектора. Хочется добавить $b_{k+1}$. Известно, что $\Lin\{a_1;\ldots;a_k\}=\Lin\{b_1;\ldots;b_k\}$. Тогда $b_{k+1}=a_{k+1}-\sum\limits_{j=1}^kc_jb_j$. Мы хотим $\dotprod{b_{k+1}}{b_i}=0\Leftrightarrow \dotprod{a_{k+1}}{b_i}-\sum\limits_{j=1}^kc_j\dotprod{b_j}{b_i}=\dotprod{a_{k+1}}{b_i}-c_i\dotprod{b_i}{b_i}$. Отсюда возьмём $c_i=\frac{\dotprod{a_{k+1}}{b_i}}{\dotprod{b_i}{b_i}}$ и получим то, что нам хочется. Надо лишь пояснить, что линейная оболочка окажется неизменной, но это тривиально.
        \end{Proof}
        \begin{Comment}
            Если вы не уверены в линейной независимости ваших векторов, то можно сначала их прополоть, а можно забить на это, и просто в ортогонализации в процессе получить $b_k=\mathbb0$, после чего выкинуть их.
        \end{Comment}
        \thm В любом евклидовом или унитарном пространстве существует ортонормированный базис.
        \begin{Proof}
            Возьмём рандомный, ортогонализуем. После этого нормируем.
        \end{Proof}
        \thm В любом евклидовом или унитарном пространстве любую ортонормированную систему векторов можно дополнить до ортонормированного базиса.
        \begin{Proof}
            Дополним чем попало, после чего запустим процесс ортогонализации, но не с начала, а с того момента, когда $k$ векторов у нас уже есть. После этого снова нормируем.
        \end{Proof}
        \begin{Example}
            Взяв множество $2\pi$-периодичных функций на отрезке $[-\pi;\pi]$, таких что $\int\limits_{-\pi}^\pi|f|~\mathrm dx<\infty$, и скалярное произведение $\int\limits_{-\pi}^\pi f(x)g(x)~\mathrm dx$, можем найти следующую ортогональную систему векторов:\\
            $\{1;\sin x;\cos x;\sin 2x;\cos 2x;\ldots\}$. Тогда коэффициенты разложения в этом базисе выглядят как $a_k=\frac{\dotprod{f}{\cos kx}}{\dotprod{\cos kx}{\cos kx}}$ и $b_k=\frac{\dotprod{f}{\sin kx}}{\dotprod{\sin kx}{\sin kx}}$. Чему равно $\dotprod{\cos kx}{\cos kx}$? Ну, $\int\limits_{-\pi}^\pi\cos^2kx~\mathrm dx=\int\limits_{-\pi}^\pi\frac{1+\cos 2x}2~\mathrm dx=\pi$. То есть $a_k=\frac1\pi\int\limits_{-\pi}^\pi f(x)\cos kx~\mathrm dx$. Аналогично для синуса, и почти аналогично для 1. Одна лишь проблема --- не всегда на самом деле будет равенство, потому что иногда ряды Фурье будут расходиться. 
        \end{Example}
        \begin{Example}
            Теперь рассмотрим пространство многочленов степени не выше $n$. В нём мы знаем базис $\{1;x;x^2;\ldots;x^n\}$. Зададим скалярное произведение также, как и в прошлый раз: $\dotprod{p}{q}=\int\limits_{-1}^1p(x)q(x)~\mathrm dx$. Система у нас не ортогональная (проверьте это сами), поэтому применим к неё ортогонализацию. Несколько первых векторов посчитаем, все считать не будем.
            \begin{enumerate}
                \item $\widetilde{e_0}=1$.
                \item $\widetilde{e_1}=x-\frac{\dotprod{x}{1}}{\dotprod{1}{1}}1=x-\frac{\int_{-1}^1x~\mathrm dx}{\int_{-1}^11~\mathrm dx}=x$.
                \item $\widetilde{e_2}=x^2-\frac{\dotprod{x^2}{x}}{\dotprod{x}{x}}x-\frac{\dotprod{x^2}{1}}{\dotprod{1}{1}}1=x^2-0x-\frac{2/3}21=x^2-\frac13$.
                \item $\widetilde{e_3}=x^3-\frac{\dotprod{x^3}{x^2-\frac13}}{\dotprod{x^2-\frac13}{x^2-\frac13}}\left(x^2-\frac13\right)-\frac{\dotprod{x^3}{x}}{\dotprod{x}{x}}x-\frac{\dotprod{x^3}{1}}{\dotprod{1}{1}}1=x^3-0\left(x^2-\frac13\right)-\frac{2/5}{2/3}x-0\cdot1=x^3-\frac35x$.
            \end{enumerate}
            То что мы получили --- многочлены Лежандра (там есть нормировки, но сейчас не о них). С их помощью можно функцию от матрицы считать, и много чего ещё интересного делать, но это не в нашем курсе, и слава Безымянному Богу. А сейчас мы выведем общую формулу для этих многочленов.
        \end{Example}
        \thm \undercolor{darkgreen}{Формула Родрига}. В условиях примера выше многочлены Лежандра выглядят так:
        $$
        \widetilde{e_k}(x)=\lambda_k\left((x^2-1)^k\right)^{(k)}
        $$
        Где $\lambda_k$ --- некоторая константа (которую вы сами можете выбрать при желании, на скалярное произведение не повлияет).
        \begin{Proof}
            Давайте покажем, что любой многочлен вида $q_k(x)=\left((x^2-1)^k\right)^{(k)}$ ортогонален любому $x^m$, где $m<k$:
            \[
            \begin{split}
                \dotprod{q_k}{x^m}&=\int_{-1}^1\left((x^2-1)^k\right)^{(k)}x^m~\mathrm dx=\\
                &=\underbrace{\left((x^2-1)^k\right)^{(k-1)}x^m\bigg|_{-1}^1}_{0\text{ см.}(*)}-m\int_{-1}^1\left((x^2-1)^k\right)^{(k-1)}x^{m-1}~\mathrm dx=\\
                &=-m\cdot0+m(m-1)\int_{-1}^1\left((x^2-1)^k\right)^{(k-2)}x^{m-2}~\mathrm dx=\\
                &=\cdots=\\
                &=\pm m!\int_{-1}^1\left((x^2-1)^k\right)^{(k-m)}~\mathrm dx=\\
                &=\pm m!\left((x^2-1)^k\right)^{(k-m-1)}\bigg|_{-1}^1\overset{(**)}=0
            \end{split}
            \]
            $(*)$: Если $x$ --- корень многочлена $p(x)$ кратности $k$, то он корень $p'(x)$ кратности $k-1$. А значит он корень $p^{(k-1)}(x)$ кратности 1.\\
            $(**)$: Поскольку $m<k$, в худшем случае левая часть равна $(x^2-1)^k$, что даёт нам 0. И любая производная также даёт 0 по $(*)$.\\
            Что следует из доказанного? Ну, то что мы берём $q_k$, оно перпендикулярно каждому из $e_m$ при $m<k$, значит линейно независим с ними. Отсюда для любого $k$ $\Lin\{e_0;\ldots;e_k\}=\Lin\{q_0;\ldots;q_k\}$. Также по аддитивности скалярного произведения $q_k\perp q_m$, что значит, что $q_0;\ldots;q_n$ --- ортогональный базис.
        \end{Proof}
        \thm \undercolor{darkgreen}{Общая формула Родрига}. При $\lambda_k=\frac1{2^kk!}$ $q_k(1)=1$.
        \begin{Proof}
            Давайте посчитаем значение многочлена Лежандра в точке 1.
            \[
            \begin{split}
                q_k(1)&=\left((x^2-1)^k\right)^{(k)}\bigg|_{x=1}=\\
                &=\sum\limits_{m=0}^k\Cnk km\left((x+1)^k\right)^{(m)}\left((x-1)^k\right)^{(k-m)}\bigg|_{x=1}\overset{k\neq m\text{ обнуляется}}=\\
                &=(x+1)^kk!=2^kk!
            \end{split}
            \]
        \end{Proof}
        \begin{Example}
            Другой пример ортогональных многочленов --- полиномы Чебышёва $T_n(x)=\cos(n\acos x)$. То что это полиномы, не очень тривиально, но тем не менее это так. Тем не менее, это можно доказать по индукции. Доказывать мы будем утверждение, что $T_{n+1}+T_{n-1}=2xT_n$. Базу посмотрите сами, переход вот:
            \[
            \cos((n+1)\acos x)+\cos((n-1)\acos x)=2\cos(n\acos x)\cos\acos x=2xT_n
            \]
            Итого, $T_n$ --- это многочлен степени $n$.\\
            Только тут скалярное произведение не стандартное, а вот такое:
            \[
            \dotprod pq=\int_{-1}^1\frac{p(x)q(x)}{\sqrt{1-x^2}}~\mathrm dx
            \]
        \end{Example}
        \begin{Example}
            Полиномы Эрмита: возьмём многочлены и скалярное произведение $\int_0^{+\infty}p(x)q(x)e^{-x^2}~\mathrm dx$. Тогда ортогональными многочленами являются $H_n(x)=e^{x^2}\left(e^{-x^2}\right)^{(n)}$. Как мы доказывали на матане, производные $e^{-x^2}$ являют собой некоторый многочлен на $e^{-x^2}$, то есть это правда многочлены. Например, $H_0(x)=1$, $H_1(x)=-2x$, $H_2(x)=4x^2-2$.
        \end{Example}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Матрица Грама}. \undercolorblack{orange}{Объём $k$-мерного параллелепипеда}. \undercolorblack{orange}{Ортогональные и унитарные матрицы}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Хочется считать скалярное произведение в координатах, как мы это делали в геометрических векторах.
        \end{Comment}
        \dfn Пусть $e_1;\ldots;e_n$ --- базис $V$, $x=\sum\limits_{i=1}^n\mathrm x_ie_i$, $y=\sum\limits_{i=1}^n\mathrm y_ie_i$. Тогда
        \[
        \dotprod{x}{y}=\dotprod{\sum\limits_{i=1}^n\mathrm x_ie_i}{\sum\limits_{j=1}^n\mathrm y_je_j}=\sum\limits_{i=1}\sum\limits_{j=1}\mathrm x_i\overline{\mathrm y_j}\dotprod{e_i}{e_j}
        \]
        То есть скалярное произведение достаточно задать на базисных векторах.\\
        \undercolor{red}{Матрица Грама} --- матрица $\Gamma_{ij}=\dotprod{e_i}{e_j}$.
        \thm В условии определения выше $\dotprod{x}{y}=\mathrm x^T\Gamma\overline{\mathrm y}$.
        \begin{Example}
            Если $e_1;\ldots;e_n$ --- ортогональный базис, то $\Gamma=\diag(\|e_1\|^2;\ldots;\|e_1\|^n)$, а значит $\dotprod{x}{y}=\sum\limits_{i=1}^n\mathrm x_i\overline{\mathrm y_i}\|e_i\|^2$. А если взять ортонормированный базис, то вообще получается $\sum\limits_{i=1}^n\mathrm x_i\overline{\mathrm y_i}$, то есть как в геометрическом случае.
        \end{Example}
        \dfn Пусть $a_1;\ldots;a_k$ --- какая-то система векторов. \undercolor{red}{Матрицей Грама} этой \undercolor{red}{системы} называется матрица $G(a_1;\ldots;a_k)$ такая что $G(a_1;\ldots;a_k)_{ij}=\dotprod{a_i}{a_j}$. Её определитель обозначается $g(a_1;\ldots;a_k)$.
        \dfn Если $A$ --- матрица, то \undercolor{red}{сопряжённой матрицей} называется матрица $A^*=\overline{A^T}$.
        \dfn Если $A=A^*$, то $A$ --- \undercolor{red}{самосопряжённая матрица}. В унитарном пространстве также используется название \undercolor{red}{эрмитова матрица}.
        \begin{Comment}
            В евклидовом пространстве самосопряжённая матрица --- это симметричная матрица.
        \end{Comment}
        \thm Тривиально, матрица Грама самосопряжена.
        \thm \undercolor{darkgreen}{Теорема об определителе матрицы Грама}. Пусть $a_1;\ldots;a_k$ --- система векторов. Тогда $g(a_1;\ldots;a_k)=\|b_1\|^2\cdots\|b_k\|^2$, где $b_1;\ldots;b_k$ --- система векторов, полученная из $a_1;\ldots;a_k$ ортогонализацией Грама-Шмидта.
        \begin{Proof}
            Итак,
            $$
            g(a_1;\ldots;a_k)=\matrd{
                \dotprod{a_1}{a_1} & \dotprod{a_1}{a_2} & \cdots & \dotprod{a_1}{a_k}\\
                \dotprod{a_2}{a_1} & \dotprod{a_2}{a_2} & \cdots & \dotprod{a_2}{a_k}\\
                \vdots & \vdots & \ddots & \vdots\\
                \dotprod{a_k}{a_1} & \dotprod{a_k}{a_2} & \cdots & \dotprod{a_k}{a_k}}
            $$
            Мы знаем, что $b_1=a_1$. Теперь возьмём формулу $b_2=a_2-\frac{\dotprod{a_2}{b_1}}{\dotprod{b_1}{b_1}}b_1$ и умножим её скалярно на $b_1$. Получим $\dotprod{b_2}{b_1}=\dotprod{a_2}{b_1}-\dotprod{a_2}{b_1}$. То есть если мы вычтем из второй строки первую, умноженную на $\frac{\dotprod{a_2}{b_1}}{\dotprod{b_1}{b_1}}$, и аналогично поступим со столбцом. Получим тот же определитель (это линейное преобразование), но вот какую матрицу:
            $$
            g(a_1;\ldots;a_k)=\matrd{
                \dotprod{b_1}{b_1} & \dotprod{b_1}{b_2} & \dotprod{b_1}{a_3} & \cdots & \dotprod{b_1}{a_k}\\
                \dotprod{b_2}{b_1} & \dotprod{b_2}{b_2} & \dotprod{b_2}{a_3} & \cdots & \dotprod{b_2}{a_k}\\
                \dotprod{a_3}{b_1} & \dotprod{a_3}{b_2} & \dotprod{a_3}{a_3} & \cdots & \dotprod{a_3}{a_k}\\
                \vdots & \vdots & \vdots & \ddots & \vdots\\
                \dotprod{a_k}{b_1} & \dotprod{a_k}{b_2} & \dotprod{a_k}{a_3} & \cdots & \dotprod{a_k}{a_k}}
            $$
            Аналогичные действия с остальными векторами, получим
            $$
            g(a_1;\ldots;a_k)=\matrd{
                \dotprod{b_1}{b_1} & \dotprod{b_1}{b_2} & \dotprod{b_1}{b_3} & \cdots & \dotprod{b_1}{b_k}\\
                \dotprod{b_2}{b_1} & \dotprod{b_2}{b_2} & \dotprod{b_2}{b_3} & \cdots & \dotprod{b_2}{b_k}\\
                \dotprod{b_3}{b_1} & \dotprod{b_3}{b_2} & \dotprod{b_3}{b_3} & \cdots & \dotprod{b_3}{b_k}\\
                \vdots & \vdots & \vdots & \ddots & \vdots\\
                \dotprod{b_k}{b_1} & \dotprod{b_k}{b_2} & \dotprod{b_k}{b_3} & \cdots & \dotprod{b_k}{b_k}
                }=\matrd{
                    \dotprod{b_1}{b_1}\!\! & 0 & 0 & \cdots & 0\\
                    0 & \!\!\dotprod{b_2}{b_2}\!\! & 0 & \cdots & 0\\
                    0 & 0 & \!\!\dotprod{b_3}{b_3}\!\! & \cdots & 0\\
                    \vdots & \vdots & \vdots & \ddots & \vdots\\
                    \!\!0 & \!\!0 & \!\!0 & \!\!\cdots & \!\!\dotprod{b_k}{b_k}
                }
            $$
        \end{Proof}
        \thm Система векторов $a_1;\ldots;a_k$ линейно независима тогда и только тогда, когда $g(a_1;\ldots;a_k)>0$.
        \thm Если $a_1;\ldots;a_k$ линейно независимы, а $a_{k+1}$ --- некоторый вектор, то
        $$
        \|b_{k+1}\|^2=\frac{g(a_1;\ldots;a_{k+1})}{g(a_1;\ldots;a_k)}
        $$
        \dfn Пусть $\dim V=n$, $k\in[1:n]$. Тогда \undercolor{red}{$k$-мерным параллелепипедом}, натянутым на вектора $a_1;\ldots;a_k$ называется множество $\left\{\sum\limits_{i=1}^k\alpha_ia_i~\middle|~\forall i\in[1:k]~\alpha_i\in[0;1]\right\}$. Я буду обозначать его как Олег Леонидович: $[a_1;\ldots;a_k]$.
        \dfn Пусть $V$ --- вещественное пространство. Тогда \undercolor{red}{объёмом параллелепипеда}, натянутого на $a_1;\ldots;a_k$ называется число $\sqrt{g(a_1;\ldots;a_k)}$.
        \thm Если $a_1;\ldots;a_k$ линейно независимы, то $V[a_1;\ldots;a_k]=\|b_{k+1}\|V[a_1;\ldots;a_{k+1}]$.
        \thm Пусть $a_i$ в ортонормированном базисе имеет координатный столбец $\mathrm a_i=\matr{\mathrm a_i^1\\\cdots\\\mathrm a_i^n}$. Тогда
        $$V[a_1;\ldots;a_k]=\sqrt{\matrd{
                \mathrm a_1^T\mathrm a_1 & \mathrm a_1^T\mathrm a_2 & \cdots & \mathrm a_1^T\mathrm a_k\\
                \mathrm a_2^T\mathrm a_1 & \mathrm a_2^T\mathrm a_2 & \cdots & \mathrm a_2^T\mathrm a_k\\
                \vdots & \vdots & \ddots & \vdots\\
                \mathrm a_k^T\mathrm a_1 & \mathrm a_k^T\mathrm a_2 & \cdots & \mathrm a_k^T\mathrm a_k
        }}=\sqrt{\left|\matr{\mathrm a_1 & \cdots & \mathrm a_k}^T\matr{\mathrm a_1 & \cdots & \mathrm a_k}\right|}$$
        \thm В случае $k=n$ объём параллелепипеда на векторах $a_1;\ldots;a_n$ равен модулю определителя матрицы, составленной из координат $a_i$.
        \thm Пусть $\scriptB\in\End(V)$. Чему тогда равно $\scriptB([a_1;\ldots;a_n])$? Ну, это $\scriptB\left(\sum\alpha_i\mathrm a_i\right)=[\scriptB a_1;\ldots;\scriptB a_n]$. А значит $V(\scriptB([a_1;\ldots;a_n]))=V([\scriptB a_1;\ldots;\scriptB a_n])=|\det(\scriptB a_1;\ldots;\scriptB a_n)|=|\det\scriptB|V([a_1;\ldots;a_n])$.
        \begin{Comment}
            В частности, если $\scriptB$ поворачивает пространство, то при таком преобразовании объём не меняется.
        \end{Comment}
        \thm Если $\Gamma$ --- матрица Грама от базиса, то $\forall x\neq0~\mathrm x^T\Gamma\overline{\mathrm x}>0$.
        \begin{Proof}
            $\mathrm x^T\Gamma\overline{\mathrm x}=\dotprod xx$, что действительно больше нуля для ненулевого $x$.
        \end{Proof}
        \dfn Матрица называется $A$ \undercolor{red}{положительно определённой}, если $A^*=A$ и $\forall x\neq0~\mathrm x^TA\overline{\mathrm x}>0$.
        \dfn Если $\Gamma$ --- матрица Грама для базиса $(e_1;\ldots;e_n)$, то определители $\Delta_k=g(e_1;\ldots;e_k)$ называются \undercolor{red}{угловыми минорами}.
        \thm Угловые миноры больше нуля.
        \begin{Proof}
            Определители матрицы Грамма независимой системы (коими являются угловые миноры) положительны.
        \end{Proof}
        \thm Если $e_1;\ldots;e_n$ и $e_1';\ldots;e_n'$ --- базисы $V$, $T=T_{e\to e'}$. Тогда $\Gamma'=T^T\Gamma\overline T$.
        \begin{Proof}
            Как мы знаем, $\mathrm x^T\Gamma\overline{\mathrm y}=\dotprod xy={{\mathrm x}'}^T\Gamma'\overline{\mathrm y'}$. При этом $\mathrm x=T\mathrm x'$, $\mathrm y=T\mathrm y'$. То есть
            $$
            \mathrm x^T\Gamma\overline{\mathrm y}={\mathrm x'}^TT^T\Gamma\overline T\overline{\mathrm y'}
            $$
            А мы знаем, что скалярное произведение равно ${\mathrm x'}^T\Gamma'\overline{\mathrm y'}$, значит ${\mathrm x'}^T\Gamma'\overline{\mathrm y'}={\mathrm x'}^TT^T\Gamma\overline T\overline{\mathrm y'}$. Осталось лишь понять, что из этого следует равенство внутренних матриц. Ну, рассмотрим $x'=e_i$, $y'=e_j$. Тогда получим $(\Gamma')_{ij}=(T^T\Gamma\overline T)_{ij}$.
        \end{Proof}
        \begin{Comment}
            В частности, если $e_1;\ldots;e_n$ и $e_1';\ldots;e_n'$ ортонормированные базисы, то $\Gamma=E$
        \end{Comment}
        \dfn Невырожденная квадратная матрица $Q$ называется \undercolor{red}{ортогональной} (в $\mathbb R$)/\undercolor{red}{унитарной} (в $\mathbb C$), если $Q^*=Q^{-1}$.
        \thm Матрица является ортогональной/унитарной тогда и только тогда, когда её строки (либо столбцы) ортогональны и нормированы.
        \begin{Proof}
            Пусть $q_i$ --- столбцы $Q$. Тогда
            $$Q^*Q=E\Leftarrow\overline{Q}^TQ=E\Leftrightarrow\forall i,j~\dotprod{\overline q_i}{q_j}=\delta_{ij}$$
            Это ли не то, что нам надо?
        \end{Proof}
        \thm Если $Q$ ортогональна/унитарна, то $Q^{-1}$ также ортогональна/унитарна.
        \begin{Proof}
            Операция ${}^*$ является обратной сама к себе, а значит ${Q^{-1}}^*=Q$.
        \end{Proof}
        \thm Если $Q,R$ ортогональны/унитарны, то $QR$ также ортогональна/унитарна.
        \begin{Proof}
            Тривиально.
        \end{Proof}
        \thm Модуль определителя ортогональной/унитарной матрицы равен единице.
        \begin{Proof}
            $$
            \det(QQ^*)=1\Leftrightarrow\det Q\det Q^*=1\Leftrightarrow\det Q\det\overline{Q}=1\Leftrightarrow\det Q\overline{\det Q}=1\Leftrightarrow|\det Q|=1
            $$
        \end{Proof}
        \thm Матрица перехода между ортонормированными базисами является ортогональной/унитарной.
        \begin{Proof}
            Одно из свойств $\Gamma$ в помощь.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Ортогональное дополнение}. \undercolorblack{orange}{Задача о перпендикуляре}. \undercolorblack{orange}{Тееорема Пифагора}. \undercolorblack{orange}{Коэффициенты Фурье}. \undercolorblack{orange}{Тождество Парсеваля}. \undercolorblack{orange}{Неравенство Бесселя}.}
    \begin{itemize}
        \dfn Если $L$ --- подпространство $V$, то $L^\perp=\{y\in V\mid\forall x\in L~\dotprod{x}{y}=0\}$ --- \undercolor{red}{ортогональное дополнение} $L$.
        \thm $L^\perp$ является линейным подпространством $L$.
        \begin{Proof}
            Следует из псевдолинейности скалярного произведения.
        \end{Proof}
        \thm $V=L\oplus L^\perp$.
        \begin{Proof}
            Пусть $a_1;\ldots;a_k$ --- ортонормированный базис $L$. Дополним  его до ортонормированного базиса $V$ векторами $a_{k+1};\ldots;a_n$. Хочется доказать, что $\Lin\{a_{k+1};\ldots;a_n\}=L^\perp$. Рассмотрим $y\in\Lin\{a_{k+1};\ldots;a_n\}$. Он, тривиально, лежит в $L^\perp$. То есть $\Lin\{a_{k+1};\ldots;a_n\}\subset L^\perp$. Теперь докажем дизъюнктность $L$ и $L^\perp$. Ну, они пересекаются только по $\mathbb0$. А эти два факта значат следующее. Во-первых, $\dim L^\perp\geqslant(n-k)$, во-вторых, $L^\perp$ не может содержать строго больше векторов по дизъюнктности. А значит $\dim L^\perp=(n-k)$, откуда $\Lin\{a_{k+1};\ldots;a_n\}=L^\perp$.
        \end{Proof}
        \thm ${L^\perp}^\perp=L$.
        \begin{Proof}
            Известно, что $L\oplus L^\perp={L^\perp}^\perp\oplus L^\perp$. Отсюда $\dim L=\dim{L^\perp}^\perp$. Тогда возьмём $x\in L$, $y\in L^\perp$. Тогда $\dotprod{x}{y}=0$, а значит $x\in{L^\perp}^\perp$, то есть $L\subset{L^\perp}^\perp$. В сочетании с равенством размерностей получаем равенство множеств.
        \end{Proof}
        \thm $(L_1+L_2)^\perp=L_1^\perp\cap L_2^\perp$. $(L_1\cap L_2)^\perp=L_1^\perp+L_2^\perp$.
        \begin{Proof}
            Рассмотрим $y\in(L_1+L_2)^\perp$. Тогда $\forall x_1\in L_1,x_2\in L_2~\dotprod{y}{x_1+x_2}=0$ или $\dotprod y{x_1}+\dotprod y{x_2}=0$. Если положить $x_1=\mathbb0$, получим $y\in L_2^\perp$, а если положить $x_1=\mathbb0$, --- то $y\in L_1^\perp$. То есть $(L_1+L_2)^\perp\subset L_1^\perp\cap L_2^\perp$.\\
            Обратное включение. Пусть $y\in L_1^\perp\cap L_2^\perp$. Тогда $\forall x_1\in L_1~\dotprod y{x_1}=0$ и $\forall x_2\in L_2~\dotprod y{x_2}=0$. Бла-бла-бла, то же самое.\\
            Чтобы доказать второе утверждение, применим к обеим частям ${}^\perp$.
        \end{Proof}
        \thm $V^\perp=\{\mathbb0\}$, $\{\mathbb0\}^\perp=V$.
        \dfn Пусть $V=L\oplus L^\perp$. Тогда $\forall x\in V~\exists!y\in L,z\in L^\perp~x=y+z$. Тогда $y$ --- \undercolor{red}{ортогональная проекция} $x$ на $L$, $z$ --- \undercolor{red}{перпендикуляр} $x$ к $L$ либо \undercolor{red}{ортогональная составляющая} $x$ относительно $L$.
        \begin{Comment}
            Хочется найти перпендикуляр. Мы умеем его искать стандартно, мы уже искали проекцию на одно подпространство относительно другого. Но в нашем случае можно немного упростить задачу.\\
            Итак, пусть $y=\sum\limits_{j=1}^kc_ja_j$, где $a_1;\ldots;a_k$ --- базис $L$. Тогда $\dotprod x{a_i}=\dotprod{y+z}{a_i}\overset{z\perp L}=\dotprod{\sum\limits_{j=1}^kc_ja_j}{a_i}=\sum\limits_{j=1}^kc_j\dotprod{a_j}{a_i}$. Тогда, записав это для всех $i\in[1:k]$, получим
            $$
            G^T\matr{c_1\\c_2\\\vdots\\c_k}=\matr{\dotprod{x}{a_1}\\\dotprod{x}{a_2}\\\cdots\\\dotprod{x}{a_k}}
            $$
            Это линейное неоднородное уравнение, где $\det G^T\neq0$, а значит имеет оно единственное решение. Из него находим $c$, потом $y$, потом $z=x-y$.
        \end{Comment}
        \thm \undercolor{darkgreen}{Теорема Пифагора}. Если $y\perp z$, то $\|y+z\|^2=\|y\|^2+\|z\|^2$.
        \begin{Proof}
            $\|y+z\|^2=\dotprod{y+z}{y+z}=\dotprod yy+\underbrace{\dotprod yz}_0+\underbrace{\dotprod zy}_0+\dotprod zz=\|y\|^2+\|z\|^2$.
        \end{Proof}
        \thm Если $x_1;\ldots;x_k$ попарно ортогональны, то $\left\|\sum\limits_{i=1}^kx_k\right\|^2=\sum\limits_{i=1}^k\|x_k\|^2$.
        \begin{Proof}
            Мат. индукция.
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема о наилучшем приближении}. Пусть $x=y+z$, где $y\in L$, $z\in L^\perp$. Тогда $\forall x\in V~\forall l\in L~\|x-y\|\leqslant\|x-l\|$.
        \begin{Proof}
            $\|x-l\|^2=\|(x-y)+(y-l)\|^2\overset{\text{Пифагор}}=\|x-y\|^2+\|y-l\|^2\geqslant\|x-y\|^2$.
        \end{Proof}
        \dfn \undercolor{red}{Расстоянием} $x\in V$ \undercolor{red}{до линейного подпространства} $L\subset V$ называется число $\operatorname{dist}(x;L)=\min\limits_{l\in L}\|x-l\|$.
        \thm Теорема о наилучшем приближении доказывает корректность определения (т.е. что минимум достигается).
        \thm \undercolor{darkgreen}{Теорема о расстоянии до подпространства}. Если $L=\Lin\{a_1;\ldots;a_k\}$, то
        $$
        (\operatorname{dist}(x;L))^2=\frac{g(a_1;\ldots;a_k;x)}{g(a_1;\ldots;a_k)}
        $$
        \begin{Proof}
            Давайте по Граму-Шмидту ортогонализуем $a_1;\ldots;a_k;x$, получив $b_1;\ldots;b_k;b_{k+1}$. Тогда $b_{k+1}=x-\sum\limits_{i=1}^kc_ib_i$. Во-первых, $L=\Lin\{b_1;\ldots;b_k\}$. Во-вторых, $b_{k+1}\in L^\perp$. В таком случае $b_{k+1}=z$. А тут уже только $\|b_{k+1}\|^2=\frac{g(b_1;\ldots;b_k;x)}{g(b_1;\ldots;b_k)}=\frac{g(a_1;\ldots;a_k;x)}{g(a_1;\ldots;a_k)}$
        \end{Proof}
        \dfn \undercolor{red}{Расстоянием} от $x$ \undercolor{red}{до линейного многообразия} $P$ называется $\operatorname{dist}(x;P)=\min\limits_{u\in P}\|x-u\|$.
        \thm Минимум достигается и равен длине перпендикуляра $x-x_0$ на $L$.
        \begin{Proof}
            Пусть $P=x_0+L$. Тогда $\operatorname{dist}(x;P)=\min\limits_{l\in L}\|x-x_0+p\|$. Пусть $x-x_0=y+z$, где $y\in L$, $z\in L^\perp$. Тогда $\operatorname{dist}(x;P)=\min\limits_{l\in L}\|y+z+l\|=\min\limits_{l\in L}\sqrt{\|y+l\|^2+\|z\|^2}$. Объективно, минимум достигается при $y=-l$, и равен он $\|z\|$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема о расстоянии до многообразия}. Если $L=\Lin\{a_1;\ldots;a_k\}$, то
        $$
        (\operatorname{dist}(x;x_0+L))^2=\frac{g(a_1;\ldots;a_k;x-x_0)}{g(a_1;\ldots;a_k)}
        $$
        \dfn \undercolor{red}{Расстоянием между многообразиями} $P_1$ и $P_2$ называется $\min\limits_{\substack{u_1\in P_1\\u_2\in P_2}}\|u_1-u_2\|$.
        \thm Минимум достигается и равен $\operatorname{dist}(x_1-x_2;L_1+L_2)$.
        \begin{Proof}
            Пусть $P_1=x_1+L_1$, $P_2=x_2+L_2$. Тогда $\min\limits_{\substack{u_1\in P_1\\u_2\in P_2}}\|u_1-u_2\|=\min\limits_{\substack{l_1\in L_1\\l_2\in L_2}}\|x_1-x_2+l_1-l_2\|=\min\limits_{l\in L_1+L_2}\|x_1-x_2+l\|=\operatorname{dist}(x_1-x_2;L_1+L_2)$.
        \end{Proof}
        \begin{Example}
            Когда мы брали две скрещивающиеся прямые, мы как раз получали $\frac{|\vec{s_1}\vec{s_2}(\vec{x_1}-\vec{x_2})|}{|\vec{s_1}\times\vec{s_2}|}=\frac{\sqrt{g(\vec{s_1};\vec{s_2};\vec{x_1}-\vec{x_2})}}{\sqrt{g(\vec{s_1};\vec{s_2}}}=\operatorname{dist}(\vec{x_1}-\vec{x_2};\Lin\{\vec{s_1};\vec{s_2}\})$, то же самое, что и здесь.
        \end{Example}
        \dfn Два \undercolor{red}{подпространства} $L_1$, $L_2$ являются \undercolor{red}{ортогональными}, если любые два вектора $x_1\in L_1$, $x_2\in L_2$ ортогональны.
        \thm Пусть $L_1,L_2,\ldots,L_k$ --- попарно ортогональные пространства. Тогда все они дизъюнктны.
        \begin{Proof}
            Тривиально.
        \end{Proof}
        \dfn Пусть $V=\bigoplus_{i=1}^kL_i$ --- попарно ортогональные подпространства. Тогда проекторы на подпространства $L_i$ называются \undercolor{red}{ортогональными проекторами}.
        \thm Пусть $e_1,e_2,\ldots,e_n$ --- ортогональный базис $V$. Тогда $\mathrm x_i=\frac{\dotprod{x}{e_i}}{\dotprod{e_i}{e_i}}$.
        \begin{Proof}
            Тривиально.
        \end{Proof}
        \dfn Коэффициенты $\mathrm x_i=\frac{\dotprod{x}{e_i}}{\dotprod{e_i}{e_i}}$ называются \undercolor{red}{коэффициентами Фурье}.
        \thm \undercolor{darkgreen}{Тождество Парсеваля}. В ортогональном базисе $\|x\|^2=\sum\limits_{i=1}^n|\mathrm x_i|\|e_i\|^2$.
        \begin{Comment}
            В бесконечномерном пространстве оно верно не всегда, а верно лишь \undercolor{darkgreen}{неравенство Бесселя} $\|x\|^2\geqslant\sum\limits_{i=1}^n|\mathrm x_i|\|e_i\|^2$.
        \end{Comment}
        \thm Пусть $V=\bigoplus_{i=1}^mL_i$, и в $e_1,\ldots,e_n$ --- ортонормированный базис, составленный из ортонормированных базисов $L_i$. Тогда $\scriptP_i=\sum\limits_{e_j\in L_i}\dotprod{\cdot}{e_j}e_j$.
        \begin{Proof}
            Тривиально следует из формулы коэффициентов Фурье.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Изометрия евклидовых (унитарных) пространств}. \undercolorblack{orange}{Теорема Рисса}. \undercolorblack{orange}{Естественный изоморфизм}. \undercolorblack{orange}{$V$ и $V^*$ в евклидовом пространстве}.}
    \begin{itemize}
        \dfn Пусть $U$ и $V$ --- евклидовы/унитарные пространства со скалярными произведениями $\dotprod{\cdot}{\cdot}_U$ и $\dotprod{\cdot}{\cdot}_V$ соответственно. Тогда $U$ и $V$ \undercolor{red}{изометричны}, если $U\cong V$ и
        $$
        \forall u_1,u_2\in U~\dotprod{u_1}{u_2}_U=\dotprod{\varphi(u_1)}{\varphi(u_2)}_V
        $$
        \begin{Comment}
            Почему это называется <<изометричны>>? Ну, потому что сохранение скалярного произведения --- сохранение метрики.
        \end{Comment}
        \thm Любая пара евклидовых/унитарных конечномерных пространств одной размерности изомерна.
        \begin{Proof}
            Давайте возьмём $e_1;\ldots;e_n$ --- ортонормированный базис $U$, $e'_1;\ldots;e'_n$ --- ортонормированный базис $V$. Тогда, как мы знаем, между $U$ и $V$ есть координатный изоморфизм. Осталось лишь доказать, что он согласован со скалярным произведением. Ну, действительно согласован, потому что $\dotprod{x}{y}$ в обоих пространствах --- сумма произведений координат, а координаты одни и те же.
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема Рисса}. Пусть $V$, $V^*$ --- сопряжённые пространства. Тогда $\forall f\in V^*~\exists!y\in V~\forall x\in V~f(x)=\dotprod{x}{y}$.
        \begin{Proof}
            Сначала докажем единственность. Пусть таких два: $y_1$, $y_2$. Тогда $\forall x\in V~f(x)=\dotprod{x}{y_1}=\dotprod{x}{y_2}$. Тогда $\forall x\in V~\dotprod{x}{y_1-y_2}=0$. Я знаю только один вектор, ортогональный любому вектору $V$ --- $\mathbb0$, а значит $y_1=y_2$.\\
            Теперь существование. Докажем конструктивно. Пусть $e_1;\ldots;e_n$ --- ортонормированный базис $V$. Тогда, как мы знаем, $f(x)=\sum\limits_{i=1}^n\mathrm x_if(e_i)$. Ну так положим $\overline{\mathrm y_i}=f(e_i)$. Тогда получим, что $f(x)=\sum\limits_{i=1}^n\mathrm x_i\overline{\mathrm y_i}=\dotprod{x}{y}$.
        \end{Proof}
        \begin{Comment}
            Также несложно заметить, что не только любая форма --- это скалярное произведение с некоторым элементом, но также скалярное произведение с любым элементом --- это форма. А значит, мы получили взаимооднозначное соответствие между $V$ и $V^*$.\\
            Кстати, это отображение будет естественным изоморфизмом в евклидовом пространстве, а в унитарном будет проблема с псевдо-линейностью.
        \end{Comment}
        \thm На $V^*$ можно ввести скалярное произведение так, чтобы полученный в комментарии выше изоморфизм был изометрией.
        \begin{Proof}
            В качестве скалярного произведения $\dotprod{f}{g}$ берём $\dotprod{\varphi^{-1}(f)}{\varphi^{-1}(g)}$.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Тензоры в евклидовом пространстве}. \undercolorblack{orange}{Метрические тензоры}. \undercolorblack{orange}{Взаимные базисы}. \undercolorblack{orange}{Операции}\\\undercolorblack{orange}{поднятия и опускания индекса}. \undercolorblack{orange}{Евклидовы тензоры}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            В рамках этого пункта имеется исключительно евклидово пространства. Не унитарное.
        \end{Comment}
        \dfn Пусть $e_1;\ldots;e_n$ --- базис $V$, $\Gamma=G(e_1;\ldots;e_n)=\dotprod{e_i}{e_j}$. \undercolor{red}{Метрическим ковариантным тензором} называется $\Gamma$.
        \begin{Proof}
            Надо доказать, что эта матрица согласована с преобразованием координат. Ну, возьмём второй базис ($e_1';\ldots;e_n'$). Получим $\Gamma'$. Как мы знаем, $\Gamma'=T^T\Gamma T$. То есть $g'_{\widetilde i\widetilde j}=t^i_{\widetilde i}g_{ij}t^j_{\widetilde j}$. Хм, нам, кажется, это и нужно было.
        \end{Proof}
        \dfn \undercolor{red}{Метрическим контрвариантным тензором} называется $\Gamma^{-1}$.
        \begin{Proof}
            Проверьте согласованность с преобразованием координат сами.
        \end{Proof}
        \begin{Comment}
            Дальше $g_{ij}$ --- элементы $\Gamma$, $g^{ij}$ --- элементы $\Gamma^{-1}$.
        \end{Comment}
        \thm Метрические тензоры симметричны.
        \thm $g^{ij}g_{jk}=\delta^i_k=g_{ij}g^{jk}$.
        \thm $\dotprod{x}{y}=g_{ij}\mathrm x^i\mathrm y^j$.
        \begin{Proof}
            Запись уже известного нам в другой форме.
        \end{Proof}
        \dfn Пусть $e_1;\ldots;e_n$ и $e^1;\ldots;e^n$ --- базисы $V$. Они называются \undercolor{red}{взаимными}, если $\dotprod{e_i}{e^j}=\delta_i^j$.
        \thm У любого базиса $e_1;\ldots;e_n$ существует единственный взаимный ему.
        \begin{Proof}
            Всё что нам нужно --- матрица перехода. То есть такую матрицу $T_{e_i\to e^j}$, что $\matr{e^1&\ldots&e^n}=\matr{e_1&\ldots&e_n}T$. То есть $e^j=T^{ij}e_i$. Тогда домножим это скалярно на $e_k$. Хочется, чтобы $T^{ij}\dotprod{e_i}{e_k}$ было равно $\delta^j_k$. Но подождите, это равносильно $\delta^{jk}=T^{ij}g_{ik}$. Ну, а тут уже всё понятно, такое $T$ существует ровно одно и равно $\Gamma^{-1}$.
        \end{Proof}
        \thm $\Gamma^{-1}=G(e^1;\ldots;e^n)$.
        \begin{Proof}
            То есть надо проверить, что $\Gamma^{-1}=\dotprod{e^i}{e^j}$. Ну, $\dotprod{e^i}{g^{jk}e_k}=g^{jk}\dotprod{e^i}{e_k}=g^{jk}\delta^i_k=g^{ji}=g^{ij}$
        \end{Proof}
        \thm Если $e_1;\ldots;e_n$ --- ортонормированный базис, то взаимный ему равен ему же.
        \dfn Два базиса называются одинаково ориентированными, если матрица перехода между ними имеет положительный определитель.
        \thm Несложно заметить, что <<быть одинаково ориентированными>> --- отношение эквивалентности, которое разбивает все базисы на 2 класса.
        \thm Взаимные базисы одинаково ориентированы.
        \begin{Proof}
            Матрица перехода между ними --- матрица Грама, а её определитель всегда положителен.
        \end{Proof}
        \thm Пусть $e_1;\ldots;e_n$ --- базис $V$, $\omega^1;\ldots;\omega^n$ --- сопряжённый ему базис $V^*$. Тогда, базис $e^1;\ldots;e^n$ получаемый из $\omega^1;\ldots;\omega^n$ при помощи теоремы Рисса, взаимен к $e_1;\ldots;e_n$.
        \begin{Proof}
            $e^i$ --- это такой вектор, что $\omega^i(x)=\dotprod{x}{e^i}$. Ну, лол, поскольку $\omega^i(e_j)=\delta^i_j$ (определение сопряжённых базисов), получаем $\dotprod{e_j}{e^i}=\delta^i_j$. Это ли не то, что нам надо.
        \end{Proof}
        \begin{Comment}
            Пусть $e_1;\ldots;e_n$ --- ортонормированный базис. Тогда $\mathrm x^i=\dotprod{x}{e_i}$, и в то же время, $\omega^i(x)=\mathrm x^i$. А это значит, что $e_1;\ldots;e_n$ взаимен сам к себе. Тут мы получаем подтверждение одного и утверждений выше.
        \end{Comment}
        \begin{Comment}
            Не ст\'{о}ит путать матрицу Грама скалярного произведения и матрицу Грама $G(e_1;\ldots;e_n)$. Они могут быть разными.
        \end{Comment}
        \thm \undercolor{darkgreen}{Формулы Гибса}. Пусть $e_i$, $e^j$ --- взаимные базисы. Тогда $x=\mathrm x^ie_i$ и $x=\mathrm x_je^j$. Тогда $\dotprod{x}{e^i}=\dotprod{\mathrm x^je_j}{e^i}=\mathrm x^i$. Аналогично, $\dotprod{x}{e_i}=\mathrm x_i$. Тогда $x=\dotprod{x}{e^j}e_j=\dotprod{x}{e_j}e^j$.
        \begin{Comment}
            Не путайте это с формулами Фурье, там базисы разные были.
        \end{Comment}
        \dfn Если $e_i$, $e^j$ --- взаимные базисы, то $\mathrm x^i$ --- \undercolor{red}{контрвариантные координаты}, $\mathrm x_j$ --- \undercolor{red}{ковариантные координаты}.
        \thm Контрвариантные и ковариантные координаты действительно являются таковыми.
        \begin{Proof}
            Про контрвариантные всё понятно, это обычные координаты. А что такое $\mathrm x_j$? Это $\dotprod{x}{e_j}=\omega^j(x)$. Хм-м-м-м.... Это же координаты в базисе $\omega^i$, а они ковариантны.
        \end{Proof}
        \thm Рассмотрим $\mathrm x_i$ и $\mathrm x^j$ как тензоры. Тогда $\mathrm x^ig_{ij}=\mathrm x^i\dotprod{e_i}{e_j}=\dotprod{x}{e_j}=\mathrm x_j$. Аналогично, $\mathrm x_ig^{ij}=\mathrm x^j$. Указанные операции называются \undercolor{darkgreen}{опусканием} и \undercolor{darkgreen}{поднятием индекса} соответственно. Да и вообще это работает с любым тензором (проверьте сами).
        \begin{Comment}
            Проблема в том, что непонятно, куда поднимать новый индекс, если какие-то индексы уже были сверху. Так вот, дело тут такое. Если вы поднимаете самый левый индекс, вы поднимаете его самое правое место. Если же вы поднимаете не самый левый, то на его метсе надо поставить точку. И когда вы будете читать тензор, вы должны увидеть точку снизу и читать вместо точек поднятые индексы. Например, $\alpha^{ijl}_{k\bullet}$ --- это всё равно сначала $i$, потом $j$, потом $k$, потом $l$. Когда вы поднимаете сразу несколько индексов, сверху они должны идти как были расположены точки. То есть если мы в указанном выше тензоры $k$ тоже поднимем, он будет \textbf{до} $l$, а не после.\\
            С опусканием индекса то же самое --- крайний правый верхний индекс опускается максимально влево нормально, остальные опускаются влево с точкой.
        \end{Comment}
        \begin{Comment}
            Если $e_1;\ldots;e_n$ --- ортонормированный базис, то $\Gamma=E=\Gamma^{-1}$. А значит опускание и/или поднятие  индекса в тензоре будет давать ту же матрицу.
        \end{Comment}
        \begin{Comment}
            Пусть $e_1;\ldots;e_n$ --- один ортонормированный базис, а $e_1';\ldots;e_n'$ --- другой ортонормированный базис. Тогда, если матрица перехода между ними $T=T_{e\to e'}$, то матрица $S$ равна $T^T$. А как в таком случае выглядит преобразование тензора в координатах? Ну,
            $$
            {\alpha'}^{\widetilde{i_1}\cdots\widetilde{i_q}}_{\widetilde{j_1}\cdots\widetilde{j_p}}=\alpha^{i_1\cdots i_q}_{j_1\cdots j_p}t^{j_1}_{\widetilde{j_1}}\cdots t^{j_p}_{\widetilde{j_p}}s_{i_1}^{\widetilde{i_1}}\cdots s_{i_q}^{\widetilde{i_q}}=
            \sum\limits_{i_1}\cdots\sum\limits_{i_q}\alpha^{i_1\cdots i_q}_{j_1\cdots j_p}t^{j_1}_{\widetilde{j_1}}\cdots t^{j_p}_{\widetilde{j_p}}t^{i_1}_{\widetilde{i_1}}\cdots t^{i_q}_{\widetilde{i_q}}
            $$
            В конце написана сумма явно, потому что не работает правило Эйнштейна, увы.\\
            Что мы получили? Что тензору вообще похую, какие индексы у него нижние, а какие --- верхние, они все одинаково преобразуются. Можно записать все как верхние и преобразовывать матрицей $T$ или все как нижние и преобразовывать матрицей $T^T$.
        \end{Comment}
        \dfn Если в результате приведения тензора к некоторому ортонормированному базису получается одна и та же матрица, этот тензор называется \undercolor{red}{евклидовым}.
    \end{itemize}
\undef{\Im}
\let\Im\operIm
    \section{Линейные операторы в евклидовом/унитарном пространстве.}
    \paragraph{\undercolorblack{orange}{Сопряжённый оператор}.}
    \begin{itemize}
        \dfn Пусть $U$, $V$ --- линейные пространства, $\scriptA\in\Hom(U;V)$. Тогда оператор $\scriptA^*\colon V^*\to U^*$ такой что $\scriptA^*f(x)=f(\scriptA x)$ никак не называется, но обозначение $\scriptA^*$ нам понадобится.
        \thm Тривиально, $\scriptA\in\Hom(V^*;U^*)$.
        \dfn Пусть $(V;\dotprod{\cdot}{\cdot})$ --- евклидово/унитарное пространство. Пусть $\scriptA\in\End(V)$. $\forall f\in V^*~\exists!y\in V~\forall x\in V~f(x)=\dotprod{x}{y}$ --- это теорема Рисса. Но также $\forall g\in V^*~\exists!z\in V~g(u)=\dotprod{u}{z}$. Тогда $\dotprod{x}{z}=\scriptA f(x)=f(\scriptA x)=\dotprod{\scriptA x}{y}$. Итого мы взаимооднозначно сопоставили $g$ и $z$, сопоставили $f$ и $y$, а ещё у нас есть $\scriptA^*$, которое сопоставляет $y$ и $f$, а также $z$ и $g$. А это значит, что $\scriptA^*y=z$.\\
        То есть $\scriptA^*\colon V\to V$ такой что $\forall x,y\in V~\dotprod{\scriptA x}{y}=\dotprod{x}{\scriptA^*y}$ называется \undercolor{red}{сопряжённым оператором}.
        \thm Пусть $e_1;\ldots;e_n$ --- базис $V$, $\Gamma=G(e_1;\ldots;e_n)$. Тогда $\scriptA^*$ соответствует матрица $\overline{\Gamma^{-1}}A^*\overline\Gamma$.
        \begin{Comment}
            Матрицу сопряжённого оператора в произвольном (не обязательно ортонормированном) базисе обозначают $A^\circledast$.
        \end{Comment}
        \begin{Proof}
            Достаточно расписать скалярное произведение.
            $$
            \forall x,y\in V~\dotprod{\scriptA x}{y}=(A\mathrm x)^T\Gamma\overline{\mathrm y}=\mathrm x^TA^T\Gamma\overline{\mathrm y}
            $$
            С другой стороны
            $$
            \forall x,y\in V~\dotprod{x}{\scriptA^*y}=\mathrm x^T\Gamma\overline{A^\circledast\mathrm y}=\mathrm x^T\Gamma\overline{A^\circledast}\overline{\mathrm y}
            $$
            В качестве векторов $x$ и $y$ возьмём базисные вектора $e_i$ и $e_j$. Тогда получим $(A^T\Gamma)_{ij}=(\Gamma\overline{A^\circledast})_{ij}$. Отсюда $A^\circledast=\overline{\Gamma^{-1}A^T\Gamma}=\overline{\Gamma^{-1}}A^*\overline{\Gamma}$
        \end{Proof}
        \begin{Comment}
            В частности, в ортонормированном базисе $A^\circledast=A^*$.
        \end{Comment}
        \thm ${\scriptA^*}^*=\scriptA$.
        \begin{Proof}
            Тривиально. Известно, что $\dotprod{\scriptA x}{y}=\dotprod{x}{\scriptA^*y}$. Следовательно, $\overline{\dotprod{\scriptA x}{y}}=\overline{\dotprod{x}{\scriptA^*y}}$ или $\dotprod{y}{\scriptA x}=\dotprod{\scriptA^*y}{x}$. А это и значит, что ${\scriptA^*}^*=\scriptA$.
        \end{Proof}
        \thm $(\scriptA\scriptB)^*=\scriptB^*\scriptA^*$.
        \begin{Proof}
            Хочется найти оператор, сопряжённый $\scriptA\scriptB$. То есть узнать, чему равно $\dotprod{\scriptA\scriptB x}{y}$. Ну,
            $$\dotprod{\scriptA\scriptB x}{y}=\dotprod{\scriptB x}{\scriptA^*y}=\dotprod{x}{\scriptB^*\scriptA^*y}$$
        \end{Proof}
        \thm $(\scriptA+\scriptB)^*=\scriptA^*+\scriptB^*$.
        \thm $(\lambda\scriptA)^*=\overline\lambda\scriptA^*$.
        \begin{Proof}
            Тривиально из псевдолинейности скалярного произведения и линейности операторов.
        \end{Proof}
        \thm $\ker\scriptA^*=(\Im\scriptA)^\perp$, $\Im\scriptA^*=(\ker\scriptA)^\perp$.
        \begin{Proof}
            Докажем второе утверждение, первое следует него по свойству о ${\scriptA^*}^*$.\\
            Рассмотрим $x\in\ker\scriptA$. Тогда $\dotprod{x}{\scriptA^*y}=\dotprod{\scriptA x}{y}=\dotprod{\mathbb0}{y}=0$. Следовательно, $\scriptA^*y\in(\ker\scriptA)^\perp$. А $\scriptA^*y$ пробегает все значения $\in\Im\scriptA^*$. Отсюда $\Im\scriptA^*\subset(\ker\scriptA)^\perp$.\\
            Обратное включение доказывать не хочется, вместо этого докажем равенство размерностей. $V=\ker\scriptA\oplus(\ker\scriptA)^\perp$. По теореме о ранге и дефекте $\dim\ker\scriptA=n-\rank\scriptA$. Если мы докажем, что $\rank A^\circledast=\rank A$, это даст нам то, что мы хотим. Ну, а это верно. Почему? А вот почему
            $$
            A^\circledast=\overline{\Gamma^{-1}A^T\Gamma}
            $$
            Поскольку $\Gamma$ и $\Gamma^{-1}$ --- невырожденные матрицы, $\rank A=\rank A^T=\rank\Gamma^{-1}A^T\Gamma$. Совпадает ли ранг матрицы с рангом её комплексного сопряжения? Да, совпадает, ведь мы доказывали, что линейная оболочка векторов равна линейной оболочке их сопряжений. А значит и правда $\rank A^\circledast=\rank A$.
        \end{Proof}
        \thm Если $\exists\scriptA^{-1}$, то $\exists(\scriptA^*)^{-1}=(\scriptA^{-1})^*$.
        \begin{Proof}
            Известно, что $\id=\scriptA\scriptA^{-1}$. Тогда $\id^*=(\scriptA\scriptA^{-1})^*=(\scriptA^{-1})^*\scriptA^*$, а $\id^*=\id$. А значит мы нашли такой оператор, что будучи умноженным на $\scriptA$, он даст $\id$ (аналогично можно доказать и с умножением в другом порядке). А значит мы явно нашли обратный к $\scriptA^*$, нашли, чему он равен.
        \end{Proof}
        \thm $\chi_\scriptA(\lambda)\Longleftrightarrow\chi_{\scriptA^*}(\overline\lambda)$.
        \begin{Proof}
            Поскольку характеристический многочлен не зависит от базиса, будем считать, что базис у нас ортонормированный. Тогда $A^\circledast=A^*$.\\
            Пусть $\chi_\scriptA(t)=\det(A-tE)=0$. Следовательно $\overline{\chi_\scriptA(t)}=\overline{\det(A-tE)}=\overline0=0$. При этом $\overline{\det(A-tE)}=\det(\overline A-\overline tE)=\det({\overline A}^T-\overline tE)=\chi_{\scriptA^*}(\overline\lambda)$.
        \end{Proof}
        \thm Если $\lambda$ --- собственное число $\scriptA$ для вектора $u$, $\mu$ --- собственное число $\scriptA^*$ для вектора $v$. Пусть $\lambda\neq\overline\mu$. Тогда $u\perp v$.
        \begin{Proof}
            $$
            \dotprod{\scriptA u}{v}=\dotprod{\lambda u}{v}=\lambda\dotprod{u}{v}
            $$
            С другой стороны
            $$
            \dotprod{\scriptA u}{v}=\dotprod{u}{\scriptA^*v}=\dotprod{u}{\mu v}=\overline\mu\dotprod{u}{v}
            $$
            Вычтем одно из другого. Получим
            $$
            (\lambda-\overline\mu)\dotprod{u}{v}=0
            $$
            Первое нулю не равно, значит равно второе.
        \end{Proof}
        \thm Пусть $L\subset V$ инвариантно относительно $\scriptA$. Тогда $L^\perp$ инвариантно относительно $\scriptA^*$.
        \begin{Proof}
            Пусть $x\in L$, $y\in L^\perp$. Тогда $0=\dotprod{\underbrace{\scriptA x}_{\in L}}{y}=\dotprod{x}{\scriptA^*y}$, следовательно $\scriptA^*y\in L^\perp$ или $L^\perp$ инвариантно относительно $\scriptA^*$.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Нормальные операторы в евклидовом/унитарном пространстве}.}
    \begin{itemize}
        \dfn $\scriptA\in\End(V)$ называется \undercolor{red}{нормальным оператором}, если $\scriptA\scriptA^*=\scriptA^*\scriptA$.
        \begin{Comment}
            В случае евклидова/унитарного пространства получаем
            \[
            \dotprod{\scriptA x}{\scriptA y}=\dotprod{x}{\scriptA^*\scriptA y}=\dotprod{x}{\scriptA\scriptA^*y}=\dotprod{\scriptA^*x}{\scriptA^*y}
            \]
        \end{Comment}
        \thm В евклидовом/унитарном пространстве оператор нормальный тогда и только тогда, когда $\dotprod{\scriptA x}{\scriptA y}=\dotprod{\scriptA^*x}{\scriptA^*y}$.
        \thm $\scriptA$ нормальный тогда и только тогда, когда в некотором базисе $AA^\circledast=A^\circledast A$.
        \begin{Proof}
            Следствие вправо очевидно --- если оператор нормальный, его матрицы перестановочные в любом базисе.\\
            Следствие влево: пусть $e_1;\ldots;e_n$ --- базис, в котором $AA^\circledast=A^\circledast A$. Докажем, что это верно в любом базисе. Пусть $e_1';\ldots;e_n'$ --- некоторый другой базис. В нём $\scriptA$ сопоставлено $A'$, а $\scriptA^*$ --- ${A'}^\circledast$. Пусть $T=T_{e\to e'}$. Тогда
            $$
            A'{A'}^\circledast=T^{-1}ATT^{-1}A^\circledast T=T^{-1}AA^\circledast T=T^{-1}A^\circledast AT=T^{-1}A^\circledast TT^{-1}AT={A'}^\circledast A'
            $$
            А отсюда уже следует аналогичное свойство для операторов.
        \end{Proof}
        \thm Если $\scriptA$ нормальный, то $\forall\lambda\in\scriptK~\scriptB=\scriptA-\lambda\id$ также нормальный.
        \begin{Proof}
            $\scriptB^*=(\scriptA-\lambda\id)^*=\scriptA^*-\overline\lambda\id$.\\
            \[\begin{split}
                \scriptB\scriptB^*&=(\scriptA-\lambda\id)(\scriptA^*-\overline\lambda\id)=\scriptA\scriptA^*-\lambda\scriptA^*-\overline\lambda\scriptA+|\lambda|^2\id\\
                \scriptB^*\scriptB&=(\scriptA^*-\overline\lambda\id)(\scriptA-\lambda\id)=\scriptA^*\scriptA-\overline\lambda\scriptA-\lambda\scriptA^*+|\lambda|^2\id
            \end{split}\]
        \end{Proof}
        \thm Пусть $\scriptA$ --- нормальный оператор, $\lambda$ --- собственное число $\scriptA$ для собственного вектора $u$. Тогда мы уже знаем, что $\overline\lambda$ --- собственное число $\scriptA^*$, но сейчас докажем, что $u$ --- собственный вектор, соответствующий $\overline\lambda$.
        \begin{Proof}
            $$
            u\in\ker(\underbrace{\scriptA-\lambda\id}_\scriptB)\Leftrightarrow\scriptB u=\mathbb0\Leftrightarrow\dotprod{\scriptB u}{\scriptB u}=0\Leftrightarrow\dotprod{\scriptB^*u}{\scriptB^*u}=0\Leftrightarrow\scriptB^*u=\mathbb0\Leftrightarrow u\in\ker\scriptB^*=\ker(\scriptA^*-\overline\lambda\id)
            $$
        \end{Proof}
        \thm Пусть $\scriptA$ нормальный, $\lambda$ --- собственное число $\scriptA$ соответствующее собственному вектору $u$, $\mu$ --- собственное число $\scriptA$ соответствующее собственному вектору $v$, $\lambda\neq\mu$. Тогда $u\perp v$.
        \begin{Proof}
            $$
            \dotprod{\scriptA u}{v}=\lambda\dotprod uv
            $$
            С другой стороны
            $$
            \dotprod{\scriptA u}{v}=\dotprod{u}{\scriptA^*v}\overset{\text{собственные вектора совпадают}}=\dotprod{u}{\overline\mu v}=\mu\dotprod{u}{v}
            $$
            Дальше всё уже было.
        \end{Proof}
        \begin{Comment}
            Отсюда у нормального оператора все $V_\lambda$ попарно ортогональны.
        \end{Comment}
        \thm Пусть $\scriptA$ нормальный тогда $\ker\scriptA=(\Im\scriptA)^\perp$, $\Im\scriptA=(\ker\scriptA)^\perp$ и $\ker\scriptA=\ker\scriptA^2$.
        \begin{Proof}
            Возьмём $x\in\ker\scriptA$. Тогда
            $$
            \scriptA x=\mathbb0\Leftrightarrow\dotprod{\scriptA x}{\scriptA x}=0\Leftrightarrow\dotprod{\scriptA^*x}{\scriptA^*x}\Leftrightarrow\scriptA^*x=\mathbb0
            $$
            То есть $\ker\scriptA=\ker\scriptA^*$. Первое два свойства доказаны.
            \begin{Comment}
                Отсюда $V=\ker\scriptA\oplus\underbrace{\Im\scriptA}_{(\ker\scriptA)^\perp}$.
            \end{Comment}
            Теперь оставшееся свойство. Тривиально, $\ker\scriptA\subset\ker\scriptA^2$. Докажем обратное включение. Рассмотрим $x\in\ker\scriptA^2$. Тогда
            $$
            \scriptA^2x=\mathbb0\Leftrightarrow\dotprod{\scriptA^2x}{\scriptA^2x}=0\Leftrightarrow\dotprod{\scriptA^*\scriptA x}{\scriptA^*\scriptA x}=0\Leftrightarrow\scriptA x\in\ker\scriptA^*\Leftrightarrow\scriptA x\in\ker\scriptA
            $$
            Хм, смотрите. $\scriptA x\in\ker\scriptA$. И в то же время $\scriptA x\in\Im\scriptA$. Но эти пространства дизъюнктны, значит $\scriptA x=\mathbb0\Leftrightarrow x\in\ker\scriptA$. Вот и обратное включение.
        \end{Proof}
        \thm \undercolor{darkgreen}{Канонический вид матрицы нормального оператора в унитарном пространстве}. $\forall\scriptA\in\End(V)$. Тогда $\scriptA$ является нормальным тогда и только тогда, когда $\exists$ортонормированный базис, в котором матрица $\scriptA$ будет иметь диагональный вид.
        \begin{Comment}
            Тогда, несложно заметить, матрица $\scriptA^*$ также будет диагональной, а на диагонали будут стоять числа, сопряжённые аналогичным в матрице $\scriptA$.
        \end{Comment}
        \begin{Comment}
            Если матрица оператора $\scriptA$ имеет диагональный вид, то во-первых, базис --- это базис собственных векторов (как мы знаем, ортонормированный), во вторых числа на диагонали --- собственные числа.\\
            Более того, по комментарию выше, тот же базис --- ортонормированный базис собственных векторов $\scriptA^*$.
        \end{Comment}
        \begin{Comment}
            Получаем, что любой нормальный оператор имеет простую структуру. Обратное неверно, потому что во-первых скалярное произведение может быть не задано, а во-вторых, даже если задано, собственные подпространства произвольного о.п.с. могут быть не ортогональны.
        \end{Comment}
        \begin{Proof}
            Следствие влево. Если такой базис есть то $A=\Lambda$, $A^\circledast=\Lambda^*$. Тогда $\Lambda\overline{\Lambda^T}=\overline{\Lambda^T}\Lambda$. Мы наши базис, в котором матрицы $\scriptA$ и $\scriptA^*$ перестановочные. У нас было свойство об этом.\\
            Следствие вправо. Пусть $\lambda$ --- собственное число $\scriptA$, соответствующее собственному вектору $u$. Тогда $V_1=\Lin\{u\}$ инвариантно относительно $\scriptA$. Также оно инвариантно относительно $\scriptA^*$, поскольку $u$ --- собственный вектор $\scriptA^*$. В таком случае $V_1^\perp$ инвариантно относительно $\scriptA$ и $\scriptA^*$. Тогда $\scriptA\Big|_{V_1^\perp}$ также нормальный.\\
            Давайте проведём математическую индукцию по размерности $V$. Если $\dim V=1$, то очевидно. Пусть для $\dim V=k$ мы доказали, докажем для $\dim V=k+1$. Тогда $V=V_1\oplus V_1^\perp$. Для $V_1^\perp$ оператор $\scriptA\Big|_{V_1^\perp}$ является нормальным, значит существует ортонормированный базис в нём, в котором $\scriptA\Big|_{V_1^\perp}$ имеет диагональную матрицу. Добавим в этот ортогональный базис $u$. Он останется ортогональным, потому что $u\in V_1\perp V_1^\perp$. И матрица останется диагональной, поскольку она будет блочной (один блок $1\times 1$, другой --- диагональный размерности $k\times k$).
        \end{Proof}
        \thm Если $A$ --- нормальная матрица в комплексном пространстве ($AA^*=A^*A$), то существует унитарная матрица $T$, такая что $T^*AT$ имеет диагональный вид ($\Lambda=\diag(\lambda_1;\ldots;\lambda_n)$, где $\lambda_i$ --- собственные числа $A$).
        \begin{Proof}
            $A$ --- матрица $\scriptA$ в каноническом (ортонормированном) базисе. Тогда $\scriptA$ --- нормальный оператор, а значит для него верна теорема о каноническом виде матрицы нормального оператора в унитарном пространстве. То есть существует базис $v_1;\ldots;v_n$, в котором матрица имеет диагональный вид. Эта самая матрица равна $T^{-1}AT$, где $T=T_{e\to e'}$. При этом, поскольку $v_1;\ldots;v_n$ --- о.н.б., $T$ унитарная матрица, а значит $T^{-1}=T^*$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема о каноническом виде матрицы нормального оператора в евклидовом пространстве}. Пусть $\scriptA\in\End(V)$. Тогда $\scriptA$ является нормальным оператором тогда и только тогда, когда существует такой о.н.б., что матрица оператора в нём имеет следующий блочно-диагональный вид:
        $$
        \Lambda=\arr{ccc|c|c|c}{
            \lambda_1 & \cdots & 0 & 0 & \cdots & 0\\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
            0 & \cdots & \lambda_k & 0 & \cdots & 0\\
            \hline
            0 & \cdots & 0 & \Phi_1 & \cdots 0\\
            \hline
            \vdots & \ddots & \vdots & \vdots & \ddots \vdots\\
            \hline
            0 & \cdots & 0 & 0 & \cdots \Phi_m\\
        }
        $$
        Где $\lambda_i\in\mathbb R$, $\Phi_r=\matr{\alpha_r & \beta_r\\-\beta_r & \alpha_r}$.
        \begin{Comment}
            Мы получим, что $\lambda_i$ --- собственные числа $\scriptA$, $\alpha_r\pm\im\beta_r$ --- пара комплексно сопряжённых корней $\chi_{\scriptA}$, повторяющийся столько раз, чему равна кратность этой пары.
        \end{Comment}
        \begin{Comment}
            По сути, тут всё идёт через комплексификацию. Но нам нужно сначала зафиксировать несколько её свойств в евклидовом пространстве. И только после этого докажем теорему.
        \end{Comment}
        \dfn Пусть $V$ --- евклидово пространство. \undercolor{red}{Скалярное произведение в комплексификации} $\dotprod zw$, где $z,w\in V_{\mathbb C}$ равно $\dotprod{\Re z}{\Re w}+\dotprod{\complexIm z}{\complexIm w}+\im\dotprod{\complexIm z}{\Re w}-\im\dotprod{\Re z}{\complexIm w}$.
        \begin{Comment}
            Или, более наглядно, $\dotprod{x+\im y}{u+\im v}=\dotprod{x}{u}+\dotprod{y}{v}+\im\dotprod{y}{u}-\im\dotprod{x}{v}$.
        \end{Comment}
        \thm Тривиально проверяется, что это действительно псевдоскалярное произведение.
        \thm $\overline{\dotprod{z_1}{z_2}}=\dotprod{\overline{z_1}}{\overline{z_2}}$.
        \thm Лемма:
        \begin{enumerate}
            \item Если $\lambda\in\mathbb R$ --- собственное число $\scriptA$, то $\lambda$ --- собственное число $\scriptA_{\mathbb C}$ и ${V_\lambda}_{\mathbb C}$ --- соответствующее ему собственное подпространство.
            \item Если $\mu$ --- собственное число $\scriptA_{\mathbb C}$, а $z$ --- собственный вектор, то $\overline\mu$ --- собственное число $\scriptA_{\mathbb C}$, а $\overline z$ --- собственный вектор. Причём если $\dotprod{z}{\overline z}=0$, то $\left\{\begin{aligned}
                \|\Re\mu\|=\|\complexIm\mu\|\\
                \dotprod{\Re\mu}{\complexIm\mu}=0
            \end{aligned}\right.$
            \item $(\scriptA_{\mathbb C})^*=(\scriptA^*)_{\mathbb C}$.
            \item Если $\scriptA$ нормальный, то $\scriptA_{\mathbb C}$ --- тоже.
        \end{enumerate}
        \begin{Proof}
            \begin{enumerate}
                \item Про то что $\lambda$ --- собственное число $\scriptA_{\mathbb C}$, мы уже знаем ($\chi_{\scriptA}$ и $\chi_{\scriptA_{\mathbb C}}$ имеют одни и те же корни).\\
                Чтобы доказать оставшуюся часть, рассмотрим $u,v\in V_\lambda$. Тогда $u+\im v$ --- собственный вектор $\scriptA_{\mathbb C}$, потому что $\scriptA_{\mathbb C}(u+\im v)=\scriptA u+\im\scriptA v=\lambda(u+\im v)$.\\
                Теперь рассмотрим $w$ --- собственный вектор для вещественного собственного числа $\lambda$ в $\scriptA_{\mathbb C}$. Тогда $\exists u,v\in V_\lambda$, где $u+\im v=w$. Ну, рассмотрим вещественную и комплексную часть $w$: $w=u+\im v$. Известно, что $\scriptA u+\im\scriptA v=\scriptA_{\mathbb C}w=\lambda w=\lambda(u+\im v)$. То есть $\scriptA u=\lambda u$, $\scriptA v=\lambda v$, а это значит $u,v\in V_\lambda$.\\
                Что мы получили? Да что надо получили, что собственное подпространство в $\scriptA_{\mathbb C}$ равно комплексификации аналогичного в $\scriptA$.
                \item Рассмотрим $\overline{\scriptA_{\mathbb C} z}$. Мы доказывали, что это равно $\scriptA_{\mathbb C}\overline z$. В то же время $\overline{\scriptA_{\mathbb C} z}=\overline\mu\overline z$, то есть $\overline z$ --- собственный вектор для собственного числа $\overline\mu$. Чтобы получить условие на $\Re z$ и $\complexIm z$, достаточно расписать $\dotprod{z}{\overline z}$ по определению.
                \item Пусть $e_1;\ldots;e_n$ --- базис $V$ (а значит и $V_{\mathbb C}$). Тогда пусть оператору $\scriptA$ соответствует матрица $A$. Тогда оператору $\scriptA_{\mathbb C}$ соответствует она же. А значит оператору $(\scriptA_{\mathbb C})^*$ соответствует $A^T$. В то же время, оператору $\scriptA^*$ соответствует $A^T$, а значит и оператору $(\scriptA^*)_{\mathbb C}$ --- тоже.
                \item Очевидно из предыдущего
            \end{enumerate}
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема о каноническом виде матрицы нормального оператора в евклидовом пространстве} (снова).\\
        Пусть $\scriptA\in\End(V)$. Тогда $\scriptA$ является нормальным оператором тогда и только тогда, когда существует такой о.н.б., что матрица оператора в нём имеет следующий блочно-диагональный вид:
        $$
        \Lambda=\arr{ccc|c|c|c}{
            \lambda_1 & \cdots & 0 & 0 & \cdots & 0\\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
            0 & \cdots & \lambda_k & 0 & \cdots & 0\\
            \hline
            0 & \cdots & 0 & \Phi_1 & \cdots & 0\\
            \hline
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
            \hline
            0 & \cdots & 0 & 0 & \cdots & \Phi_m\\
        }
        $$
        Где $\lambda_i\in\mathbb R$, $\Phi_r=\matr{\alpha_r & \beta_r\\-\beta_r & \alpha_r}$.
        \begin{Proof}
            Следствие влево опять простое: если мы имеем $\Lambda$ и $\Lambda^T$ указанного вида, то очевидно $\Lambda\Lambda^T=\Lambda^T\Lambda$ (проверьте лапками). Отсюда $\scriptA$ и $\scriptA^*$ перестановочны.\\
            Следствие вправо. Рассмотрим все $k$ вещественных собственных чисел $\scriptA$. Они дают нам ортогональные собственные подпространства $V_{\lambda_i}$. Нам этого не хватает, поэтому рассмотрим комплексификацию $\scriptA_{\mathbb C}$. Там мы получим $\bigoplus_\lambda(V_\lambda)_{\mathbb C}$ и ещё какие-то комплексно сопряжённые пары корней $\mu_j$ и $\overline{\mu_j}$. Тогда по теореме о каноническом виде матрицы нормального оператора в унитарном пространстве (а $V_{\mathbb C}$ унитарное)
            $$V_{\mathbb C}=\bigoplus_\lambda(V_\lambda)_{\mathbb C}\oplus\bigoplus_\mu(V_{\mu_j}\oplus V_{\overline{\mu_j}})$$
            Тогда пусть $\mu=\alpha+\im\beta$, $\overline\mu=\alpha-\im\beta$. Давайте рассмотрим собственные вектора для $\mu$ и $\overline\mu$ $z$ и $\overline z$. Поскольку $\scriptA_{\mathbb C}$ нормально, $z\perp\overline z$, а значит если $x+\im y=z$, то $\|x\|=\|y\|$ и $\dotprod xy=0$. Чему равно $\Lin_{\mathbb C}\{z;\overline z\}$? Оно равно $\Lin_{\mathbb C}\{x;y\}=\left(\Lin_{\mathbb R}\{x;y\}\right)_{\mathbb C}$. А отсюда
            $$
            V_\mathbb C=\bigoplus_\lambda(V_\lambda)_{\mathbb C}\oplus\bigoplus_j\Lin\{z_j;\overline{z_j}\}=\Lin_{\mathbb C}\{v_1;\ldots;v_k;x_1;y_1;\ldots;x_m;y_m\}
            $$
            Это вещественный базис пространства $V_{\mathbb C}$. А значит это и базис $V$. Матрица $\scriptA_{\mathbb C}$ совпадает с матрицей $\scriptA$ в этом базисе.\\
            Тогда посмотрим, чему равно $\scriptA_{\mathbb C}x$. Оно равно $\scriptA_{\mathbb C}\left(\frac{z+\overline z}2\right)=\frac{\mu z+\overline\mu\overline z}2=\cdots=\alpha x-\beta y$, то есть первый столбец $\Phi$ состоит из $\alpha$ и $-\beta$. Аналогично второй столбец состоит из $\beta$ и $\alpha$.\\
            Вопрос в том, почему базис ортонормированный? Ну, с $v$ понятно, мы их изначально ортонормированными получили. Почему $\dotprod{x_j}{y_j}=0$ мы тоже знаем, по лемме. Почему это выполнено для разных векторов, мы тоже знаем, потому что для $z_j$, соответствующих разным числам, имеет теорему, для одинаковых --- Грам --- Шмидт.
        \end{Proof}
        \thm Если $A$ --- нормальная вещественная матрица, то существует ортогональная матрица $Q$, такая что $Q^TAQ$ выглядит тем страшным образом, что выше.
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Самосопряжённые и изометрические операторы в евклидовом/унитарном пространстве}.}
    \begin{itemize}
        \dfn \undercolor{red}{Самосопряжённый оператор} --- такой оператор $\scriptA$, что $\scriptA^*=\scriptA$.
        \begin{Comment}
            В случае евклидова/унитарного пространства определение равносильно $\dotprod{\scriptA x}{y}=\dotprod{x}{\scriptA y}$.
        \end{Comment}
        \begin{Comment}
            В евклидовом пространстве самосопряжённый оператор называют симметричным, в унитарном --- эрмитовым.
        \end{Comment}
        \thm Самосопряжённый оператор нормален.
        \thm $\scriptA$ самосопряжённый тогда и только тогда, когда существует ортонормированный базис, в котором матрица оператора $\scriptA$ самосопряжена.
        \begin{Proof}
            Вправо --- очевидно, влево --- докажите, что в любом базисе это верно сами.
        \end{Proof}
        \thm Если $\scriptA$ и $\scriptB$ самосопряжены, то $\scriptA+\scriptB$ самосопряжён.
        \thm Если $\scriptA$ самосопряжен, $\underline{\lambda\in\mathbb R}$, то $\lambda\scriptA$ самосопряжён.
        \thm Если $\scriptA$ и $\scriptB$ самосопряжены и перестановочны, то $\scriptA\scriptB$ самосопряжён.
        \thm Если $\scriptA$ обратим и самосопряжён, то $\scriptA^{-1}$ самосопряжён.
        \begin{Proof}
            Всё выше --- очевидно.
        \end{Proof}
        \thm Оператор является самосопряжённым (вне зависимости от того, пространство над $\mathbb R$ или $\mathbb C$) тогда и только тогда, когда он нормальный и имеет вещественный спектр.
        \begin{Proof}
            Следствие вправо. Нормальность очевидна. По теоремам о каноническом виде я заебался это писать существует ортонормированный базис такой что матрица имеет форму.
            \begin{itemize}
                \item[Евклидово] $\Lambda^*=\Lambda^T$ выглядит также, как $\Lambda$, но только $\Phi_k$ транспонируются. $\Phi_k=\Phi_k^T$ когда $\beta_k=0$. А это нелегальный $\Phi_k$-блок, значит таких нет.
                \item[Унитарное] $\Lambda^*=\overline{\Lambda^T}=\overline{\Lambda}$. Ну, мы берём комплексное сопряжение всех комплексных корней, а получаем то же самое. Значит все они чисто вещественные.
            \end{itemize}
            В обратную сторону аналогично.
        \end{Proof}
        \thm Если $L\subset V$ инвариантно относительно $\scriptA$, то $L^\perp$ инвариантно относительно $\scriptA$.
        \thm \undercolor{darkgreen}{Теорема о каноническом виде матрицы самосопряжённого оператора} где угодно. Существует ортонормированный базис, где матрица самосопряжённого оператора вещественна и имеет диагональный вид.
        \begin{Proof}
            Смотри свойство выше и обе теоремы о каноническим виде матрицы нормального оператора.
        \end{Proof}
        \thm $\scriptA$ самосопряжено, когда все его собственные подпространства ортогональны и все собственные числа вещественны.
        \thm Если $A^*=A$, существует такая унитарная/ортогональная матрица $T$/$Q$, что $T^*AT$/$Q^TAQ$ диагональна и вещественна.
        \dfn Невырожденный оператор $\scriptQ$ называется \undercolor{red}{изометричным}, если $\scriptQ^*=\scriptQ^{-1}$.
        \begin{Comment}
            В пространстве со скалярным произведением имеем $\dotprod{\scriptQ x}{\scriptQ y}=\dotprod{x}{\scriptQ^{-1}\scriptQ y}=\dotprod{x}{y}$.
        \end{Comment}
        \begin{Comment}
            В унитарном пространстве изометричный оператор называют унитарным, в евклидовом --- ортогональным.
        \end{Comment}
        \thm Изометричный оператор нормален.
        \thm $\scriptQ$ изометричный тогда и только тогда, когда существует ортонормированный базис, в котором матрица оператора $\scriptQ$ унитарна/ортогональна.
        \begin{Proof}
            D\'{e}j\`{a} vu, I've just been in this place before...
        \end{Proof}
        \thm $\scriptQ$ изометричный тогда и только тогда, когда он переводит ортонормированный базис в ортонормированный.
        \begin{Proof}
            Вправо --- тривиально из определения.\\
            Влево. Мы знаем, что $\dotprod{\scriptQ e_i}{\scriptQ e_j}=\delta_{ij}=\dotprod{e_i}{e_j}$. Чтобы доказать это для произвольного вектора, рассмотрим $x,y\in V$ и распишем скалярное произведение:
            $$
            \dotprod{\scriptQ x}{\scriptQ y}=\sum\limits_{i=1}^n\sum\limits_{j=1}^n\mathrm x_i\overline{\mathrm y_j}\dotprod{\scriptQ e_i}{\scriptQ e_j}=\dotprod{\scriptQ x}{\scriptQ y}=\sum\limits_{i=1}^n\sum\limits_{j=1}^n\mathrm x_i\overline{\mathrm y_j}\dotprod{e_i}{e_j}=\dotprod{x}{y}
            $$
        \end{Proof}
        \thm Если $\scriptQ$ изометричный, то $\scriptQ^{-1}$ изометричный.
        \thm Если $\scriptQ$ и $\scriptR$ изометричные, то $\scriptQ\scriptR$ изометричный.
        \begin{Proof}
            Всё выше --- очевидно.
        \end{Proof}
        \thm $\scriptQ$ изометричный тогда и только тогда, когда он нормальный и все корни его характеристического многочлена по модулю равны единице.
        \begin{Proof}
            Унитарное пространство: $\Lambda\overline{\Lambda^T}=E$. Если перемножить две такие диагональные матрицы, получим $\diag(\lambda_1\overline\lambda_1;\ldots;\lambda_n\overline\lambda_n)$. Это квадраты модулей собственных чисел, они должны быть равны единице. Ч.Т.Д.\\
            Евклидово пространство: все модули собственных чисел также равны единице, но также $\Phi_k\Phi_k^T=E$. А $\Phi_k\Phi_k^T=\diag(\alpha_j^2+\beta_j^2;\alpha_j^2+\beta_j^2)$. Это значит, что $|\alpha_j\pm\beta_j\im|=1$.
        \end{Proof}
        \thm Если $L\subset V$ --- инвариантно относительно изометричного оператора $\scriptQ$, то $L^\perp$ инвариантно относительно $\scriptQ$.
        \begin{Proof}
            рассмотрим ненулевой $x\in L$. Поскольку $\scriptQ$ невырожденный, $\exists!u\in L~x=\scriptQ u$. Тогда $\forall y\in L^\perp~\dotprod{x}{\scriptQ y}=\dotprod{\scriptQ u}{\scriptQ y}=\dotprod uy=0$ так как $u\in L$, $y\in L^\perp$. То есть $\scriptQ y\in L^\perp$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема о каноническом виде матрицы изометричного оператора}. $\scriptQ\in\End(V)$ изометричный тогда и только тогда, когда существует ортонормированный базис, в котором матрица $\scriptQ$ будет иметь вид
        \begin{center}
            \begin{tabular}{|m{0.4\textwidth}|m{0.4\textwidth}|}
                \hline
                Для унитарного пространства & Для евклидова пространства\\
                \hline
                Диагональный, на диагонали числа по модулю равны 1. & Такую же, как нормальный оператор, $|\lambda_i|=1$, $\Phi_k=\matr{\alpha_k & \beta_k\\-\beta_k & \alpha_k}$, где $\alpha_j^2+\beta_j^2=1$.\\
                \hline
            \end{tabular}
        \end{center}
        \begin{Proof}
            Смотри куда-то вверх.
        \end{Proof}
        \thm $\scriptQ$ унитарный оператор тогда и только тогда, когда собственные подпространства ортогональны, а все собственные числа по модулю равны 1.
        \thm Матрица $Q$ унитарная/ортогональная тогда и только тогда, когда существует такая унитарная/ортогональная матрица $T$, что $T^*QT=\Lambda$ --- имеет ту же форму, что и в теореме о каноническом виде матрицы изометричного оператора.
        \begin{Comment}
            В евклидовом пространстве, несложно заметить, $\Phi_k$ в изометричной матрице можно интерпретировать как косинус и синус некоторого угла. А значит $\Phi_k$ --- матрица поворота. То есть в евклидовом пространстве в изометричной матрице мы имеем отражение (знак собственных чисел) и поворот ($\Phi_k$).
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Разложения матриц}. \undercolorblack{orange}{LDU/LU}. \undercolorblack{orange}{Разложение Холецкого}. \undercolorblack{orange}{QR}. \undercolorblack{orange}{Полярное разложение}.}
    \begin{itemize}
        \dfn $L$ --- \undercolor{red}{левая треугольная}/\undercolor{red}{нижне-треугольная} матрица, если все её элементы выше главной диагонали равны нулю. Если при этом все элементы на диагонали равны единице, то матрица \undercolor{red}{нижняя унитреугольная}.
        \dfn $R$ --- \undercolor{red}{правая треугольная}/\undercolor{red}{верхне-треугольная}, если понятно что.
        \dfn Если $A=\matr{
            a_{11} & \cdots & a_{1n}\\
            \vdots & \ddots & \vdots\\
            a_{n1} & \cdots & a_{nn}
        }$, то матрицы вида $A_k=\matr{
            a_{11} & \cdots & a_{1k}\\
            \vdots & \ddots & \vdots\\
            a_{k1} & \cdots & a_{kk}
        }$, где $k\leqslant n$ называются \undercolor{red}{угловыми матрицами}.
        \thm Все угловые миноры матрицы $A$, кроме (возможно) $\Delta_n$ не равны нулю тогда и только тогда, когда $\exists!L$ --- нижняя унитреугольная $\exists!D=\diag(d_1;\ldots;d_n)~\exists!U$ --- верхняя унитреугольная, такие что $\forall k\in[1:n-1]~d_k\neq0$ и $A=LDU$.
        \begin{Proof}
            Докажем влево. Несложно заметить, что $\det A=\det L\det D\det U=1\det D1$. Это равно произведению диагональных элементов $D$. Пусть $L_k$, $D_k$, $U_k$ --- угловые матрицы $L$, $D$ и $U$. Тогда
            $$
            a_{ij}=\sum\limits_{m=1}^n\sum\limits_{r=1}^nl_{im}d_{mr}u_{rj}
            $$
            При $m>i$ или $r<j$ слагаемое равно нулю. То есть эта сумма равна
            $$
            \sum\limits_{m=1}^i\sum\limits_{r=1}^jl_{im}d_{mr}u_{rj}
            $$
            При этом мы хотим доказать, что $A_k=L_kD_kU_k$. Для этого посмотрим на $a_{ij}$, где $i,j<k$, и получим, что в сумме есть только
            $$
            \sum\limits_{m=1}^k\sum\limits_{r=1}^kl_{im}d_{mr}u_{rj}
            $$
            При этом $\Delta_k=d_1\cdots d_k$, что действительно не равно нулю.
            \begin{Comment}
                Интересный факт: $d_k=\frac{\Delta_k}{\Delta_{k-1}}$.
            \end{Comment}
            Теперь вправо. Докажем индукцией по размерности. База:
            $A=\matr{a_{11}}$, тогда $L=U=\matr{1}$, $D=\matr{a_{11}}$. Понятно, что это единственные такие матрицы.\\
            Переход. Пусть $A_k=L_kD_kU_k$. Докажем для $k+1$. Хорошо, что такое $L_{k+1}$ и $U_{k+1}$?
            $$
            L_{k+1}=\arr{c|c}{
                L_k & \mathbb0\\\hline
                X & 1
            }\qquad\qquad
            U_{k+1}=\arr{c|c}{
                U_k & Y\\\hline
                \mathbb0 & 1
            }
            $$
            Также $D_{k+1}=\diag(\cdots;d_{k+1})$. Решим относительно $X$, $Y$ и $d_{k+1}$ уравнение $A_{k+1}=L_{k+1}D_{k+1}U_{k+1}$. Ну, это равносильно
            \[\begin{split}
                \arr{c|c}{
                    A_k & c_{k+1}\\\hline
                    b_{k+1} & a_{k+1,k+1}
                }=
                L_{k+1}D_{k+1}U_{k+1}=\arr{c|c}{
                    L_k & \mathbb0\\\hline
                    X & 1
                }\arr{c|c}{
                    D_k & \mathbb0\\\hline
                    \mathbb0 & d_{k+1}
                }\arr{c|c}{
                    U_k & Y\\\hline
                    \mathbb0 & 1
                }=\\
                =\arr{c|c}{
                    L_kD_k & \mathbb0\\\hline
                    xD_k & d_{k+1}
                }\arr{c|c}{
                    U_k & Y\\\hline
                    \mathbb0 & 1
                }=\arr{c|c}{
                    L_kD_kU_k & L_kD_kY\\\hline
                    U_kD_kx & xD_ky+d_{k+1}
                }
            \end{split}\]
            По индукционному предположению $A_k=L_kD_kU_k$. Осталось решить систему
            $$
            \left\{\begin{aligned}
                &L_kD_kY=c_{k+1}\\
                &XD_kU_k=b_{k+1}\\
                &XD_kY+d_{k+1}=a_{k+1,k+1}
            \end{aligned}\right.
            $$
            Первые две штуки являются системами линейных неоднородных уравнений. При этом $L_kD_k$ имеет ненулевой определитель, значит система имеет единственное решение для $X$ и $Y$. А значит и $d_{k+1}=a_{k+1,k+1}-XD_kY$ --- тоже единственное решение.
        \end{Proof}
        \begin{Comment}
            Ещё в литературе встречается LU-разложение --- без матрицы $D$, но с матрицами $L$ и $U$ не унитреугольными, а просто треугольными. Его мы, впрочем, можем легко найти из LDU-разложения, взяв в качестве новой $U$ $DU$ (или в качестве нового $L$ $LD$).
        \end{Comment}
        \begin{Comment}
            Есть более простой алгоритм, нежели решать две системы. Он такой: берём $\arr{c|c}{A & E}$, проводим алгоритм Гаусса, не меняя местами строки и столбцы. Тогда мы получим слева верхне-треугольную матрицу матрицу. При этом, поскольку мы не меняли местами строки и столбцы, у нас сохранятся угловые миноры. Утверждается, что мы слева получим $DU$, а справа --- $L^{-1}$. Сейчас мы это докажем.
        \end{Comment}
        \dfn \undercolor{red}{Элементарная нижняя унитреугольная} --- это единичная матрица, в которой заменили элемент ${}_{ji}$ на $\lambda$ (и $i\neq j$). Обозначается $L_{ji}(\lambda)$.
        \begin{Comment}
            Что будет, когда мы умножим такую матрицу \textbf{слева} на некоторую другую? А вот к элементам $j$-той строки прибавятся элементы $i$-той, умноженные на $\lambda$. Это вы сами можете легко проверить, перемножив матрицы.\\
            А что когда умножим \textbf{справа}? Будет то же самое, но наоборот и со столбцами. То есть к элементам $i$-того столбца добавятся элементы $j$-того, умноженного на $\lambda$.
        \end{Comment}
        \begin{Proof}
            Итак, докажем алгоритм. Мы умножили на некоторое количество элементарных матриц обе части (и $A$, и $E$). То есть из $\arr{c|c}{A&E}$ мы получили $\arr{c|c}{L_m\cdots L_1A&L_m\cdots L_1E}$. Понятно, что мы можем взять $L_m\cdots L_1A$ за $DU$, потому что после Гаусса получается верхнетреугольная матрица. Но это значит, что $A=L_1^{-1}\cdots L_m^{-1}DU$. Хм-м, $L_1^{-1}\cdots L_m^{-1}$ --- перемножение некоторого количества нижних унитреугольных матриц, значит из результат является таковым, его можно взять за $L$. Поскольку мы нашли разложение $A=LDU$, это то что нам нужно. При этом справа (там где $E$) мы, как несложно заметить, также получим $L^{-1}=L_m\cdots L_1E$.
        \end{Proof}
        \thm $A=A^*$ и $\Delta_k\neq0$ для всех $k\in[1:n-1]$ тогда и только тогда, когда $\exists!L$ --- нижняя унитреугольная $\exists!U$ --- верхняя унитреугольная, такие что $LDL^*=U^*DU$, причём все $d_k\in\mathbb R$.
        \begin{Proof}
            Ну, тривиально. Известно, что существуют единственные $L,D,U$ такие что $A=LDU$. Отсюда $A^*=U^*\overline DL^*$. Но это же разложение $A^*$. К тому же, $A^*=A$, а значит $L=U^*$, $U=L^*$, $D=\overline D$. Это, собственно, то что нам нужно.
        \end{Proof}
        \begin{Comment}
            В случае таких матриц, в алгоритме с $\arr{c|c}{A&E}$ можно вообще не считать правую часть.
        \end{Comment}
        \dfn Самосопряжённый оператор $\scriptA\in\End(V)$ называется \undercolor{red}{положительно/отрицательно определённым}, если $\forall u\neq\mathbb0~\dotprod{\scriptA u}{u}>0/{}<0$. Обозначается $\scriptA>0/{}<0$.
        \dfn Самосопряжённый оператор $\scriptA\in\End(V)$ называется \undercolor{red}{положительно/отрицательно полуопределённым}, если $\forall u\neq\mathbb0~\dotprod{\scriptA u}{u}\geqslant0/{}\leqslant0$. Обозначается $\scriptA\geqslant0/{}\leqslant0$.
        \dfn Самосопряжённый оператор $\scriptA\in\End(V)$ называется \undercolor{red}{неопределённым}, если $\exists u,v\neq\mathbb0~\dotprod{\scriptA u}{u}>0\land\dotprod{\scriptA v}{v}<0$. Обозначается $\scriptA\lessgtr0$.
        \thm Поскольку оператор самосопряжённый, все условия можно писать не с $\dotprod{\scriptA u}{u}$, а с $\dotprod{u}{\scriptA u}$.
        \thm $\scriptA>0$ имеет только положительные собственные числа. $\scriptA<0$ имеет только отрицательные собственные числа. $\scriptA\geqslant0$ имеет только неотрицательные собственные числа. $\scriptA\leqslant0$ имеет только неположительные собственные числа. У $\scriptA\lessgtr0$ есть хотя бы одно положительное и хотя бы одно отрицательное собственное число.
        \begin{Proof}
            Сначала заметим, что у самосопряжённого оператора все собственные числа вещественные, а значит утверждения корректны. Также для $\lambda\neq\mu$ верно $V_\lambda\perp V_\mu$. Итак. Любой вектор $u\in V$ раскладывается как сумма собственных векторов $\sum\limits_\lambda u_\lambda$. А для $u_\lambda$ верно, что $\scriptA u_\lambda=\lambda u_\lambda$. если любой вектор раскладывается, то
            $$
            \dotprod{\scriptA}{u}=\sum\limits_{\lambda}\sum\limits_\mu\lambda\dotprod{u_\lambda}{u_\mu}\overset\perp=\sum\limits_{\lambda}\lambda\dotprod{u_\lambda}{u_\lambda}
            $$
            Несложно заметить, что $\dotprod{u_\lambda}{u_\lambda}>0$, так как $u_\lambda\neq0$. А дальше всё просто. Следствие вправо: если для любого $u$ верно, значит верно и для $u_\lambda$. Тогда сумма вырождается просто в $\lambda$, которые должны быть нужного знака. Следствие влево: если все $\lambda>0$, то любая сумма положительна, для отрицательно определённого и полуопределённых операторов то же самое, для неопределённого --- как в следствии вправо.
        \end{Proof}
        \begin{Comment}
            Определения переносится на матрицы операторов.
        \end{Comment}
        \thm \undercolor{darkgreen}{Теорема о разложении Холецкого}. Если $A$ --- положительно определённая матрица, все угловые миноры которой не равны нулю, $\exists!L$ --- нижнетреугольная, все диагональные элементы которой положительны, $\exists!U$ --- верхнетреугольная, все диагональные элементы которой положительны такие что $A=LL^*=U^*U$.
        \begin{Proof}
            Как мы знаем существует единственное LDU-разложение, причём, поскольку $A=A^*$, $A=L_0DL_0^*=U_0^*DU_0$, и при этом $d_k\in\mathbb R$. Как мы знаем по положительной определённости $\dotprod{U_0^*DU_0x}{x}>0\Leftarrow\dotprod{DU_0x}{U_0x}>0$. Давайте перебирать $y=U_0x$. Поскольку $\det U_0=1$, $\operIm U_0=V$, а значит перебор всех $y$ эквивалентен перебору всех $x$. Итак, перебираем $y$. Для каждого $\dotprod{Dy}{y}=d_1|\mathrm y_1|^2+\cdots+d_n|\mathrm y_n|^2>0$. Поскольку это верно для любых $y\in V$, можно взять $y=e_i$, и получить $d_i>0$. Это значит, что из матрицы $D$ можно взять корень: $\sqrt D=\diag(\sqrt{d_1}\cdots \sqrt{d_n})$. А в таком случае $A=\underbrace{U_0^*\sqrt D}_{U^*}\underbrace{\sqrt DU_0}_U$. Аналогично с $L$.
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема о QR-разложении}. Для любой невырожденной матрицы $A$ существуют $Q$ --- ортогональная/унитарная матрица, $R$ --- правая (т.е. верхняя) треугольная матрица, такие что $A=QR$.
        \begin{Proof}
            Пусть $A=\matr{A_1;\ldots;A_n}$ --- столбцы. Поскольку матрица невырождена, столбцы линейно независимы. Их можно ортонормировать. Тогда $Q$ будет состоять из этих ортонормированных столбцов. Она, очевидно, ортогональна. При этом в ортогонализации Грама --- Шмидта мы имеем что-то такое:
            \[
            \begin{split}
                Q_1&=r_{11}A_1\\
                Q_2&=r_{12}A_1+r_{22}A_2\\
                &\vdots\\
                Q_n&=r_{1n}A_1+\cdots+r_{nn}A_n\\
            \end{split}
            \]
            Очень похоже на матрицу $R$. Но на самом деле это не она. Мы получили, что $Q=AR$, а значит $A=QR^{-1}$. Обратная к $R$ существует по построению (элементы $r_{kk}$ не равны нулю), а ещё $R^{-1}$ также правая треугольная.
        \end{Proof}
        \thm Аналогично, для невырожденной матрицы существуют $Q$ --- всё такая же, $L$ --- левая треугольная матрица, такие что $A=LQ$.
        \begin{Proof}
            Запишем QR-разложение для $A^T$. Всё.
        \end{Proof}
        \thm \undercolor{darkgreen}{Теорема о полярном разложении}. Для любой матрицы $A$ $\exists!H\geqslant0$ самосопряжённая $\exists U$ унитарная/ортогональная $A=HU$. Причём если $A$ невырождена, то $U$ единственна.
        \begin{Comment}
            Такие обозначения приняты только в комплексном пространстве, в вещественном приняты $S$ и $Q$.
        \end{Comment}
        \begin{Comment}
            Это своего рода разложение на модуль и аргумент как для комплексных чисел.
        \end{Comment}
        \thm \undercolor{darkgreen}{Теорема о полярном разложении операторов}. Пусть $\scriptA\in\End(V)$. Тогда $\exists!\scriptH>0$ самосопряжённый $\exists\scriptU$ изометричный, что $\scriptA=\scriptH\scriptU$, причём если $\scriptA\in\Aut(V)$, то $\scriptU$ единственный.
        \thm Пусть $\scriptA$ --- оператор простой структуры, все собственные числа которого больше либо равны нуля. Тогда $\exists!\scriptB$ --- о.п.с., все собсьтвенные числа которого неотрицательны, такой что $\scriptB^2=\scriptA$.
        \begin{Proof}
            Пусть $V=\bigoplus\limits_{\lambda}V_\lambda$. При этом все $\lambda\geqslant0$. Возьмём $V=\Lin\{v_1;\ldots;v_n\}$ --- базис из собственных векторов. Тогда $\scriptA v_j=\lambda_jv_j$. Возьмём такой оператор $\scriptB$, что $\scriptB v_j=\sqrt{\lambda_j}v_j$. Получится оператор, который на базисных векторах в квадрате равен $\scriptA$, а значит просто $\scriptB^2=\scriptA$. Также, тривиально, что $\scriptB$ --- о.п.с.\\
            Теперь единственность. Пусть есть ещё один такой: $C$. Рассмотрим $\scriptA$. Он равен $\sum\limits_\lambda\lambda\scriptP_\lambda$. И поскольку $C$ --- о.п.с., $\sum\limits_\mu\mu\scriptQ_\mu$. Поскольку $\scriptQ_\mu$ --- проекторы, если возвести сумму в квадрат, получится $\scriptA=\scriptC^2=\sum\limits_\mu\mu^2\scriptQ_\mu$. В силу единственности спектрального разложения, это равно $\sum\limits_\lambda\lambda\scriptP_\lambda$. А отсюда $Q_\mu=P_\lambda$ и $\mu^2=\lambda$. Кажется, это всё-таки $\scriptB$.
        \end{Proof}
        \item[] Теперь докажем теорему о полярном разложении.
        \begin{Proof}
            Рассмотрим $\scriptA\scriptA^*$. Утверждается, что этот оператор самосопряжённый. Проверьте сами по определению. Также и $\scriptA^*\scriptA$ самосопряжённый. Также $\scriptA^*\scriptA$ и $\scriptA\scriptA^*$ положительно полуопределены (также элементарно по определению).\\
            Если $\scriptA^*\scriptA$ самосопряжён, он имеет ортонормированный базис из собственных векторов. При этом также все собственные числа $\scriptA^*\scriptA$ больше либо равны нуля. Отсюда, $\dotprod{\scriptA^*\scriptA v_j}{v_j}=\dotprod{\scriptA v_j}{\scriptA v_j}$, что не равно нулю, если $\lambda_j\neq0$. То есть $\scriptA$ переводит ортонормированный базис $v_1;\ldots;v_n$ в ортогональную систему векторов (возможно, не полную).\\
            Хорошо, пусть $z_1=\frac{\scriptA v_1}{\sqrt{\lambda_1}}$, $z_2=\frac{\scriptA v_2}{\sqrt{\lambda_2}}$, так будем делать, пока $\lambda_i\neq0$, а потом дополним $z$ до ортонормированного базиса (тривиально, уже имеющиеся вектора и так ортонормированы). Тогда пусть
            $\scriptH z_i=\sqrt{\lambda_i}z_i$, $\scriptU v_i=z_i$. Знаете что тогда? Тогда $\scriptA v_i=\sqrt{\lambda_i}z_i=\scriptH\scriptU v_i$. То есть на базисных векторах $v_i$ $\scriptA$ действует также, как и $\scriptH\scriptU$. А значит $\scriptA=\scriptH\scriptU$.\\
            Теперь посмотрим, какой вид имеют $\scriptH$ и $\scriptU$. Первое, тривиально, имеет диагональный вид в базисе $z_i$. Более того, в этом базисе все числа на диагонали положительны и вещественны. А $\scriptU$ тем временем переводит ортонормированный базис $v_i$ в ортонормированный базис $z_i$. Кайф, то что надо.\\
            Теперь единственность. Пусть $\scriptA=\scriptH\scriptU$. Тогда $\scriptA\scriptA^*=\scriptH\scriptU\scriptU^*\scriptH=\scriptH^2$. Корень извлечь мы можем единственным образом, значит $\scriptH$ существует только одно такое: $\sqrt{\scriptA\scriptA^*}$.\\
            В случае же когда $\scriptA$ невырождено, её определитель не равен нулю, определитель $\scriptU$ также нулю не равен (а равен единице, изометрия же), а значит и определитель $\scriptH$ ненулевой, то есть у $\scriptH$ есть единственная обратная, отсюда есть единственное $\scriptU$.
        \end{Proof}
        \thm В теореме о полярном разложении операторов также истинно утверждение о существовании $\scriptH'$ и $\scriptU'$ таких что $\scriptA=\scriptU'\scriptH'$.
        \begin{Proof}
            Разложим $\scriptA^*=\scriptH\scriptU$, $\scriptH'=\scriptH^*$, $\scriptU'=\scriptU^*$.\\
            Также тривиально доказывается единственность, получается что $\scriptH=\sqrt{\scriptA^*\scriptA}$.
        \end{Proof}
        \dfn $\sqrt{\scriptA\scriptA^*}$ и $\sqrt{\scriptA^*\scriptA}$ называются соответственно \undercolor{red}{левым и правым модулем} $\scriptA$.
    \end{itemize}
    \section{Квадратичные формы.}
    \paragraph{\undercolorblack{orange}{Общие понятия}.}
    \begin{itemize}
        \dfn Отображение $f\colon\mathbb R^n\to\mathbb R$ называется \undercolor{red}{квадратичной формой}, если имеет следующий вид:
        $$
        f(\mathrm x)=\sum\limits_{i=1}^n\sum\limits_{j=1}^na_{ij}\mathrm x_i\mathrm x_j
        $$
        Причём $a_{ij}=a_{ji}$.
        \begin{Comment}
            Варианты записи у этого есть такие:
            \begin{enumerate}
                \item $$\sum\limits_{i=1}^na_{ii}\mathrm x_i^2+2\sum\limits_{1\leqslant i<j\leqslant n}a_{ij}\mathrm x_i\mathrm x_j$$
                \item $f(\mathrm x)=\mathrm x^TA\mathrm x$ в стандартном скалярном произведении.
            \end{enumerate}
        \end{Comment}
        \dfn Ранг квадратичной нормы --- ранг её матрицы.
        \begin{Comment}
            Также у квадратичной формы есть второе определение: пусть $\alpha\colon V^2\to\mathbb R$ --- билинейная симметричная форма. Тогда, зафиксировав базис $e_1;\ldots;e_n$ в пространстве $V$, получим $\alpha(x;y)=\sum\limits_{i=1}^n\sum\limits_{j=1}^na_{ij}\mathrm x_i\mathrm y_j$, и достаточно сказать, что $f(x)=\alpha(x;x)$.
        \end{Comment}
        \dfn Отображение $f\colon\mathbb C^n\to\mathbb C$ называется эрмитовой формой, если оно имеет следующий вид:
        $$
        f(\mathrm x)=\sum\limits_{i=1}^n\sum\limits_{j=1}^na_{ij}\mathrm x_i\overline{\mathrm x_j}
        $$
        Причём $a_{ij}=\overline{a_{ji}}$.
        \begin{Comment}
            Этим мы заниматься не будем, но такое определение есть.
        \end{Comment}
        \dfn Говорят, что квадратичная форма $g(y)=y^TBy$ \undercolor{red}{получена из} формы $f(x)=x^TAx$, если $x=Qy$.
        \begin{Comment}
            Несложно заметить, что $Q^TAQ=B$. В курсе мы будем рассматривать только невырожденные преобразования $Q$. Это гарантируем нам неизменность ранга.
        \end{Comment}
        \dfn Если при $i\neq j$ $a_{ij}=0$, такой \undercolor{red}{вид} формы называется \undercolor{red}{каноническим}.
        \dfn Если $f$ --- форма в каноническом виде, то количество положительных $a_{ii}$ называется \undercolor{red}{положительным} \undercolor{red}{индексом инерции} $f$ (обозначение $\sigma^+(f)$), количество отрицательных --- \undercolor{red}{отрицательным индексом инерции} ($\sigma^-(f)$), количество нулевых никак не называется, но обозначается $\sigma_0(f)$. \undercolor{red}{Сигнатурой квадратичной формы} называется $(\sigma^+(f);\sigma^-(f);\sigma_0(f))$.
        \dfn Если в каноническом виде все $a_{ii}$ равны $0$, $1$ или $-1$, то \undercolor{red}{форма} называется \undercolor{red}{нормальной}.
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Методы приведения формы к каноническому виду}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Один способ мы уже знаем --- ортогональное преобразование. Поскольку матрица симметричная, $\exists Q$ ортогональная, такая что $Q^TAQ=\Lambda=\diag(\lambda_1;\ldots;\lambda_n)$ --- собственные числа матрицы $A$. См. теорему о каноническом виде матрицы самосопряжённого оператора.
        \end{Comment}
        \begin{Comment}
            Второй метод --- метод Лагранжа. Осанован на выделении полного квадрата для функции с несколькими переменными. Тут есть два случая:
            \begin{enumerate}
                \item Если $\forall i~a_{ii}=0$, то хочется заработать квадрат на свою голову. Давайте возьмём некоторое $a_{ij}\neq0$. При нём стоит слагаемое $a_{ij}\mathrm x_i\mathrm x_j$. Давайте скажем, что $\mathrm x_i=\mathrm y_i+\mathrm y_j$, $\mathrm x_j=\mathrm y_i-\mathrm y_j$, а все остальные координаты оставим без изменений. Тогда из слагаемого $a_{ij}\mathrm x_i\mathrm x_j$ мы получим $a_{ij}(\mathrm y_i^2-\mathrm y_j^2)$. Мы получим хотя бы один квадрат.
                Матрица такого преобразования выглядит так:
                $$
                \matr{
                    1 & 0 & \cdots & 0 & \cdots & 0 & \cdots & 0\\
                    0 & 1 & \cdots & 0 & \cdots & 0 & \cdots & 0\\
                    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots\\
                    0 & 0 & \cdots & 1 & \cdots & 1 & \cdots & 0\\
                    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots\\
                    0 & 0 & \cdots & 1 & \cdots & -1 & \cdots & 0\\
                    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots\\
                    0 & 0 & \cdots & 0 & \cdots & 0 & \cdots & 1\\
                }
                $$
                \item Если у нас есть квадрат (изначально или после первого шага), то делаем вот что. Пусть $a_{ii}\neq0$. Рассмотрим все $a_{ij}$ и всё остальное. Всё остальное --- некоторая квадратичная форма $\widetilde f$, не содержащая $\mathrm x_i$. То есть
                $$
                f(x)=a_{ii}\mathrm x_i^2+2\sum\limits_{j\neq i}a_{ij}\mathrm x_i\mathrm x_j+\widetilde f(\ldots;\mathrm x_{i-1};\mathrm x_{i+1};\ldots)
                $$
                Это преобразуем так:
                \[\begin{split}
                    f(x)&=a_{ii}\mathrm x_i^2+2\sum\limits_{j\neq i}a_{ij}\mathrm x_i\mathrm x_j+\widetilde f(\ldots;\mathrm x_{i-1};\mathrm x_{i+1};\ldots)=\\
                    &=\frac1{a_{ii}}\left(a_{ii}^2\mathrm x_i^2+2\sum\limits_{j\neq i}a_{ii}a_{ij}\mathrm x_i\mathrm x_j\right)+\widetilde f(\ldots)=\\
                    &=\frac1{a_{ii}}\left(\sum\limits_{j=1}^na_{ij}\mathrm x_i\right)^2-\frac1{a_{ii}}\left(\sum\limits_{j=1}^na_{ij}^2\mathrm x_j^2\right)+\widetilde f(\ldots)=\\
                    &=\frac1{a_{ii}}\left(\underbrace{\sum\limits_{j=1}^na_{ij}\mathrm x_i}_{\mathrm y_i}\right)^2+\widetilde{\widetilde f}(\ldots)=
                \end{split}\]
                Итого $\mathrm y_i=\sum\limits_{j=1}^na_{ij}\mathrm x_i$, все остальные $\mathrm y_j=\mathrm x_j$.
                Итого мы получаем квадрат и квадратичную форму меньшего порядка, с которой делаем то же самое.\\
                Как выглядит матрица $Q$? Вот так:
                $$
                \matr{
                    1 & 0 & \cdots & 0\\
                    \vdots & \vdots & \ddots & \vdots\\
                    a_{i1} & a_{i2} & \cdots & a_{in}\\
                    \vdots & \vdots & \ddots & \vdots\\
                    0 & 0 & \cdots & 1
                }
                $$
                Тривиально, что она невырождена.
            \end{enumerate}
        \end{Comment}
        \begin{Comment}
            Метод Якоби. $f(x)$ приводится к каноническому виду при помощи верхней унитреугольной матрицы. Если матрица $A$ достаточно хорошая (имеет ранг $n-1$ или $n$), то по LDU-разложении поскольку $A=A^T$ существуют единственные $D$ и $U$ с нужными свойствами такие что $A=U^TDU$. При этом $D=\diag(d_1;\ldots;d_n)$, где $d_k=\frac{\Delta_k}{\Delta_{k-1}}$. В таком случае $D=(U^{-1})^TAU^{-1}$. Ну так и всё, $U^{-1}$ --- это наше $Q$. При этом мы даже когда будем искать $L^{-1}$ методом Гаусса, можно будет его даже не обращать, поскольку $L^{-1}=Q^T$.
        \end{Comment}
        \thm \undercolor{darkgreen}{Теорема Якоби}. Если $f(x)=\mathrm x^TA\mathrm x$, $\forall k\in[1:n-1]~\Delta_k\neq0$, то существует единственное верхнее униреугольное преобразование $Q$: $\mathrm x=Q\mathrm y$, что $f(x)=g(y)=\mathrm y^T\Lambda\mathrm y$, где $\Lambda=\diag\left(\Delta_1;\frac{\Delta_2}{\Delta_1};\ldots;\frac{\Delta_n}{\Delta_{n-1}}\right)$. Причём $Q$ может быть найдена как $A_{k-1}q_k=-b_k$, где
        $$
        Q=\matr{
            1 & q_{12} & q_{13} & \cdots & q_{1n}\\
            0 & 1 & q_{23} & \cdots & q_{2n}\\
            0 & 0 & 1 & \cdots & q_{3n}\\
            \vdots & \vdots & \vdots & \ddots & \vdots\\
            0 & 0 & 0 & \cdots & 1\\
        }\quad
        A=\matr{
            a_{11} & a_{12} & a_{13} & \cdots & a_{1n}\\
            ? & a_{22} & a_{23} & \cdots & a_{2n}\\
            ? & ? & a_{33} & \cdots & a_{3n}\\
            \vdots & \vdots & \vdots & \ddots & \vdots\\
            ? & ? & ? & \cdots & a_{nn}\\
        }\quad
        q_k=\matr{q_{1k}\\q_{2k}\\\vdots\\q_{k-1,k}}\quad
        b_k=\matr{a_{1k}\\a_{2k}\\\vdots\\a_{k-1,k}}
        $$
        \begin{Proof}
            Существование и единственность следует из LDU-разложения, докажем формулу. Докажем методом математической индукции.\\
            База: $n=2$. Применим алгоритм LDU:
            $$
            \arr{cc|cc}{a_{11} & a_{12} & 1 & 0\\a_{12} & a_{22} & 0 & 1}\sim
            \arr{cc|cc}{a_{11} & a_{12} & 1 & 0\\0 & a_{22}-\frac{a_{12}^2}{a_{11}} & -\frac{a_{12}}{a_{11}} & 1}
            $$
            Левая часть --- это $DU$, значит $$D=\diag(a_{11};\frac{a_{22}a_{11}-a_{12}^2}{a_{11}})=\diag(\Delta_1;\frac{\Delta_2}{\Delta_1})$$
            Итого
            $$
            Q=\matr{1 & -\frac{a_{12}}{a_{11}}\\0&1}\qquad q_2=-\frac{a_{12}}{a_{11}}\qquad a_{11}q_2=-a_{12}
            $$
            То что нужно.\\
            Переход. Пусть для $n=k$ мы доказали и $\forall m\in[2:k]~A_{m-1}q_m=-b_m$. Докажем для $n=k+1$. Рассмотрим такое $Q_{k+1}$, что
            $$
            Q_{k+1}=\arr{c|c}{Q_{k} & q_{k+1}\\\hline \mathbb0 & 1}
            $$
            Хочется сказать, что она подходит.
            \[\begin{split}
                Q_{k+1}^TA_{k+1}Q_{k+1}&=\arr{c|c}{Q_k^T & \mathbb0\\\hline q_{k+1}^T & 1}\arr{c|c}{A_k & b_{k+1}\\\hline b_{k+1}^T & a_{k+1,k+1}}\arr{c|c}{Q_{k} & q_{k+1}\\\hline \mathbb0 & 1}=\\
                &=\arr{c|c}{Q_k^TA_k & Q_k^Tb_{k+1}\\\hline \underbrace{q^T_{k+1}A_k+b_{k+1}^T}_{\mathbb0} & q_{k+1}^Tb_{k+1}+a_{k+1,k+1}}\arr{c|c}{Q_{k} & q_{k+1}\\\hline \mathbb0 & 1}=\\
                &=\arr{c|c}{Q_k^TA_kQ_k & \overbrace{Q_k^TA_kq_{k+1}+Q_k^Tb_{k+1}}^{Q_k^T(A_kq_{k+1}+b_{k+1})=\mathbb0}\\\hline\mathbb0 & q_{k+1}^Tb_{k+1}+a_{k+1,k+1}}=\\
                &=\arr{c|c}{D_k & \mathbb0\\\hline\mathbb0&d_{k+1}}
            \end{split}\]
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Закон инерции квадратичных форм}. \undercolorblack{orange}{Критерий Сильвестра}.}
    \begin{itemize}
        \thm \undercolor{darkgreen}{Закон инерции квадратичных форм}. Вне зависимости от того, каким (невырожденным) преобразованием квадратичная форма была приведена к каноническому виду, сигнатура будет одной и той же. Говоря формально, пусть $Q_1$, $Q_2$ --- невырожденные преобразования, пусть $f(x)=\mathrm x^TA\mathrm x=g(y)=h(z)$, $\mathrm x=Q_1\mathrm y=Q_2\mathrm z$. Тогда $\sigma(g)=\sigma(h)$
        \begin{Comment}
            А в таком случае мы можем сказать, что у неканоничной формы тоже есть сигнатура.
        \end{Comment}
        \begin{Proof}
            От противного. Давайте переобозначим переменные так, чтобы сначала шли положительные коэффициенты канонической формы, потом отрицательные, затем нулевые. Тогда
            \[
            \begin{split}
                g(y)=\lambda_1\mathrm y_1^2+\cdots+\lambda_p\mathrm y_p^2-\lambda_{p+1}\mathrm y_{p+1}^2-\cdots-\lambda_r\mathrm y_r^2\\
                h(z)=\mu_1\mathrm z_1^2+\cdots+\mu_s\mathrm z_s^2-\mu_{s+1}\mathrm z_{s+1}^2-\cdots-\mu_r\mathrm z_r^2
            \end{split}
            \]
            $r$ в обеих равенствах одинаковое, потому что это ранг, а он не меняется. Хочется доказать, что $p=s$. Пусть нет. Не умаляя общности, $p<s$. Рассмотрим вот что:
            $$
            \mathrm y=Q_1^{-1}\mathrm x\qquad\qquad\mathrm z=Q_2^{-1}\mathrm x
            $$
            Это две системы уравнений, обе имеют единственное решение. Тогда рассмотрим $\mathrm y^0$ --- вектор, у которого первые $p$ координат нули, а остальные любые --- и $\mathrm z^0$ --- вектор, у которого последние $n-s$ координат нули, а остальные --- любые. Они задают какие-то системы. Которые имеют единственные решения. Перём первые $p$ уравнений из $\mathrm y^0$ и последние $n-s$ из $\mathrm z^0$. Получим \textbf{однородную} систему из $n-\underbrace{(s-p)}_{>0}<n$ уравнений. У неё есть нетривиальное решение $\mathrm x^*$. При этом если $\mathrm y^*=Q_1^{-1}\mathrm x^*$, то у $\mathrm y^*$ первые $p$ координат нули. Но поскольку $\mathrm x^*\neq\mathbb0$ (нетривиальное решение), $\mathrm y^*$ также не ноль. Аналогично $\mathrm z^*=Q_2^{-1}\mathrm x^*$ также имеет последние $n-s$ координат нули, но $\mathrm z^*\neq\mathbb0$. Но тогда есть проблема:
            \[\begin{split}
                f(x^*)&=g(y^*)=\lambda_10+\cdots+\lambda_p0-\lambda_{p+1}\mathrm y_{p+1}^2-\cdots-\lambda_r\mathrm y_r^2<0\\
                f(x^*)&=h(z^*)=\mu_1\mathrm z_1^2+\cdots+\mu_s\mathrm z_1^2-\mu_{s+1}0-\cdots-\mu_r0>0
            \end{split}\]
            Противоречие.
        \end{Proof}
        \dfn Форма называется \undercolor{red}{положительно определённой}, если $\forall x\neq\mathbb0~f(x)>0$.
        \dfn Форма называется \undercolor{red}{отрицательно определённой}, если $\forall x\neq\mathbb0~f(x)<0$.
        \dfn Форма называется \undercolor{red}{положительно полуопределённой}, если $\forall x\neq\mathbb0~f(x)\geqslant0$.
        \dfn Форма называется \undercolor{red}{отрицательно полуопределённой}, если $\forall x\neq\mathbb0~f(x)\leqslant0$.
        \dfn Форма называется \undercolor{red}{неопределённой}, если $\exists x_1,x_2~f(x_1)<0,f(x_2)>0$.
        \begin{Comment}
            $$
            f(x)=\mathrm x^TA\mathrm x=\dotprod{x}{Ax}
            $$
            В силу того, что $A=A^T$, получаем, что форма также определена, как и её матрица.
        \end{Comment}
        \begin{Comment}
            Если $f>0$, то $A>0$, а значит все собственные числа $\lambda$ строго положительны. А это значит, что $\sigma(f)=(n;0;0)$.\\
            В случае $f<0$ получаем $\sigma(f)=(0;0;n)$, в случае $f\geqslant0$ --- $(r;0;n-r)$, в случае $f\leqslant0$ --- $(0;r;n-r)$.\\
            Когда же $f\lessgtr0$ получаем только то, что $\sigma^+(f)\neq0\neq\sigma^-(f)$. В сумме они дают $r$.
        \end{Comment}
        \thm \undercolor{darkgreen}{Критерий Сильвестра}. Пусть $f$ имеет матрицу со всеми ненулевыми угловыми минорами. Тогда $f>0$, если все миноры положительны, $f<0$, если знак чередуется.
        \begin{Proof}
            Методом Якоби приводим к каноническому виду. Дальше сами.
        \end{Proof}
        \begin{Comment}
            Из метода Якоби можно расширить область применения критерия Сильвестра. Если $\Delta_n=0$, то форма получится полуопределённой.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Приведение поверхности второго порядка к каноническому виду}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            \sout{Оставь надежду всяк сюда входящий.} Так я раньше думал, но на самом деле всё не так плохо.
        \end{Comment}
        \begin{Comment}
            Итак, что у нас было в поверхности 2 порядка? У нас было уравнение
            $$
            a_{11}x^2+a_{22}y^2+a_{33}z^2+2a_{12}xy+2a_{13}xz+2a_{23}yz+2a_1x+2a_2y+2a_3z+a_0=0
            $$
            Оно задавало поверхность второго порядка. Теперь мы видим, что у нас есть
            $$
            A=\matr{
                a_{11} & a_{12} & a_{13}\\
                a_{12} & a_{22} & a_{23}\\
                a_{13} & a_{23} & a_{33}
            }\qquad\vec a\matr{a_1\\a_2\\a_3}
            $$
            И тогда мы получаем, что поверхность второго порядка --- это
            $v^TAv+2a^Tv+a_0=0$
            Хочется привести поверхность второго порядка к каноническому виду. Первый шаг --- привести к каноническому виду квадратичную форму. Это мы умеем тремя способами, но нам хочется, чтобы это было не нечто произвольное, а что-то, что можно трактовать, как преобразование пространства. Метод Лагранжа отпадает сразу, там какой-то итеративный кринж происходит. Метод Якоби применим не в любом случае. Остаётся идеально подходящее ортогональное преобразование. При этом мы даже знаем, что получится. Получится $\lambda_1{x'}^2+\lambda_2{y'}^2+\lambda_3{z'}^2$, где $\lambda$ --- собственные числа $A$. А для преобразования координат используется матрица $Q$ --- матрица перехода из стандартного базиса в базис собственных векторов $A$. Но тут есть одно условие: нужно следить за $\det Q$, он должен быть равен 1, чтобы тройка осталось той же ориентации, что и была. Понятно, что из $Q$ с определителем, равным $-1$ можно сделать правильное преобразование, домножив один из столбцов на $-1$, отчего он не перестанет быть координатами собственного вектора. А зачем нам $Q$? А затем, чтобы узнать, как преобразуется линейная часть уравнения поверхности. Поскольку $v=Qv'$, можно просто подставить это в уравнение $v^TAv+2a^Tv+a_0=0$, и получить уравнение на $v'$.\\
            Хорошо, мы получили уравнение в форме $\lambda_1{x'}^2+\lambda_2{y'}^2+\lambda_3{z'}^2+2a_1'x'+2a_2'y'+2a_3'z'+a_0=0$, что дальше? Дальше начинаем разбирать случаи в зависимости от количества нулевых собственных чисел $A$ (т.е. в зависимости от количества нулевых коэффициентов при квадратах координат).
            \begin{itemize}
                \item Ни одно из чисел $\lambda_1$, $\lambda_2$, $\lambda_3$ не равно нулю.\\
                Тогда выносим полные квадраты:
                \[\begin{split}
                    &\lambda_1{x'}^2+\lambda_2{y'}^2+\lambda_3{z'}^2+2a_1'x'+2a_2'y'+2a_3'z'+a_0=0\Leftrightarrow\\
                    &\lambda_1{\underbrace{\left(x'+\frac{a_1'}{\lambda_1}\right)}_{x''}}^2+\lambda_2{\underbrace{\left(y'+\frac{a_2'}{\lambda_2}\right)}_{y''}}^2+\lambda_3{\underbrace{\left(z'+\frac{a_3'}{\lambda_3}\right)}_{z''}}^2\underbrace{-\frac{{a_1'}^2}{\lambda_1}-\frac{{a_2'}^2}{\lambda_2}-\frac{{a_3'}^2}{\lambda_3}+a_0}_{a'_0}=0
                \end{split}\]
                В таком случае имеем параллельный перенос
                $$
                \left\{\begin{aligned}
                    x''=x'+\frac{a_1'}{\lambda_1}\\
                    y''=y'+\frac{a_2'}{\lambda_2}\\
                    z''=z'+\frac{a_3'}{\lambda_3}
                \end{aligned}\right.
                $$
                После этого имеем ещё 2 варианта:
                \begin{itemize}
                    \item Если $a'_0\neq0$, его можно перенести в правую часть и поделить. Получится
                    $$\alpha{x''}^2+\beta{y''}^2+\gamma{z''}^2=1$$
                    Когда все коэффициенты больше нуля, это эллипсоид, когда один меньше --- однополостной гиперболоид, когда два меньше нуля --- двуполостной, когда все меньше --- пустое множество.
                    \item Если $a'_0=0$, то имеем
                    $$\alpha{x''}^2+\beta{y''}^2={z''}^2$$
                    Тут если хотя бы одно из $\alpha$ и $\beta$ положительно, это конус, в противном случае --- точка.
                \end{itemize}
                \item Одно из чисел $\lambda_1$, $\lambda_2$, $\lambda_3$ не равно нулю. Не умаляя общности (и порядка столбцов в $Q$), $\lambda_3=0$.\\
                Если $a_1'\neq0$ или $a_2'\neq0$, выносим полный квадрат так, как делали пунктом выше.
                \begin{itemize}
                    \item Если $a_3'\neq0$, то вдобавок к параллельному переносу по $Ox'y'$, совершаем перенос по оси $z'$ как $z''=z'+\frac{a_0'}{2a_3'}$:
                    \[\begin{split}
                        &\lambda_1{x'}^2+\lambda_2{y'}^2+2a_1'x'+2a_2'y'+2a_3'z'+a_0=0\Leftrightarrow\\
                        &\lambda_1\left(x'+\frac{a_1'}{\lambda_1}\right)^2+\lambda_2\left(y'+\frac{a_2'}{\lambda_2}\right)^2+2a_3'z'\underbrace{-\frac{{a_1'}^2}{\lambda_1}-\frac{{a_2'}^2}{\lambda_2}+a_0}_{a'_0}=0\Leftrightarrow\\
                        &\lambda_1{x''}^2+\lambda_2{y''}^2+2a_3'\underbrace{\left(z'+\frac{a_0'}{2a_3'}\right)}_{z''}=0
                    \end{split}\]
                    То есть параллельный перенос выглядит так:
                    $$
                    \left\{\begin{aligned}
                        x''=x'+\frac{a_1'}{\lambda_1}\\
                        y''=y'+\frac{a_2'}{\lambda_2}\\
                        z''=z'+\frac{a_0'}{2a_3'}
                    \end{aligned}\right.
                    $$
                    Обратите особое внимание на то, что в последнем уравнении стоит $a_0'$, а не $a_0$, то есть свободный член, полученный после преобразования $x'$ и $y'$ в $x''$ и $y''$.\\
                    После переноса, можно перенести $z''$ вправо и поделить на коэффициент при нём. Получится
                    $$
                    \alpha{x''}^2+\beta{y''}^2=z''
                    $$
                    Это либо эллиптический параболоид (при $\alpha\beta>0$), либо гиперболический $\alpha\beta<0$.
                    \item Если же у нас $a_3'=0$, то совершаем параллельный перенос только вдоль $Oxy$:
                    $$
                    \left\{\begin{aligned}
                        x''=x'+\frac{a_1'}{\lambda_1}\\
                        y''=y'+\frac{a_2'}{\lambda_2}\\
                        z''=z'
                    \end{aligned}\right.
                    $$
                    При этом от $z''$, как несложно заметить, в уравнении ничего не зависит, потому что выглядит оно так:
                    $$
                    \lambda_1{x''}^2+\lambda_2{y''}^2+a_0'=0
                    $$
                    \begin{itemize}
                        \item При $a_0'\neq0$, переносим вправо и делим:
                        $$
                        \alpha{x''}^2+\beta{y''}^2=1
                        $$
                        Что даёт нам либо эллиптический ($\alpha>0$ и $\beta>0$), либо гиперболический ($\alpha$ и $\beta$ разных знаков) цилиндр, либо пустое множество ($\alpha<0$ и $\beta<0$).
                        \item Если $a_0'=0$, то получаем вот что:
                        $$
                        {y''}^2=\alpha{x''}^2
                        $$
                        Это пара пересекающихся плоскостей при $\alpha>0$, либо прямая при $\alpha<0$.
                    \end{itemize}
                \end{itemize}
                \item Остаётся случай, когда только одно собственное число $A$ не равно нулю. Опять же, не умаляя общности и порядка столбцов в $Q$, оно стоит при $x'$. Тогда мы имеем уравнение
                $$
                \lambda_1{x'}^2+2a_1x'+2a_2y'+2a_3z'+a_0=0
                $$
                Уже описанным выше образом совершим параллельный перенос вдоль оси $Ox$:
                $$
                    x''=x'+\frac{a_1'}{\lambda_1}
                $$
                Это превратит наше уравнение в
                $$
                \lambda_1{x''}^2+2a_2'y'+2a_3'z'+a_0'=0
                $$
                \begin{itemize}
                    \item Если $a_2'\neq0\neq a_3'$, то таким уравнением жить очень плохо, потому что нужен ещё один поворот. Повернуть мы хотим уравнение вокруг $Ox''$ так, чтобы из абсциссы и аппликаты осталось только одно. Как это сделать? Ну, посмотрим на систему:
                    $$
                    \left\{\begin{aligned}
                        y'&=y''\cos\phi-z''\sin\phi\\
                        z'&=y''\sin\phi+z''\cos\phi
                    \end{aligned}\right.
                    $$
                    Тогда
                    \[\begin{split}
                        \lambda_1{x''}^2+2a_2'(y''\cos\phi-z''\sin\phi)+2a_3'(y''\sin\phi+z''\cos\phi)+a_0'=0\Leftrightarrow\\
                        \lambda_1{x''}^2+2y''\underbrace{(a_2'\cos\phi+a_3'\sin\phi)}_{a_2''}+2z''(-a_2'\sin\phi+a_3'\cos\phi)+a_0'=0
                    \end{split}\]
                    Хочется, чтобы $-a_2'\sin\phi+a_3'\cos\phi$ было равно нулю, и достигается это при $\tan\phi=\frac{a_3'}{a_2'}$ или $\phi=\atan\frac{a_3'}{a_2'}$.
                    \item Второй вариант --- строго одно из $a_2'$ и $a_3'$ равно нулю (либо сразу, либо после предыдущего преобразования). Тогда мы имеем
                    $$
                    \alpha{x''}^2+2a_2''y''+a_0'=0
                    $$
                    Это параболический цилиндр.
                    \item Если $a_2'=a_3'=0$, то уравнение преобразуется в
                    $$
                    {x''}^2=-\frac{a_0'}{\lambda_1}=\alpha
                    $$
                    И это в зависимости от $\alpha$ либо две параллельные плоскости ($\alpha>0$), либо одна плоскость ($\alpha=0$), либо пустое множество ($\alpha<0$).
                \end{itemize}
                \item Все три $\lambda_1$, $\lambda_2$ и $\lambda_3$ не могут получиться равными нулю по определению квадратичной формы.
            \end{itemize}
        \end{Comment}
    \end{itemize}
\end{document}